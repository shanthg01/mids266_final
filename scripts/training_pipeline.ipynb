{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Pipeline for LSTMABAR\n",
    "Includes data loading, training loop, and evaluation\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lstmabar_model import LSTMABAR, LSTMABARTrainer\n",
    "from musiccaps_loader import MusicCapsLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicCapsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for MusicCaps with archetype annotations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        training_data_path: str,\n",
    "        sample_rate: int = 44100,\n",
    "        audio_duration: float = 2.0,\n",
    "        augment: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            training_data_path: Path to .npz file with training data\n",
    "            sample_rate: Audio sample rate\n",
    "            audio_duration: Duration to load from each audio file\n",
    "            augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.sample_rate = sample_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.augment = augment\n",
    "        self.target_samples = int(sample_rate * audio_duration)\n",
    "        \n",
    "        # Load training data\n",
    "        print(f\"Loading training data from {training_data_path}\")\n",
    "        data = np.load(training_data_path, allow_pickle=True)\n",
    "        \n",
    "        self.archetype_vectors = torch.from_numpy(data['archetype_vectors']).float()\n",
    "        self.descriptions = data['descriptions'].tolist()\n",
    "        self.audio_paths = data['audio_paths'].tolist()\n",
    "        self.archetype_order = data['archetype_order'].tolist()\n",
    "        \n",
    "        print(f\"Loaded {len(self.descriptions)} training examples\")\n",
    "        \n",
    "        # Filter out samples with missing audio files\n",
    "        self.valid_indices = self._find_valid_samples()\n",
    "        print(f\"Found {len(self.valid_indices)} samples with valid audio files\")\n",
    "    \n",
    "    def _find_valid_samples(self) -> List[int]:\n",
    "        \"\"\"Find indices with existing audio files\"\"\"\n",
    "        valid = []\n",
    "        for i, audio_path in enumerate(self.audio_paths):\n",
    "            if Path(audio_path).exists():\n",
    "                valid.append(i)\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Get a single training sample\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'audio', 'description', 'archetype_weights'\n",
    "        \"\"\"\n",
    "        # Map to valid index\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        \n",
    "        # Load audio\n",
    "        audio_path = self.audio_paths[actual_idx]\n",
    "        audio, sr = librosa.load(\n",
    "            audio_path,\n",
    "            sr=self.sample_rate,\n",
    "            duration=self.audio_duration\n",
    "        )\n",
    "        \n",
    "        # Pad or trim to exact length\n",
    "        if len(audio) < self.target_samples:\n",
    "            audio = np.pad(audio, (0, self.target_samples - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:self.target_samples]\n",
    "        \n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment:\n",
    "            audio = self._augment_audio(audio)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        audio_tensor = torch.from_numpy(audio).float()\n",
    "        \n",
    "        # Get description and archetype weights\n",
    "        description = self.descriptions[actual_idx]\n",
    "        archetype_weights = self.archetype_vectors[actual_idx]\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_tensor,\n",
    "            'description': description,\n",
    "            'archetype_weights': archetype_weights\n",
    "        }\n",
    "    \n",
    "    def _augment_audio(self, audio: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply random audio augmentations\"\"\"\n",
    "        # Random gain (±3dB)\n",
    "        if np.random.random() > 0.5:\n",
    "            gain_db = np.random.uniform(-3, 3)\n",
    "            audio = audio * (10 ** (gain_db / 20))\n",
    "        \n",
    "        # Random time shift\n",
    "        if np.random.random() > 0.5:\n",
    "            shift = np.random.randint(-self.sample_rate // 10, self.sample_rate // 10)\n",
    "            audio = np.roll(audio, shift)\n",
    "        \n",
    "        # Add slight noise\n",
    "        if np.random.random() > 0.5:\n",
    "            noise = np.random.randn(len(audio)) * 0.005\n",
    "            audio = audio + noise\n",
    "        \n",
    "        return audio\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader\n",
    "    Handles variable-length descriptions\n",
    "    \"\"\"\n",
    "    audio = torch.stack([item['audio'] for item in batch])\n",
    "    descriptions = [item['description'] for item in batch]\n",
    "    archetype_weights = torch.stack([item['archetype_weights'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'audio': audio,\n",
    "        'descriptions': descriptions,\n",
    "        'archetype_weights': archetype_weights\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    \"\"\"\n",
    "    Complete training pipeline for LSTMABAR\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LSTMABAR,\n",
    "        train_dataset: MusicCapsDataset,\n",
    "        val_dataset: Optional[MusicCapsDataset] = None,\n",
    "        batch_size: int = 16,\n",
    "        learning_rate: float = 1e-4,\n",
    "        num_epochs: int = 50,\n",
    "        checkpoint_dir: str = 'checkpoints',\n",
    "        log_interval: int = 10\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        if val_dataset is not None:\n",
    "            self.val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        else:\n",
    "            self.val_loader = None\n",
    "        \n",
    "        # Initialize trainer\n",
    "        self.trainer = LSTMABARTrainer(\n",
    "            model,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_weights={\n",
    "                'contrastive': 0.7,\n",
    "                'archetype_prediction': 0.2,\n",
    "                'audio_archetype_supervision': 0.1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"Training pipeline initialized:\")\n",
    "        print(f\"  Training samples: {len(train_dataset)}\")\n",
    "        print(f\"  Validation samples: {len(val_dataset) if val_dataset else 0}\")\n",
    "        print(f\"  Batch size: {batch_size}\")\n",
    "        print(f\"  Total epochs: {num_epochs}\")\n",
    "        print(f\"  Steps per epoch: {len(self.train_loader)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Run complete training loop\"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Training\n",
    "            train_losses = self.trainer.train_epoch(self.train_loader, epoch)\n",
    "            print(f\"\\nTrain Losses: {train_losses}\")\n",
    "            \n",
    "            # Validation\n",
    "            if self.val_loader is not None:\n",
    "                val_losses = {'total': 0.0}\n",
    "                for batch in self.val_loader:\n",
    "                    batch_losses = self.trainer.validate(\n",
    "                        batch['descriptions'],\n",
    "                        batch['audio'].to(self.model.device),\n",
    "                        batch['archetype_weights'].to(self.model.device)\n",
    "                    )\n",
    "                    for key in batch_losses:\n",
    "                        val_losses[key] = val_losses.get(key, 0.0) + batch_losses[key]\n",
    "                \n",
    "                # Average validation losses\n",
    "                for key in val_losses:\n",
    "                    val_losses[key] /= len(self.val_loader)\n",
    "                \n",
    "                print(f\"Val Losses: {val_losses}\")\n",
    "                \n",
    "                # Save best model\n",
    "                if val_losses['total'] < best_val_loss:\n",
    "                    best_val_loss = val_losses['total']\n",
    "                    save_path = self.checkpoint_dir / 'best_model.pth'\n",
    "                    self.model.save_checkpoint(\n",
    "                        str(save_path),\n",
    "                        epoch,\n",
    "                        self.trainer.optimizer.state_dict()\n",
    "                    )\n",
    "                    print(f\"✓ Best model saved (val_loss: {best_val_loss:.4f})\")\n",
    "            \n",
    "            # Save periodic checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                save_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "                self.model.save_checkpoint(\n",
    "                    str(save_path),\n",
    "                    epoch,\n",
    "                    self.trainer.optimizer.state_dict()\n",
    "                )\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Training complete!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save final model\n",
    "        final_path = self.checkpoint_dir / 'final_model.pth'\n",
    "        self.model.save_checkpoint(\n",
    "            str(final_path),\n",
    "            self.num_epochs - 1,\n",
    "            self.trainer.optimizer.state_dict()\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        self.plot_training_history()\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training curves\"\"\"\n",
    "        history = self.trainer.history\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Total loss\n",
    "        axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "        if history['val_loss']:\n",
    "            axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Total Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Component losses\n",
    "        axes[1].plot(history['contrastive_loss'], label='Contrastive Loss')\n",
    "        axes[1].plot(history['archetype_loss'], label='Archetype Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].set_title('Component Losses')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.checkpoint_dir / 'training_history.png', dpi=300)\n",
    "        print(f\"Training history plot saved to {self.checkpoint_dir / 'training_history.png'}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def prepare_musiccaps_data(\n",
    "    csv_path: str,\n",
    "    audio_dir: str = 'musiccaps_audio',\n",
    "    output_path: str = 'musiccaps_training_data.npz',\n",
    "    max_downloads: int = 500,\n",
    "    train_split: float = 0.7,\n",
    "    val_split: float = 0.15,\n",
    "    test_split: float = 0.15,\n",
    "    random_seed: int = 42\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Prepare MusicCaps dataset for training\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to MusicCaps CSV\n",
    "        audio_dir: Directory to save audio\n",
    "        output_path: Base path for output files\n",
    "        max_downloads: Max clips to download\n",
    "        train_split: Fraction for training (default 0.7)\n",
    "        val_split: Fraction for validation (default 0.15)\n",
    "        test_split: Fraction for testing (default 0.15)\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_path, val_path, test_path) .npz files\n",
    "    \"\"\"\n",
    "\n",
    "    assert abs(train_split + val_split + test_split - 1.0) < 1e-6, \\\n",
    "        \"Splits must sum to 1.0\"\n",
    "\n",
    "    print(\"=== Preparing MusicCaps Dataset ===\")\n",
    "    \n",
    "    # Load and process MusicCaps\n",
    "    loader = MusicCapsLoader(csv_path, audio_dir)\n",
    "    \n",
    "    # Download audio clips\n",
    "    print(f\"\\nDownloading up to {max_downloads} audio clips...\")\n",
    "    downloaded, failed = loader.download_audio_clips(\n",
    "        max_clips=max_downloads,\n",
    "        use_balanced_subset=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully downloaded: {len(downloaded)}\")\n",
    "    print(f\"Failed: {len(failed)}\")\n",
    "    \n",
    "    # Create archetype training data\n",
    "    print(\"\\nCreating archetype training data...\")\n",
    "    training_data = loader.create_archetype_training_data(use_tfidf_weighting=True)\n",
    "    \n",
    "    # Shuffle data for random splits\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.random.permutation(len(training_data))\n",
    "    training_data = [training_data[i] for i in indices]\n",
    "    \n",
    "    # Calculate split points\n",
    "    n_total = len(training_data)\n",
    "    n_train = int(n_total * train_split)\n",
    "    n_val = int(n_total * val_split)\n",
    "    n_test = n_total - n_train - n_val\n",
    "    \n",
    "    # Split data\n",
    "    train_data = training_data[:n_train]\n",
    "    val_data = training_data[n_train:n_train + n_val]\n",
    "    test_data = training_data[n_train + n_val:]\n",
    "    \n",
    "    print(f\"\\nDataset split:\")\n",
    "    print(f\"  Train: {len(train_data)} samples ({train_split*100:.1f}%)\")\n",
    "    print(f\"  Val:   {len(val_data)} samples ({val_split*100:.1f}%)\")\n",
    "    print(f\"  Test:  {len(test_data)} samples ({test_split*100:.1f}%)\")\n",
    "    \n",
    "    # Save train set\n",
    "    train_path = output_path.replace('.npz', '_train.npz')\n",
    "    vectors_train = np.array([item['archetype_vector'] for item in train_data])\n",
    "    descriptions_train = [item['description'] for item in train_data]\n",
    "    audio_paths_train = [item['audio_path'] for item in train_data]\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        train_path,\n",
    "        archetype_vectors=vectors_train,\n",
    "        descriptions=descriptions_train,\n",
    "        audio_paths=audio_paths_train,\n",
    "        archetype_order=train_data[0]['archetype_order']\n",
    "    )\n",
    "    print(f\"\\n✓ Saved train set: {train_path}\")\n",
    "    \n",
    "    # Save val set\n",
    "    val_path = output_path.replace('.npz', '_val.npz')\n",
    "    vectors_val = np.array([item['archetype_vector'] for item in val_data])\n",
    "    descriptions_val = [item['description'] for item in val_data]\n",
    "    audio_paths_val = [item['audio_path'] for item in val_data]\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        val_path,\n",
    "        archetype_vectors=vectors_val,\n",
    "        descriptions=descriptions_val,\n",
    "        audio_paths=audio_paths_val,\n",
    "        archetype_order=val_data[0]['archetype_order']\n",
    "    )\n",
    "    print(f\"✓ Saved val set: {val_path}\")\n",
    "    \n",
    "    # Save test set\n",
    "    test_path = output_path.replace('.npz', '_test.npz')\n",
    "    vectors_test = np.array([item['archetype_vector'] for item in test_data])\n",
    "    descriptions_test = [item['description'] for item in test_data]\n",
    "    audio_paths_test = [item['audio_path'] for item in test_data]\n",
    "    \n",
    "    np.savez_compressed(\n",
    "        test_path,\n",
    "        archetype_vectors=vectors_test,\n",
    "        descriptions=descriptions_test,\n",
    "        audio_paths=audio_paths_test,\n",
    "        archetype_order=test_data[0]['archetype_order']\n",
    "    )\n",
    "    print(f\"✓ Saved test set: {test_path}\")\n",
    "    \n",
    "    return train_path, val_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'embedding_dim': 768,\n",
    "    'audio_architecture': 'resnet',  # or 'ast'\n",
    "    'sample_rate': 44100,\n",
    "    'audio_duration': 2.0,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 20,\n",
    "    'max_downloads': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/shanthgopalswamy/.cache/kagglehub/datasets/googleai/musiccaps/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Download data (run once)\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"googleai/musiccaps\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "csv_path = f\"{path}/musiccaps-public.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing MusicCaps dataset with train/val/test split...\n",
      "=== Preparing MusicCaps Dataset ===\n",
      "Loaded 5521 MusicCaps examples\n",
      "\n",
      "Downloading up to 500 audio clips...\n",
      "Using balanced subset: 1000 examples\n",
      "✓ Already exists: -bgHkxwoliw\n",
      "✓ Already exists: -kpR93atgd8\n",
      "✓ Already exists: -wymN80CiYU\n",
      "✓ Already exists: 07xGXxIHOL4\n",
      "✓ Already exists: 0PMFAO4TIU4\n",
      "✓ Already exists: 0TV9zvfwFhs\n",
      "✓ Already exists: 0fiOM---7QI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] 0i8VM_EooCs: Video unavailable. This video is no longer available due to a copyright claim by Terrabyte Music Limited\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 0i8VM_EooCs: ERROR: [youtube] 0i8VM_EooCs: Video unavailable. This video is no longer available due to a copyright claim by Terrabyte Music Limited\n",
      "✗ Failed: 0i8VM_EooCs\n",
      "✓ Already exists: 0jFQ21A6GRA\n",
      "✓ Already exists: 1ACn3u5UnBw\n",
      "✓ Already exists: 1BVSYfNCcv0\n",
      "✓ Already exists: 1JpeDWbgUO8\n",
      "✓ Already exists: 1PKxdTlquCA\n",
      "✓ Already exists: 1Q9DXhXMSFI\n",
      "✓ Already exists: 1TyOPtg0Yfk\n",
      "✓ Already exists: 1V7ReAk9k-4\n",
      "✓ Already exists: 1j4rFfU5XKQ\n",
      "✓ Already exists: 20Vh6z6Ie0E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] 2G5bSYHcJSM: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 2G5bSYHcJSM: ERROR: [youtube] 2G5bSYHcJSM: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
      "✗ Failed: 2G5bSYHcJSM\n",
      "✓ Already exists: 2GWkKVHxGRM\n",
      "✓ Already exists: 2JnlmS1zzls\n",
      "✓ Already exists: 2RU4CSDzS-g\n",
      "✓ Already exists: 2U8Dvh7nwFI\n",
      "✓ Already exists: 2ZfthfWQowE\n",
      "✓ Already exists: 2bCuw7U_Rac\n",
      "✓ Already exists: 2dyEnOo3yJ8\n",
      "✓ Already exists: 2vQTq4QLP8U\n",
      "✓ Already exists: 2xGRCsW6-Bk\n",
      "✓ Already exists: 3JYQgXudiH8\n",
      "✓ Already exists: 3TQmts_MxyQ\n",
      "✓ Already exists: 40D4L5Ndi6k\n",
      "✓ Already exists: 44sbWBFswUY\n",
      "✓ Already exists: 4i11P4OCRfk\n",
      "✓ Already exists: 5JQIsqc8HBc\n",
      "✓ Already exists: 5XXAeSybGK0\n",
      "✓ Already exists: 5ZpVhmhVYoI\n",
      "✓ Already exists: 5_orEetudIA\n",
      "✓ Already exists: 5gyMt0YzPQ0\n",
      "✓ Already exists: 60OIHit4Q-M\n",
      "✓ Already exists: 6N1LWG4aztA\n",
      "✓ Already exists: 6k4lcF9IGUk\n",
      "✓ Already exists: 7-mNJ4IUY5Q\n",
      "✓ Already exists: 7WZwlOrRELI\n",
      "✓ Already exists: 7_q36NyJtQY\n",
      "✓ Already exists: 8BJljuSm2Aw\n",
      "✓ Already exists: 8rCYRgePG-4\n",
      "✓ Already exists: 8zCZzzAaC4I\n",
      "✓ Already exists: 9Qd6AdTq3Ls\n",
      "✓ Already exists: 9UD7qz7DuVY\n",
      "✓ Already exists: 9ZAmdxKLnhs\n",
      "✓ Already exists: 9z4YXc9rjTo\n",
      "✓ Already exists: A4jSRfZ6yd0\n",
      "✓ Already exists: AEyeITzfPa0\n",
      "✓ Already exists: AGNqX_OL-dU\n",
      "✓ Already exists: AHrUfa2H_5s\n",
      "✓ Already exists: ALVS3Q_jNaU\n",
      "✓ Already exists: A_oaLt-n4fQ\n",
      "✓ Already exists: AobHGHJSd-s\n",
      "✓ Already exists: AzWIKyRnhG8\n",
      "✓ Already exists: AzsBSUmhl1M\n",
      "✓ Already exists: B00nfVc4FPI\n",
      "✓ Already exists: B1beLwV4yzw\n",
      "✓ Already exists: B7V_grbxflg\n",
      "✓ Already exists: B8pesuUc8Ek\n",
      "✓ Already exists: BC7dI3lQ2Cg\n",
      "✓ Already exists: BHu95Y_kVQA\n",
      "✓ Already exists: BL181hSAG60\n",
      "✓ Already exists: BMgYWTTJv3s\n",
      "✓ Already exists: BR5MDhvcGM8\n",
      "✓ Already exists: BRTHyoVgZT0\n",
      "✓ Already exists: BS2ZnUhmHj4\n",
      "✓ Already exists: BWKQXn5xDwo\n",
      "                             \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ERROR: Postprocessing: WARNING: unable to obtain file audio codec with ffprobe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading BXo1Tr_oJds: ERROR: Postprocessing: WARNING: unable to obtain file audio codec with ffprobe\n",
      "✗ Failed: BXo1Tr_oJds\n",
      "✓ Already exists: Bl-lCgr5hGY\n",
      "✓ Already exists: BnkDQXlrIX4\n",
      "✓ Already exists: Byk9p21g51g\n",
      "✓ Already exists: C5MhO2HM2Wg\n",
      "✓ Already exists: C6roSYqchkk\n",
      "✓ Already exists: C8VECv8kicU\n",
      "✓ Already exists: CJjyrDGmxIY\n",
      "✓ Already exists: CP3phqztym0\n",
      "✓ Already exists: CRxIJ7YbcZA\n",
      "✓ Already exists: CWQvCCRuU6k\n",
      "✓ Already exists: CZuH43NPynA\n",
      "✓ Already exists: Cchf2QH63bI\n",
      "✓ Already exists: ChyayWIp_vU\n",
      "✓ Already exists: CphwhKgYHaM\n",
      "✓ Already exists: CzMNiypg1I8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] Czbi1u-gwUU: Video unavailable. This video is no longer available due to a copyright claim by Cheb Hasni\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading Czbi1u-gwUU: ERROR: [youtube] Czbi1u-gwUU: Video unavailable. This video is no longer available due to a copyright claim by Cheb Hasni\n",
      "✗ Failed: Czbi1u-gwUU\n",
      "✓ Already exists: D2w3qHmJrdU\n",
      "✓ Already exists: D3FyfFIKLVc\n",
      "✓ Already exists: D4ccFYk3bhU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] D8-x1T8M4gk: Video unavailable. This video has been removed by the uploader\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading D8-x1T8M4gk: ERROR: [youtube] D8-x1T8M4gk: Video unavailable. This video has been removed by the uploader\n",
      "✗ Failed: D8-x1T8M4gk\n",
      "✓ Already exists: DAPGvg8qOAU\n",
      "✓ Already exists: DCFrCX4HPO8\n",
      "✓ Already exists: DG5d4megH8g\n",
      "✓ Already exists: DGbMEkQerYs\n",
      "✓ Already exists: DKflAAykh6A\n",
      "✓ Already exists: DP2vmsftZHY\n",
      "✓ Already exists: DU5pD63Pv30\n",
      "✓ Already exists: DaiVfxATCEE\n",
      "✓ Already exists: DdxW_JziHTA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] DysXetu2I0E: Video unavailable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading DysXetu2I0E: ERROR: [youtube] DysXetu2I0E: Video unavailable\n",
      "✗ Failed: DysXetu2I0E\n",
      "✓ Already exists: EKZvq0dUk50\n",
      "✓ Already exists: EUNTykrvpok\n",
      "✓ Already exists: EaGhKzpkNso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] EfUUgsioXyU: Private video. Sign in if you've been granted access to this video. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading EfUUgsioXyU: ERROR: [youtube] EfUUgsioXyU: Private video. Sign in if you've been granted access to this video. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n",
      "✗ Failed: EfUUgsioXyU\n",
      "✓ Already exists: EmSZKb0LdVM\n",
      "✓ Already exists: Es9FNjZ-SHI\n",
      "✓ Already exists: FCzMqo8kh1o\n",
      "✓ Already exists: FDO5BekX478\n",
      "✓ Already exists: FENJIDecy5s\n",
      "✓ Already exists: Fsm-xDmyFKg\n",
      "✓ Already exists: FsnRM2irjvI\n",
      "✓ Already exists: FteW_2gNtD4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] Fv9swdLA-lo: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading Fv9swdLA-lo: ERROR: [youtube] Fv9swdLA-lo: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
      "✗ Failed: Fv9swdLA-lo\n",
      "✓ Already exists: G2uCAwYS6w0\n",
      "✓ Already exists: GHQlBD-6rkA\n",
      "✓ Already exists: GJYhDjThTHM\n",
      "✓ Already exists: GLIXnXZEOxY\n",
      "✓ Already exists: GPSqrciDLog\n",
      "✓ Already exists: GQbUpJFArKI\n",
      "✓ Already exists: GYCfrx0ruz4\n",
      "✓ Already exists: GbjtSTTEFK4\n",
      "✓ Already exists: Gc8xf7CJiFY\n",
      "✓ Already exists: GkB_BkyVyPs\n",
      "✓ Already exists: Guu30szkA-0\n",
      "✓ Already exists: H6qzijVEqZQ\n",
      "✓ Already exists: HFH9tcIK_PM\n",
      "✓ Already exists: HFVM5pVTwkM\n",
      "✓ Already exists: HHTgjmgTV6c\n",
      "✓ Already exists: HNf9eHqDT1A\n",
      "✓ Already exists: HS_ikHx4LIQ\n",
      "✓ Already exists: HU7oqkJeItQ\n",
      "✓ Already exists: HYjSrwSm0T4\n",
      "✓ Already exists: HfzEa06vDLg\n",
      "✓ Already exists: Hg4f2xt3oKA\n",
      "✓ Already exists: HkXSX7Kdhms\n",
      "✓ Already exists: Hnk45Z0EAxg\n",
      "✓ Already exists: HzXWXYxXyYA\n",
      "✓ Already exists: ID4AoAfHMVk\n",
      "✓ Already exists: IKq2OF8jq1c\n",
      "✓ Already exists: Ikdb_jA9ehU\n",
      "✓ Already exists: JDBu-3VCyWc\n",
      "✓ Already exists: Jcd63Ev7JXA\n",
      "✓ Already exists: Jdy08IPLKdw\n",
      "✓ Already exists: Jjr0_CbcYdg\n",
      "✓ Already exists: JoBRbtAnbVM\n",
      "✓ Already exists: K-zkbbliQcI\n",
      "✓ Already exists: KB79k456DhI\n",
      "✓ Already exists: KCs_VPmsnKo\n",
      "✓ Already exists: KDzy3ZL626U\n",
      "✓ Already exists: KMQmM12G9Z4\n",
      "✓ Already exists: KdNhYvN4Xoo\n",
      "✓ Already exists: KzvdKLdBw3s\n",
      "✓ Already exists: L0oun9F67tg\n",
      "✓ Already exists: L1s-oPHsOac\n",
      "✓ Already exists: L1s7KZgWXGc\n",
      "✓ Already exists: L5CgdTtGv8o\n",
      "✓ Already exists: LB0u0PrlDHU\n",
      "✓ Already exists: LCzldLY3E4g\n",
      "✓ Already exists: LFYRuK8YstI\n",
      "✓ Already exists: LKUYtvUHn0Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] LRfVQsnaVQE: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading LRfVQsnaVQE: ERROR: [youtube] LRfVQsnaVQE: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
      "✗ Failed: LRfVQsnaVQE\n",
      "✓ Already exists: L_nC2BvhRdQ\n",
      "✓ Already exists: LfvdxSBCtFE\n",
      "✓ Already exists: LjihfG0fit0\n",
      "✓ Already exists: LybSS4amIS0\n",
      "✓ Already exists: LzSWdj4izHM\n",
      "✓ Already exists: MHkfPjW0aRg\n",
      "✓ Already exists: MIexFfOsuJs\n",
      "✓ Already exists: MKikHxKeodA\n",
      "✓ Already exists: MM0seezR2F4\n",
      "✓ Already exists: MVYSWTF11Nc\n",
      "✓ Already exists: MY0PsDE3xHs\n",
      "✓ Already exists: MdYXznF3Eac\n",
      "✓ Already exists: MpWGx5odhh8\n",
      "✓ Already exists: MsjeOXuUYG4\n",
      "✓ Already exists: MvnC1TfNiPY\n",
      "✓ Already exists: MzUgHy7SyS8\n",
      "✓ Already exists: N-dzfI3L5ic\n",
      "✓ Already exists: NHA1l_Czm38\n",
      "✓ Already exists: N_Wx35sNqdM\n",
      "✓ Already exists: NlCfScKw_Mk\n",
      "✓ Already exists: NsYVaRI6rXg\n",
      "✓ Already exists: Nt0U-CXK6O0\n",
      "✓ Already exists: NwA9JSlK_lM\n",
      "✓ Already exists: O1RmrE_HfpE\n",
      "✓ Already exists: OB7GyVqufwQ\n",
      "✓ Already exists: OEjgIDubFbg\n",
      "✓ Already exists: OH2SQhJqZDg\n",
      "✓ Already exists: OI7S7vaBT4I\n",
      "✓ Already exists: OKquGBKOgME\n",
      "✓ Already exists: ONfd_rHtL74\n",
      "✓ Already exists: OR_YbeqV5tA\n",
      "✓ Already exists: ORikRIu7s1o\n",
      "✓ Already exists: OcdrbkDm2LQ\n",
      "✓ Already exists: OmjfHQB_lcs\n",
      "✓ Already exists: OpWCljke4oQ\n",
      "✓ Already exists: Orge4_UlvNI\n",
      "✓ Already exists: OrsfEkAhie4\n",
      "✓ Already exists: P-eIhvCaK-s\n",
      "✓ Already exists: P240GHf9Eq4\n",
      "✓ Already exists: P25JeM4lPGw\n",
      "✓ Already exists: P8nK4i8XscM\n",
      "✓ Already exists: PAX2PMha2dU\n",
      "✓ Already exists: PZZxVIIOQPo\n",
      "✓ Already exists: PeWXdkEUPbo\n",
      "✓ Already exists: PvHKu1XRSJ0\n",
      "✓ Already exists: Q789S_9JCio\n",
      "✓ Already exists: QBhhtVMiQBQ\n",
      "✓ Already exists: QK-mjNg8cPo\n",
      "✓ Already exists: QKkhwAAGLIE\n",
      "✓ Already exists: QSaX7QfeWog\n",
      "✓ Already exists: QZoclbefgak\n",
      "✓ Already exists: QfM5-WqvquQ\n",
      "✓ Already exists: QhF0CFyzzAc\n",
      "✓ Already exists: QutCXtWmzIs\n",
      "✓ Already exists: R5JRh08zgMo\n",
      "✓ Already exists: RIiN9Ed1fqU\n",
      "✓ Already exists: ROM--1yVra8\n",
      "✓ Already exists: RPqz3vJYMLQ\n",
      "✓ Already exists: RcfaWoTywcA\n",
      "✓ Already exists: Rdwtr2IX8ek\n",
      "✓ Already exists: RmyyW-TMkVc\n",
      "✓ Already exists: RneRJ5ZnHlE\n",
      "✓ Already exists: RtgHU1UMo5o\n",
      "✓ Already exists: S7TYAcOEPt4\n",
      "✓ Already exists: SG24NL2Xi3A\n",
      "✓ Already exists: SJ9TY-iqD9Q\n",
      "✓ Already exists: S_Z7o4OmU30\n",
      "✓ Already exists: T0oF8MyYhBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] T6iv9GFIVyU: Video unavailable. This video is no longer available due to a copyright claim by Rishad Zahir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading T6iv9GFIVyU: ERROR: [youtube] T6iv9GFIVyU: Video unavailable. This video is no longer available due to a copyright claim by Rishad Zahir\n",
      "✗ Failed: T6iv9GFIVyU\n",
      "✓ Already exists: T7A0RejsZIo\n",
      "✓ Already exists: T7ZSZhcsfjA\n",
      "✓ Already exists: TN53jpjqAGI\n",
      "✓ Already exists: TPYNIc_M1ng\n",
      "✓ Already exists: Tp8PG2xae8c\n",
      "✓ Already exists: Tsmx6Pb7CnU\n",
      "✓ Already exists: TworrkXAPuI\n",
      "✓ Already exists: TzPuAqjoL80\n",
      "✓ Already exists: U4UtZeTl2DE\n",
      "✓ Already exists: UDN11Q90Fa4\n",
      "✓ Already exists: UFyOGqmITjM\n",
      "✓ Already exists: UIOnnpaqBy8\n",
      "✓ Already exists: UNJswfXKJ3s\n",
      "✓ Already exists: UQKLBsZJsww\n",
      "✓ Already exists: UcabTrKowlI\n",
      "✓ Already exists: UnFEqUWTefM\n",
      "✓ Already exists: UoxHwOl2gN0\n",
      "✓ Already exists: UrgzGbGVV8I\n",
      "✓ Already exists: UsdoUjuczY4\n",
      "✓ Already exists: UtZofZjccBs\n",
      "✓ Already exists: UvCY9FHpKC8\n",
      "✓ Already exists: V3Vvp5HS90k\n",
      "✓ Already exists: V9jIsOTC1lY\n",
      "✓ Already exists: VCusyLPrFCo\n",
      "✓ Already exists: VG6-MlmCgzI\n",
      "✓ Already exists: VHYxygh1STA\n",
      "✓ Already exists: VL6uF-XeE_A\n",
      "✓ Already exists: VMzn9GytUTk\n",
      "✓ Already exists: VNjYW4OXqTs\n",
      "✓ Already exists: VV-DgnPPhSo\n",
      "✓ Already exists: VV85n-ebuUU\n",
      "✓ Already exists: VZJfkEet6EQ\n",
      "✓ Already exists: Vbcy6KBJsAA\n",
      "✓ Already exists: VfARCp38XtA\n",
      "✓ Already exists: VnJeG9RGUVM\n",
      "✓ Already exists: VswY2mI7Wbo\n",
      "✓ Already exists: Vt3HzkNtOP4\n",
      "✓ Already exists: VuWr1HXHoZg\n",
      "✓ Already exists: VzYc-c-uHjI\n",
      "✓ Already exists: W-nkxlYTdV4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] W0aT3SdtnfY: Private video. Sign in if you've been granted access to this video. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading W0aT3SdtnfY: ERROR: [youtube] W0aT3SdtnfY: Private video. Sign in if you've been granted access to this video. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n",
      "✗ Failed: W0aT3SdtnfY\n",
      "✓ Already exists: W3lKc2hj4XU\n",
      "✓ Already exists: W7U-glgu4GM\n",
      "✓ Already exists: WCifI6rwOoM\n",
      "✓ Already exists: WEVBqGarEIY\n",
      "✓ Already exists: WMtztIW1f6k\n",
      "✓ Already exists: WPguqXCBQCI\n",
      "✓ Already exists: WTVC7ZI9WtY\n",
      "✓ Already exists: WT_wvvEvkw4\n",
      "✓ Already exists: WaddbqEQ1NE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] We0WIPYrtRE: Video unavailable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading We0WIPYrtRE: ERROR: [youtube] We0WIPYrtRE: Video unavailable\n",
      "✗ Failed: We0WIPYrtRE\n",
      "✓ Already exists: WeDA1mDFSCo\n",
      "✓ Already exists: WgZ8KAnnTb8\n",
      "✓ Already exists: WsDb16qzA5Q\n",
      "✓ Already exists: WtN6uiDikRM\n",
      "✓ Already exists: X96v9LlsjJM\n",
      "✓ Already exists: XE4NRSDLYG8\n",
      "✓ Already exists: XUD-9HkQuTE\n",
      "✓ Already exists: XXBVsNt2Qr8\n",
      "✓ Already exists: XYOnq7ju7o0\n",
      "✓ Already exists: XgOA5oRkL2A\n",
      "✓ Already exists: XjUmXwVlDDo\n",
      "✓ Already exists: XkBXsaSXDJ0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] XvtL_TTLXHY: Video unavailable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading XvtL_TTLXHY: ERROR: [youtube] XvtL_TTLXHY: Video unavailable\n",
      "✗ Failed: XvtL_TTLXHY\n",
      "✓ Already exists: XwhAoMLNYWQ\n",
      "✓ Already exists: XykUpCigu4w\n",
      "✓ Already exists: Y7mTjfgcybQ\n",
      "✓ Already exists: YZx0_GRtvJk\n",
      "✓ Already exists: YcWJUHWt-64\n",
      "✓ Already exists: YrGQKTbiG1g\n",
      "✓ Already exists: YzpzKyzyL0Y\n",
      "✓ Already exists: Z31gI08SMzI\n",
      "✓ Already exists: Z7V7Curou7s\n",
      "✓ Already exists: Z8L3jychP14\n",
      "✓ Already exists: ZEuY5HnECuo\n",
      "✓ Already exists: ZFimyfPWltk\n",
      "✓ Already exists: ZJHlHb-VyDc\n",
      "✓ Already exists: ZLXW4ewrVpQ\n",
      "✓ Already exists: ZMd8mAKe-k8\n",
      "✓ Already exists: ZNGvyFsCx4g\n",
      "✓ Already exists: ZUcHBeueBww\n",
      "✓ Already exists: ZUkh168Nyus\n",
      "✓ Already exists: ZaUaqnLdg6k\n",
      "✓ Already exists: Zhurw43-Y1g\n",
      "✓ Already exists: ZkfKOLp5SxU\n",
      "✓ Already exists: Zlbo8ygfPSM\n",
      "✓ Already exists: ZmgkpmzvL6c\n",
      "✓ Already exists: ZoAfkpmztww\n",
      "✓ Already exists: ZsmfIMEzrQs\n",
      "✓ Already exists: Zt8x7tvP9Qs\n",
      "✓ Already exists: Zz1Bz1a7yPE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] _3OlK_1yQOk: Video unavailable. This video contains content from Storm Labels Inc., who has blocked it on copyright grounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading _3OlK_1yQOk: ERROR: [youtube] _3OlK_1yQOk: Video unavailable. This video contains content from Storm Labels Inc., who has blocked it on copyright grounds\n",
      "✗ Failed: _3OlK_1yQOk\n",
      "✓ Already exists: _43OOP6UEw0\n",
      "✓ Already exists: _78P-0zWJtg\n",
      "✓ Already exists: _9OUh0uwDec\n",
      "✓ Already exists: _R9Ma9rjEWg\n",
      "✓ Already exists: _b5n-mny1lM\n",
      "✓ Already exists: _fKntnlIYTQ\n",
      "✓ Already exists: _gWEpDgPAho\n",
      "✓ Already exists: _h2rFVPCSPE\n",
      "✓ Already exists: _lq8nEXh064\n",
      "✓ Already exists: _m-N4i-ge28\n",
      "✓ Already exists: _mQ6KuA2p6k\n",
      "✓ Already exists: _n3r2inlqBc\n",
      "✓ Already exists: _n9boKzVRhs\n",
      "✓ Already exists: _yXtw_z2xf4\n",
      "✓ Already exists: a2Wuroc8DQU\n",
      "✓ Already exists: aJHv6TV7JpY\n",
      "✓ Already exists: aOGNUGgTQ8k\n",
      "✓ Already exists: aPQTrv2B1sw\n",
      "✓ Already exists: aUH12rRIVDw\n",
      "✓ Already exists: aUXKK9AmrPU\n",
      "✓ Already exists: aUvHaURNgY8\n",
      "✓ Already exists: aW6greyYuO4\n",
      "✓ Already exists: aWK9CcvOK9w\n",
      "✓ Already exists: aY8-pXDdwiw\n",
      "✓ Already exists: ad6UhYwTXXQ\n",
      "✓ Already exists: adYFXYPqo2M\n",
      "✓ Already exists: ajy9PM2SJ6c\n",
      "✓ Already exists: ak7R0_8aKwI\n",
      "✓ Already exists: ao-TFiShaWU\n",
      "✓ Already exists: aryufzYGhbM\n",
      "✓ Already exists: as7MhTe961k\n",
      "✓ Already exists: asT8yaJPP1s\n",
      "✓ Already exists: aupCwPVWsMo\n",
      "✓ Already exists: awax48X8YlU\n",
      "✓ Already exists: ax8hXst_b5g\n",
      "✓ Already exists: b4FomUpNaJE\n",
      "✓ Already exists: b9rgWct9ivI\n",
      "✓ Already exists: bBfi3iEu9fk\n",
      "✓ Already exists: bHiRX6QYwEk\n",
      "✓ Already exists: bJ6e9Ja1ahQ\n",
      "✓ Already exists: bLjOJRg2P_Q\n",
      "✓ Already exists: bOOJRGRy0zc\n",
      "✓ Already exists: bTuKGXDrqdQ\n",
      "✓ Already exists: bVc7-sZAi6s\n",
      "✓ Already exists: bcybO-SMY5E\n",
      "✓ Already exists: bkYWnzw_5Bs\n",
      "✓ Already exists: bl-eQ8XD5CY\n",
      "✓ Already exists: blsYgo-B1k8\n",
      "✓ Already exists: bmVd2Zj8_Cc\n",
      "✓ Already exists: bqMgL5qmZ-k\n",
      "✓ Already exists: bzOeufhFITk\n",
      "✓ Already exists: c9JyKnsegog\n",
      "✓ Already exists: c9h4p6325Xo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] cADT8fUucLQ: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading cADT8fUucLQ: ERROR: [youtube] cADT8fUucLQ: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
      "✗ Failed: cADT8fUucLQ\n",
      "✓ Already exists: cBd0yZ27dtA\n",
      "✓ Already exists: cGUhG5PZp0A\n",
      "✓ Already exists: cS2gRhH6it4\n",
      "✓ Already exists: cXEJWtj2kT8\n",
      "✓ Already exists: cYRsnYEPIiM\n",
      "✓ Already exists: cbq6Q2htPRM\n",
      "✓ Already exists: chw8sAKOM5k\n",
      "✓ Already exists: clefr8E-iZQ\n",
      "✓ Already exists: cnvmLwFZr28\n",
      "✓ Already exists: cp8t27oT_ww\n",
      "✓ Already exists: cs-zcTX2tRA\n",
      "✓ Already exists: d1nz5tZckSA\n",
      "✓ Already exists: dBAeAk7dXnU\n",
      "✓ Already exists: dMAp3dvs3kE\n",
      "✓ Already exists: dNOHIxD0j_Q\n",
      "✓ Already exists: dSJpZQ8u_xY\n",
      "✓ Already exists: dSs4xfvATjc\n",
      "✓ Already exists: darQBSIlol8\n",
      "✓ Already exists: deIj55UAxeo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] doX8FjlNPf8: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading doX8FjlNPf8: ERROR: [youtube] doX8FjlNPf8: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
      "✗ Failed: doX8FjlNPf8\n",
      "✓ Already exists: dvDSgmqbrM0\n",
      "✓ Already exists: dwAo0dKCyBI\n",
      "✓ Already exists: dwFtlQLdbq0\n",
      "✓ Already exists: dwSj0Rr3vFc\n",
      "✓ Already exists: dy_yFZ6dL34\n",
      "✓ Already exists: e1KHGfMekek\n",
      "✓ Already exists: e2tZmQI8ICw\n",
      "✓ Already exists: e7WPFeDPFB4\n",
      "✓ Already exists: e8wnUU5pIWE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] eHeUipPZHIc: Video unavailable. This video contains content from SVG Music, who has blocked it on copyright grounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading eHeUipPZHIc: ERROR: [youtube] eHeUipPZHIc: Video unavailable. This video contains content from SVG Music, who has blocked it on copyright grounds\n",
      "✗ Failed: eHeUipPZHIc\n",
      "✓ Already exists: eI4PbSh6g_Y\n",
      "✓ Already exists: eM0PkfqGmIE\n",
      "✓ Already exists: eOmQbJljnqE\n",
      "✓ Already exists: eQTK2fo3RoE\n",
      "✓ Already exists: eSesh6vnek8\n",
      "✓ Already exists: eStzDzEopDI\n",
      "✓ Already exists: eW8se7t0s-U\n",
      "✓ Already exists: eWwWwoQLtVg\n",
      "✓ Already exists: eXWBC3XfiXY\n",
      "✓ Already exists: eXrJL1VUQNE\n",
      "✓ Already exists: eYngZ5It0b8\n",
      "✓ Already exists: eZE0RmJESFU\n",
      "✓ Already exists: eZNnuRvrZDU\n",
      "✓ Already exists: e_W17jp40G4\n",
      "✓ Already exists: efTVnvwI2PQ\n",
      "✓ Already exists: eiFyXXqd9Rk\n",
      "✓ Already exists: eiUjc4UPnSs\n",
      "✓ Already exists: esIzFH7vYLY\n",
      "✓ Already exists: euAQCWBX6ns\n",
      "✓ Already exists: evscfdO-oSY\n",
      "✓ Already exists: f3l6KnC8930\n",
      "✓ Already exists: f8nysknTFUo\n",
      "✓ Already exists: fEfe8jznp5Q\n",
      "✓ Already exists: fH9CY48sfJY\n",
      "✓ Already exists: fHNAxa0QaOM\n",
      "✓ Already exists: fPYeqTFc3IQ\n",
      "✓ Already exists: fWypK9RHJJI\n",
      "✓ Already exists: fX8A5Uxc8R0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] fZyq2pM2-dI: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading fZyq2pM2-dI: ERROR: [youtube] fZyq2pM2-dI: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
      "✗ Failed: fZyq2pM2-dI\n",
      "✓ Already exists: feC0L9MtghM\n",
      "✓ Already exists: fer_4HvG3aY\n",
      "✓ Already exists: fgCTFyzKQtk\n",
      "✓ Already exists: fhWzjWZqzvs\n",
      "✓ Already exists: fow1TC_MpHs\n",
      "✓ Already exists: fsTVRca31nI\n",
      "✓ Already exists: fsXfBoNcLeM\n",
      "✓ Already exists: ftaHv79hRoY\n",
      "✓ Already exists: fvw3Bi0GONA\n",
      "✓ Already exists: g0scnRzoo9M\n",
      "✓ Already exists: g4xhZgKwiNo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] g8USMvt9np0: We're processing this video. Check back later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading g8USMvt9np0: ERROR: [youtube] g8USMvt9np0: We're processing this video. Check back later.\n",
      "✗ Failed: g8USMvt9np0\n",
      "✓ Already exists: gAURHUoIK0M\n",
      "✓ Already exists: gBuLpP4klvI\n",
      "✓ Already exists: gDm4IphrlYg\n",
      "✓ Already exists: gDnJoHpSL4M\n",
      "✓ Already exists: gDzi8N3BYMw\n",
      "✓ Already exists: gEvCUcZ6w88\n",
      "✓ Already exists: gFxLnprPgv4\n",
      "✓ Already exists: gRn6OjQf2ZQ\n",
      "✓ Already exists: gWRfk8nCcPs\n",
      "✓ Already exists: gXOyw8a4_Xs\n",
      "✓ Already exists: g_bgmnJ1b_g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] gdtw54I8soM: Video unavailable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading gdtw54I8soM: ERROR: [youtube] gdtw54I8soM: Video unavailable\n",
      "✗ Failed: gdtw54I8soM\n",
      "✓ Already exists: giPa2vVEyVc\n",
      "✓ Already exists: gjJWbtCShqo\n",
      "✓ Already exists: gsBXngKgy-Q\n",
      "✓ Already exists: guRyU4B5LlA\n",
      "✓ Already exists: guYWKdxrtIg\n",
      "✓ Already exists: gxzU5EqNL14\n",
      "✓ Already exists: h0-6U948u7Y\n",
      "✓ Already exists: h8JS_FEF_fY\n",
      "✓ Already exists: hDsA_ky9Hfw\n",
      "✓ Already exists: hDzmNYd_eaA\n",
      "✓ Already exists: hFj0KUzofNg\n",
      "✓ Already exists: hFqZZrj0rnM\n",
      "✓ Already exists: hQ5OBio4Cy0\n",
      "✓ Already exists: hRbukCd6N68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] hTAWbHXCJ2A: Sign in to confirm your age. This video may be inappropriate for some users. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading hTAWbHXCJ2A: ERROR: [youtube] hTAWbHXCJ2A: Sign in to confirm your age. This video may be inappropriate for some users. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies\n",
      "✗ Failed: hTAWbHXCJ2A\n",
      "✓ Already exists: hTNKYJ6suII\n",
      "✓ Already exists: hUcuXIvDN2E\n",
      "✓ Already exists: hVPQu1UJ2N8\n",
      "✓ Already exists: hgitRq_0410\n",
      "✓ Already exists: hlquKjPgxmY\n",
      "✓ Already exists: hpiFoinUgvY\n",
      "✓ Already exists: hqQvatf1RUY\n",
      "✓ Already exists: hu6sChY-Yps\n",
      "✓ Already exists: i6WtNBpRll0\n",
      "✓ Already exists: i6k1yiyO5jQ\n",
      "✓ Already exists: iBH5X5SKirU\n",
      "✓ Already exists: iBezxlI_f_c\n",
      "✓ Already exists: iCIa_pmLDqs\n",
      "✓ Already exists: iEMTTKA7NxU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] iEQwupwwp0s: The uploader has not made this video available in your country\n",
      "This video is available in United Arab Emirates, Afghanistan, Albania, Armenia, Angola, Antarctica, Argentina, American Samoa, Australia, Aruba, Åland Islands, Azerbaijan, Bosnia and Herzegovina, Bangladesh, Burkina Faso, Bulgaria, Bahrain, Benin, Bermuda, Brunei Darussalam, Bolivia, Plurinational State of, Bonaire, Sint Eustatius and Saba, Brazil, Bhutan, Bouvet Island, Botswana, Belarus, Belize, Cocos (Keeling) Islands, Congo, the Democratic Republic of the, Central African Republic, Congo, Côte d'Ivoire, Cook Islands, Chile, Cameroon, China, Colombia, Costa Rica, Cape Verde, Curaçao, Christmas Island, Cyprus, Czech Republic, Denmark, Ecuador, Estonia, Western Sahara, Spain, Finland, Fiji, Falkland Islands (Malvinas), Micronesia, Federated States of, Faroe Islands, Gabon, Georgia, French Guiana, Guernsey, Ghana, Gibraltar, Greenland, Gambia, Guinea, Guadeloupe, Equatorial Guinea, Greece, South Georgia and the South Sandwich Islands, Guatemala, Guam, Guinea-Bissau, Guyana, Hong Kong, Heard Island and McDonald Islands, Honduras, Croatia, Hungary, Indonesia, Israel, Isle of Man, India, British Indian Ocean Territory, Iraq, Iran, Islamic Republic of, Iceland, Italy, Jersey, Jordan, Japan, Kyrgyzstan, Cambodia, Kiribati, Korea, Democratic People's Republic of, Korea, Republic of, Kuwait, Cayman Islands, Kazakhstan, Lao People's Democratic Republic, Lebanon, Sri Lanka, Liberia, Lesotho, Lithuania, Latvia, Moldova, Republic of, Montenegro, Marshall Islands, Macedonia, the Former Yugoslav Republic of, Mali, Myanmar, Mongolia, Macao, Northern Mariana Islands, Malta, Maldives, Mexico, Malaysia, Namibia, New Caledonia, Niger, Norfolk Island, Nigeria, Nicaragua, Norway, Nepal, Nauru, Niue, New Zealand, Oman, Panama, Peru, French Polynesia, Papua New Guinea, Philippines, Pakistan, Poland, Saint Pierre and Miquelon, Pitcairn, Palestine, State of, Palau, Paraguay, Qatar, Romania, Serbia, Russian Federation, Saudi Arabia, Solomon Islands, Sweden, Singapore, Slovenia, Svalbard and Jan Mayen, Slovakia, Sierra Leone, San Marino, Senegal, Suriname, Sao Tome and Principe, El Salvador, Sint Maarten (Dutch part), Syrian Arab Republic, Swaziland, Chad, French Southern Territories, Togo, Thailand, Tajikistan, Tokelau, Timor-Leste, Turkmenistan, Tonga, Turkey, Tuvalu, Taiwan, Province of China, Ukraine, United States Minor Outlying Islands, Uruguay, Uzbekistan, Holy See (Vatican City State), Venezuela, Bolivarian Republic of, Viet Nam, Vanuatu, Wallis and Futuna, Samoa, Yemen, Mayotte, South Africa.\n",
      "You might want to use a VPN or a proxy server (with --proxy) to workaround.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading iEQwupwwp0s: ERROR: [youtube] iEQwupwwp0s: The uploader has not made this video available in your country\n",
      "This video is available in United Arab Emirates, Afghanistan, Albania, Armenia, Angola, Antarctica, Argentina, American Samoa, Australia, Aruba, Åland Islands, Azerbaijan, Bosnia and Herzegovina, Bangladesh, Burkina Faso, Bulgaria, Bahrain, Benin, Bermuda, Brunei Darussalam, Bolivia, Plurinational State of, Bonaire, Sint Eustatius and Saba, Brazil, Bhutan, Bouvet Island, Botswana, Belarus, Belize, Cocos (Keeling) Islands, Congo, the Democratic Republic of the, Central African Republic, Congo, Côte d'Ivoire, Cook Islands, Chile, Cameroon, China, Colombia, Costa Rica, Cape Verde, Curaçao, Christmas Island, Cyprus, Czech Republic, Denmark, Ecuador, Estonia, Western Sahara, Spain, Finland, Fiji, Falkland Islands (Malvinas), Micronesia, Federated States of, Faroe Islands, Gabon, Georgia, French Guiana, Guernsey, Ghana, Gibraltar, Greenland, Gambia, Guinea, Guadeloupe, Equatorial Guinea, Greece, South Georgia and the South Sandwich Islands, Guatemala, Guam, Guinea-Bissau, Guyana, Hong Kong, Heard Island and McDonald Islands, Honduras, Croatia, Hungary, Indonesia, Israel, Isle of Man, India, British Indian Ocean Territory, Iraq, Iran, Islamic Republic of, Iceland, Italy, Jersey, Jordan, Japan, Kyrgyzstan, Cambodia, Kiribati, Korea, Democratic People's Republic of, Korea, Republic of, Kuwait, Cayman Islands, Kazakhstan, Lao People's Democratic Republic, Lebanon, Sri Lanka, Liberia, Lesotho, Lithuania, Latvia, Moldova, Republic of, Montenegro, Marshall Islands, Macedonia, the Former Yugoslav Republic of, Mali, Myanmar, Mongolia, Macao, Northern Mariana Islands, Malta, Maldives, Mexico, Malaysia, Namibia, New Caledonia, Niger, Norfolk Island, Nigeria, Nicaragua, Norway, Nepal, Nauru, Niue, New Zealand, Oman, Panama, Peru, French Polynesia, Papua New Guinea, Philippines, Pakistan, Poland, Saint Pierre and Miquelon, Pitcairn, Palestine, State of, Palau, Paraguay, Qatar, Romania, Serbia, Russian Federation, Saudi Arabia, Solomon Islands, Sweden, Singapore, Slovenia, Svalbard and Jan Mayen, Slovakia, Sierra Leone, San Marino, Senegal, Suriname, Sao Tome and Principe, El Salvador, Sint Maarten (Dutch part), Syrian Arab Republic, Swaziland, Chad, French Southern Territories, Togo, Thailand, Tajikistan, Tokelau, Timor-Leste, Turkmenistan, Tonga, Turkey, Tuvalu, Taiwan, Province of China, Ukraine, United States Minor Outlying Islands, Uruguay, Uzbekistan, Holy See (Vatican City State), Venezuela, Bolivarian Republic of, Viet Nam, Vanuatu, Wallis and Futuna, Samoa, Yemen, Mayotte, South Africa.\n",
      "You might want to use a VPN or a proxy server (with --proxy) to workaround.\n",
      "✗ Failed: iEQwupwwp0s\n",
      "✓ Already exists: iFSaNmZyPQo\n",
      "✓ Already exists: iMmYVLSb1IY\n",
      "✓ Already exists: iQ7qeIrssds\n",
      "✓ Already exists: iQfPmJ19ZUc\n",
      "✓ Already exists: iS8YQGp2_ng\n",
      "✓ Already exists: iUHqyjf3NcQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] iXgEQj1Fs7g: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading iXgEQj1Fs7g: ERROR: [youtube] iXgEQj1Fs7g: Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.\n",
      "✗ Failed: iXgEQj1Fs7g\n",
      "✓ Already exists: iZjIuV_cTe8\n",
      "\n",
      "=== Download Summary ===\n",
      "Successfully downloaded: 477\n",
      "Failed: 23\n",
      "Successfully downloaded: 477\n",
      "Failed: 23\n",
      "\n",
      "Creating archetype training data...\n",
      "\n",
      "=== Archetype Distribution Statistics ===\n",
      "\n",
      "SINE:\n",
      "  Mean weight: 0.431\n",
      "  Std dev: 0.392\n",
      "  Max weight: 1.000\n",
      "  % samples with weight > 0.3: 48.6%\n",
      "\n",
      "SQUARE:\n",
      "  Mean weight: 0.138\n",
      "  Std dev: 0.253\n",
      "  Max weight: 1.000\n",
      "  % samples with weight > 0.3: 15.1%\n",
      "\n",
      "SAWTOOTH:\n",
      "  Mean weight: 0.198\n",
      "  Std dev: 0.311\n",
      "  Max weight: 1.000\n",
      "  % samples with weight > 0.3: 21.6%\n",
      "\n",
      "TRIANGLE:\n",
      "  Mean weight: 0.144\n",
      "  Std dev: 0.248\n",
      "  Max weight: 1.000\n",
      "  % samples with weight > 0.3: 15.1%\n",
      "\n",
      "NOISE:\n",
      "  Mean weight: 0.088\n",
      "  Std dev: 0.187\n",
      "  Max weight: 1.000\n",
      "  % samples with weight > 0.3: 8.8%\n",
      "\n",
      "=== Examples with Dominant Archetypes ===\n",
      "\n",
      "SINE dominant (1.00):\n",
      "  Description: A male vocalist sings this soft love song. The tempo is slow with a romantic piano accompaniment, gr...\n",
      "  Vector: ['1.00', '0.00', '0.00', '0.00', '0.00']\n",
      "\n",
      "SQUARE dominant (1.00):\n",
      "  Description: Spirited Christmas music with a fast four on the floor dance beat. There is a high pitched 'chipmunk...\n",
      "  Vector: ['0.00', '1.00', '0.00', '0.00', '0.00']\n",
      "\n",
      "SAWTOOTH dominant (1.00):\n",
      "  Description: A male singer sings this beautiful melody with backup singers in vocal harmony. The song is medium t...\n",
      "  Vector: ['0.00', '0.00', '1.00', '0.00', '0.00']\n",
      "\n",
      "TRIANGLE dominant (1.00):\n",
      "  Description: The R&B music features a male voice singing and being backed by similar male voices from time to tim...\n",
      "  Vector: ['0.00', '0.00', '0.00', '1.00', '0.00']\n",
      "\n",
      "NOISE dominant (1.00):\n",
      "  Description: This is an opera song that's recorded at a very low quality. The vocal audio is distorted and create...\n",
      "  Vector: ['0.00', '0.00', '0.00', '0.00', '1.00']\n",
      "\n",
      "Dataset split:\n",
      "  Train: 333 samples (70.0%)\n",
      "  Val:   71 samples (15.0%)\n",
      "  Test:  73 samples (15.0%)\n",
      "\n",
      "✓ Saved train set: musiccaps_training_data_train.npz\n",
      "✓ Saved val set: musiccaps_training_data_val.npz\n",
      "✓ Saved test set: musiccaps_training_data_test.npz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare dataset (run once)\n",
    "print(\"Step 1: Preparing MusicCaps dataset with train/val/test split...\")\n",
    "train_data_path, val_data_path, test_data_path = prepare_musiccaps_data(\n",
    "    csv_path=csv_path,\n",
    "    audio_dir='musiccaps_audio',\n",
    "    max_downloads=config['max_downloads'],\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    test_split=0.15,\n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Creating PyTorch datasets...\n",
      "Loading training data from musiccaps_training_data_train.npz\n",
      "Loaded 333 training examples\n",
      "Found 333 samples with valid audio files\n",
      "Loading training data from musiccaps_training_data_val.npz\n",
      "Loaded 71 training examples\n",
      "Found 71 samples with valid audio files\n",
      "Loading training data from musiccaps_training_data_test.npz\n",
      "Loaded 73 training examples\n",
      "Found 73 samples with valid audio files\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create datasets\n",
    "print(\"\\nStep 2: Creating PyTorch datasets...\")\n",
    "train_dataset = MusicCapsDataset(\n",
    "    train_data_path,\n",
    "    sample_rate=config['sample_rate'],\n",
    "    audio_duration=config['audio_duration'],\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = MusicCapsDataset(\n",
    "    val_data_path,\n",
    "    sample_rate=config['sample_rate'],\n",
    "    audio_duration=config['audio_duration'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = MusicCapsDataset(\n",
    "    test_data_path,\n",
    "    sample_rate=config['sample_rate'],\n",
    "    audio_duration=config['audio_duration'],\n",
    "    augment=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Reload the modules to pick up changes\n",
    "import importlib\n",
    "import text_tower\n",
    "import lstmabar_model\n",
    "\n",
    "importlib.reload(text_tower)\n",
    "importlib.reload(lstmabar_model)\n",
    "\n",
    "# Re-import the classes\n",
    "from text_tower import TextEncoder\n",
    "from lstmabar_model import LSTMABAR, LSTMABARTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Initializing LSTMABAR model...\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Model has 37,203,724 parameters\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize model\n",
    "print(\"\\nStep 3: Initializing LSTMABAR model...\")\n",
    "model = LSTMABAR(\n",
    "    embedding_dim=config['embedding_dim'],\n",
    "    audio_architecture=config['audio_architecture'],\n",
    "    sample_rate=config['sample_rate'],\n",
    "    use_quantum_attention=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Setting up training pipeline...\n",
      "Training pipeline initialized:\n",
      "  Training samples: 333\n",
      "  Validation samples: 71\n",
      "  Batch size: 16\n",
      "  Total epochs: 20\n",
      "  Steps per epoch: 21\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create training pipeline\n",
    "print(\"\\nStep 4: Setting up training pipeline...\")\n",
    "pipeline = TrainingPipeline(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    num_epochs=config['num_epochs'],\n",
    "    checkpoint_dir='checkpoints'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Starting training...\n",
      "\n",
      "============================================================\n",
      "Epoch 1/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanthgopalswamy/Desktop/mids266_final/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0/21: Loss=1.9718, Contrastive=2.7729, Archetype=0.1024\n",
      "Epoch 0, Batch 10/21: Loss=1.9569, Contrastive=2.7576, Archetype=0.0877\n",
      "Epoch 0, Batch 20/21: Loss=1.8003, Contrastive=2.5298, Archetype=0.0973\n",
      "\n",
      "Train Losses: {'total': np.float64(1.9567998023260207), 'contrastive': np.float64(2.751888933635893), 'archetype': np.float64(0.10551349180085319)}\n",
      "Val Losses: {'total': 1.8490639925003052, 'contrastive': 2.6003254413604737, 'archetype': 0.09782975912094116}\n",
      "Checkpoint saved to checkpoints/best_model.pth\n",
      "✓ Best model saved (val_loss: 1.8491)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/20\n",
      "============================================================\n",
      "Epoch 1, Batch 0/21: Loss=1.8529, Contrastive=2.6029, Archetype=0.1075\n",
      "Epoch 1, Batch 10/21: Loss=1.8889, Contrastive=2.6589, Archetype=0.0963\n",
      "Epoch 1, Batch 20/21: Loss=1.7524, Contrastive=2.4618, Archetype=0.0992\n",
      "\n",
      "Train Losses: {'total': np.float64(1.8320271628243583), 'contrastive': np.float64(2.5745021729242232), 'archetype': np.float64(0.10390859984216236)}\n",
      "Val Losses: {'total': 1.7285382747650146, 'contrastive': 2.4283926010131838, 'archetype': 0.09792872071266175}\n",
      "Checkpoint saved to checkpoints/best_model.pth\n",
      "✓ Best model saved (val_loss: 1.7285)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/20\n",
      "============================================================\n",
      "Epoch 2, Batch 0/21: Loss=1.8875, Contrastive=2.6582, Archetype=0.0955\n",
      "Epoch 2, Batch 10/21: Loss=1.6883, Contrastive=2.3765, Archetype=0.0791\n",
      "Epoch 2, Batch 20/21: Loss=1.5800, Contrastive=2.2302, Archetype=0.0614\n",
      "\n",
      "Train Losses: {'total': np.float64(1.6441268864132108), 'contrastive': np.float64(2.3086560453687395), 'archetype': np.float64(0.09590596084793408)}\n",
      "Val Losses: {'total': 1.7671653032302856, 'contrastive': 2.4852622032165526, 'archetype': 0.09262886345386505}\n",
      "\n",
      "============================================================\n",
      "Epoch 4/20\n",
      "============================================================\n",
      "Epoch 3, Batch 0/21: Loss=1.4703, Contrastive=2.0665, Archetype=0.0793\n",
      "Epoch 3, Batch 10/21: Loss=1.3881, Contrastive=1.9458, Archetype=0.0920\n",
      "Epoch 3, Batch 20/21: Loss=1.1485, Contrastive=1.6012, Archetype=0.0923\n",
      "\n",
      "Train Losses: {'total': np.float64(1.4662928865069436), 'contrastive': np.float64(2.0555624734787714), 'archetype': np.float64(0.09277961474089395)}\n",
      "Val Losses: {'total': 1.8098636150360108, 'contrastive': 2.547985887527466, 'archetype': 0.08652157038450241}\n",
      "\n",
      "============================================================\n",
      "Epoch 5/20\n",
      "============================================================\n",
      "Epoch 4, Batch 0/21: Loss=1.1074, Contrastive=1.5532, Archetype=0.0688\n",
      "Epoch 4, Batch 10/21: Loss=1.0083, Contrastive=1.3935, Archetype=0.1073\n",
      "Epoch 4, Batch 20/21: Loss=0.9652, Contrastive=1.3576, Archetype=0.0472\n",
      "\n",
      "Train Losses: {'total': np.float64(1.071988508814857), 'contrastive': np.float64(1.493241242000035), 'archetype': np.float64(0.089173954334997)}\n",
      "Val Losses: {'total': 1.8650737285614014, 'contrastive': 2.626994323730469, 'archetype': 0.08542696088552475}\n",
      "\n",
      "============================================================\n",
      "Epoch 6/20\n",
      "============================================================\n",
      "Epoch 5, Batch 0/21: Loss=0.9464, Contrastive=1.3102, Archetype=0.0979\n",
      "Epoch 5, Batch 10/21: Loss=0.7494, Contrastive=1.0244, Archetype=0.1106\n",
      "Epoch 5, Batch 20/21: Loss=0.5809, Contrastive=0.7940, Archetype=0.0803\n",
      "\n",
      "Train Losses: {'total': np.float64(0.7849996771131244), 'contrastive': np.float64(1.0831595687639146), 'archetype': np.float64(0.08829885685727709)}\n",
      "Val Losses: {'total': 1.9443664073944091, 'contrastive': 2.7402496337890625, 'archetype': 0.08502984941005706}\n",
      "\n",
      "============================================================\n",
      "Epoch 7/20\n",
      "============================================================\n",
      "Epoch 6, Batch 0/21: Loss=0.6199, Contrastive=0.8604, Archetype=0.0526\n",
      "Epoch 6, Batch 10/21: Loss=0.4642, Contrastive=0.6148, Archetype=0.1089\n",
      "Epoch 6, Batch 20/21: Loss=1.0659, Contrastive=1.4774, Archetype=0.1074\n",
      "\n",
      "Train Losses: {'total': np.float64(0.6457465432939076), 'contrastive': np.float64(0.8841689200628371), 'archetype': np.float64(0.08809037258227666)}\n",
      "Val Losses: {'total': 1.951067566871643, 'contrastive': 2.7489341735839843, 'archetype': 0.0882804587483406}\n",
      "\n",
      "============================================================\n",
      "Epoch 8/20\n",
      "============================================================\n",
      "Epoch 7, Batch 0/21: Loss=0.3124, Contrastive=0.4047, Archetype=0.0978\n",
      "Epoch 7, Batch 10/21: Loss=0.4318, Contrastive=0.5713, Archetype=0.1083\n",
      "Epoch 7, Batch 20/21: Loss=0.5007, Contrastive=0.6788, Archetype=0.0766\n",
      "\n",
      "Train Losses: {'total': np.float64(0.4867564950670515), 'contrastive': np.float64(0.6586377322673798), 'archetype': np.float64(0.08274186952483087)}\n",
      "Val Losses: {'total': 2.1342600107192995, 'contrastive': 3.011263036727905, 'archetype': 0.0862720787525177}\n",
      "\n",
      "============================================================\n",
      "Epoch 9/20\n",
      "============================================================\n",
      "Epoch 8, Batch 0/21: Loss=0.4503, Contrastive=0.6033, Archetype=0.0913\n",
      "Epoch 8, Batch 10/21: Loss=0.3486, Contrastive=0.4611, Archetype=0.0881\n",
      "Epoch 8, Batch 20/21: Loss=0.4386, Contrastive=0.5893, Archetype=0.0743\n",
      "\n",
      "Train Losses: {'total': np.float64(0.37370737748486654), 'contrastive': np.float64(0.4982282136167799), 'archetype': np.float64(0.07907694595910254)}\n",
      "Val Losses: {'total': 2.025299859046936, 'contrastive': 2.854704570770264, 'archetype': 0.0891752615571022}\n",
      "\n",
      "============================================================\n",
      "Epoch 10/20\n",
      "============================================================\n",
      "Epoch 9, Batch 0/21: Loss=0.1477, Contrastive=0.1723, Archetype=0.0845\n",
      "Epoch 9, Batch 10/21: Loss=0.2682, Contrastive=0.3451, Archetype=0.0917\n",
      "Epoch 9, Batch 20/21: Loss=0.3324, Contrastive=0.4451, Archetype=0.0607\n",
      "\n",
      "Train Losses: {'total': np.float64(0.2903571838424319), 'contrastive': np.float64(0.37893401370162055), 'archetype': np.float64(0.08030859718010538)}\n",
      "Val Losses: {'total': 1.9633165836334228, 'contrastive': 2.767954397201538, 'archetype': 0.08356543481349946}\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_10.pth\n",
      "\n",
      "============================================================\n",
      "Epoch 11/20\n",
      "============================================================\n",
      "Epoch 10, Batch 0/21: Loss=0.1454, Contrastive=0.1742, Archetype=0.0716\n",
      "Epoch 10, Batch 10/21: Loss=0.1705, Contrastive=0.2084, Archetype=0.0814\n",
      "Epoch 10, Batch 20/21: Loss=0.2202, Contrastive=0.2861, Archetype=0.0620\n",
      "\n",
      "Train Losses: {'total': np.float64(0.23208985726038614), 'contrastive': np.float64(0.2972749528430757), 'archetype': np.float64(0.07544186214605968)}\n",
      "Val Losses: {'total': 2.125776529312134, 'contrastive': 2.9988263130187987, 'archetype': 0.08792966902256012}\n",
      "\n",
      "============================================================\n",
      "Epoch 12/20\n",
      "============================================================\n",
      "Epoch 11, Batch 0/21: Loss=0.1636, Contrastive=0.2030, Archetype=0.0714\n",
      "Epoch 11, Batch 10/21: Loss=0.1809, Contrastive=0.2294, Archetype=0.0652\n",
      "Epoch 11, Batch 20/21: Loss=0.0833, Contrastive=0.0825, Archetype=0.0851\n",
      "\n",
      "Train Losses: {'total': np.float64(0.22221032423632486), 'contrastive': np.float64(0.28475796644176754), 'archetype': np.float64(0.07037594807999474)}\n",
      "Val Losses: {'total': 1.9802996397018433, 'contrastive': 2.7909367084503174, 'archetype': 0.08870237469673156}\n",
      "\n",
      "============================================================\n",
      "Epoch 13/20\n",
      "============================================================\n",
      "Epoch 12, Batch 0/21: Loss=0.1517, Contrastive=0.1888, Archetype=0.0581\n",
      "Epoch 12, Batch 10/21: Loss=0.3067, Contrastive=0.4171, Archetype=0.0463\n",
      "Epoch 12, Batch 20/21: Loss=0.1231, Contrastive=0.1324, Archetype=0.0908\n",
      "\n",
      "Train Losses: {'total': np.float64(0.1925600387510799), 'contrastive': np.float64(0.2415518214305242), 'archetype': np.float64(0.0736263490148953)}\n",
      "Val Losses: {'total': 1.967839241027832, 'contrastive': 2.7730549812316894, 'archetype': 0.08887580335140229}\n",
      "\n",
      "============================================================\n",
      "Epoch 14/20\n",
      "============================================================\n",
      "Epoch 13, Batch 0/21: Loss=0.1489, Contrastive=0.1821, Archetype=0.0635\n",
      "Epoch 13, Batch 10/21: Loss=0.1296, Contrastive=0.1468, Archetype=0.0867\n",
      "Epoch 13, Batch 20/21: Loss=0.1064, Contrastive=0.1322, Archetype=0.0434\n",
      "\n",
      "Train Losses: {'total': np.float64(0.14353628988776887), 'contrastive': np.float64(0.17472361524899802), 'archetype': np.float64(0.06354333176499322)}\n",
      "Val Losses: {'total': 2.202607536315918, 'contrastive': 3.112315559387207, 'archetype': 0.076808400452137}\n",
      "\n",
      "============================================================\n",
      "Epoch 15/20\n",
      "============================================================\n",
      "Epoch 14, Batch 0/21: Loss=0.1564, Contrastive=0.2015, Archetype=0.0443\n",
      "Epoch 14, Batch 10/21: Loss=0.1073, Contrastive=0.1167, Archetype=0.0828\n",
      "Epoch 14, Batch 20/21: Loss=0.1322, Contrastive=0.1663, Archetype=0.0380\n",
      "\n",
      "Train Losses: {'total': np.float64(0.12270418341670718), 'contrastive': np.float64(0.14519210691962922), 'archetype': np.float64(0.06341625288838432)}\n",
      "Val Losses: {'total': 2.085692596435547, 'contrastive': 2.9443249702453613, 'archetype': 0.08033237829804421}\n",
      "\n",
      "============================================================\n",
      "Epoch 16/20\n",
      "============================================================\n",
      "Epoch 15, Batch 0/21: Loss=0.0973, Contrastive=0.1044, Archetype=0.0716\n",
      "Epoch 15, Batch 10/21: Loss=0.0748, Contrastive=0.0803, Archetype=0.0591\n",
      "Epoch 15, Batch 20/21: Loss=0.0481, Contrastive=0.0391, Archetype=0.0562\n",
      "\n",
      "Train Losses: {'total': np.float64(0.12518208012694404), 'contrastive': np.float64(0.14900965403233254), 'archetype': np.float64(0.06298475925411497)}\n",
      "Val Losses: {'total': 2.023099994659424, 'contrastive': 2.8545027494430544, 'archetype': 0.08148838728666305}\n",
      "\n",
      "============================================================\n",
      "Epoch 17/20\n",
      "============================================================\n",
      "Epoch 16, Batch 0/21: Loss=0.0821, Contrastive=0.0830, Archetype=0.0596\n",
      "Epoch 16, Batch 10/21: Loss=0.1008, Contrastive=0.1203, Archetype=0.0509\n",
      "Epoch 16, Batch 20/21: Loss=0.1103, Contrastive=0.1312, Archetype=0.0512\n",
      "\n",
      "Train Losses: {'total': np.float64(0.10261728011426471), 'contrastive': np.float64(0.12015903545987039), 'archetype': np.float64(0.05214189693686508)}\n",
      "Val Losses: {'total': 2.1166744232177734, 'contrastive': 2.987785530090332, 'archetype': 0.0826970636844635}\n",
      "\n",
      "============================================================\n",
      "Epoch 18/20\n",
      "============================================================\n",
      "Epoch 17, Batch 0/21: Loss=0.0633, Contrastive=0.0610, Archetype=0.0590\n",
      "Epoch 17, Batch 10/21: Loss=0.0811, Contrastive=0.0915, Archetype=0.0487\n",
      "Epoch 17, Batch 20/21: Loss=0.0536, Contrastive=0.0503, Archetype=0.0547\n",
      "\n",
      "Train Losses: {'total': np.float64(0.07368424339663415), 'contrastive': np.float64(0.0788341916742779), 'archetype': np.float64(0.05300909812961306)}\n",
      "Val Losses: {'total': 2.3323944330215456, 'contrastive': 3.295721483230591, 'archetype': 0.0838820293545723}\n",
      "\n",
      "============================================================\n",
      "Epoch 19/20\n",
      "============================================================\n",
      "Epoch 18, Batch 0/21: Loss=0.0466, Contrastive=0.0451, Archetype=0.0444\n",
      "Epoch 18, Batch 10/21: Loss=0.1091, Contrastive=0.1201, Archetype=0.0749\n",
      "Epoch 18, Batch 20/21: Loss=0.0403, Contrastive=0.0295, Archetype=0.0570\n",
      "\n",
      "Train Losses: {'total': np.float64(0.06868341778005872), 'contrastive': np.float64(0.0722374554191317), 'archetype': np.float64(0.0518262145065126)}\n",
      "Val Losses: {'total': 2.1975950241088866, 'contrastive': 3.1035458564758303, 'archetype': 0.0825897917151451}\n",
      "\n",
      "============================================================\n",
      "Epoch 20/20\n",
      "============================================================\n",
      "Epoch 19, Batch 0/21: Loss=0.0846, Contrastive=0.1007, Archetype=0.0415\n",
      "Epoch 19, Batch 10/21: Loss=0.0624, Contrastive=0.0661, Archetype=0.0467\n",
      "Epoch 19, Batch 20/21: Loss=0.0326, Contrastive=0.0258, Archetype=0.0407\n",
      "\n",
      "Train Losses: {'total': np.float64(0.06692692442309289), 'contrastive': np.float64(0.07113937270783242), 'archetype': np.float64(0.048598572789203556)}\n",
      "Val Losses: {'total': 2.2904500484466555, 'contrastive': 3.237202835083008, 'archetype': 0.08024711459875107}\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_20.pth\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "============================================================\n",
      "Checkpoint saved to checkpoints/final_model.pth\n",
      "Training history plot saved to checkpoints/training_history.png\n",
      "\n",
      "✓ Training complete! Model checkpoints saved to checkpoints/\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train!\n",
    "print(\"\\nStep 5: Starting training...\")\n",
    "pipeline.train()\n",
    "\n",
    "print(\"\\n✓ Training complete! Model checkpoints saved to checkpoints/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
