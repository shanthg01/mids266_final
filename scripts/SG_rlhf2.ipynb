{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model location: checkpoints/quantum_8qubit_depth3_high_reg_best.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a335cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "from ipywidgets import interact, IntSlider, Button, Output, VBox, HBox, Label\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import your scripts\n",
    "from lstmabar_model import LSTMABAR\n",
    "from macro_archetype_predictor import MacroArchetypePredictor\n",
    "from parameter_decoder import ParameterDecoder\n",
    "from harmonic_ddsp_engine import HarmonicDDSPEngine\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36dee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoints/quantum_8qubit_depth3_high_reg_best.pth\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "ðŸ§  Initializing Macro Archetype Predictor...\n",
      "Translation Layer: Parameter Decoder...\n",
      "ðŸŽ¹ Initializing Harmonic DDSP Engine...\n",
      "âœ“ Backbone loaded and frozen. Generator ready for training.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "CHECKPOINT_PATH = 'checkpoints/quantum_8qubit_depth3_high_reg_best.pth'\n",
    "\n",
    "def load_pretrained_model(checkpoint_path):\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    config = checkpoint['config']\n",
    "    \n",
    "    # Initialize the Full Model\n",
    "    model = LSTMABAR(\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        text_model='sentence-transformers/all-MiniLM-L6-v2',\n",
    "        audio_architecture=config['audio_architecture'],\n",
    "        sample_rate=44100,\n",
    "        use_quantum_attention=config['use_quantum_attention'],\n",
    "        n_qubits=config['n_qubits'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Load Weights\n",
    "    # Note: The checkpoint only has weights for Text/Audio/Aligner.\n",
    "    # The Predictor/Decoder/Engine will remain random (initialized fresh).\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    \n",
    "    # Freeze the Backbone (We only want to train the Generator now)\n",
    "    for param in model.text_encoder.parameters(): param.requires_grad = False\n",
    "    for param in model.audio_encoder.parameters(): param.requires_grad = False\n",
    "    for param in model.contrastive_aligner.parameters(): param.requires_grad = False\n",
    "    \n",
    "    print(\"âœ“ Backbone loaded and frozen. Generator ready for training.\")\n",
    "    return model, config\n",
    "\n",
    "model, config = load_pretrained_model(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8848e8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated 15 synthetic samples.\n"
     ]
    }
   ],
   "source": [
    "# --- GENERATE WARM-START METADATA ---\n",
    "import soundfile as sf\n",
    "\n",
    "DATASET_DIR = Path(\"synthetic_dynamic_dataset\")\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "SR = 44100\n",
    "\n",
    "# 1. Physics Rules (4 Timbre + 5 Dynamics)\n",
    "ARCHETYPE_MAP = {\n",
    "    \"pure\":      [1.0, 0.0, 0.0, 0.0],\n",
    "    \"bright\":    [0.0, 0.0, 0.8, 0.2], \n",
    "    \"harsh\":     [0.0, 0.5, 0.5, 0.0], \n",
    "    \"warm\":      [0.6, 0.0, 0.0, 0.4],\n",
    "    \"hollow\":    [0.0, 0.9, 0.0, 0.1]\n",
    "}\n",
    "\n",
    "def get_dynamics(text):\n",
    "    # A, D, S, R, Cutoff\n",
    "    params = [0.1, 0.1, 1.0, 0.1, 1.0] \n",
    "    if \"pluck\" in text: params = [0.01, 0.2, 0.0, 0.2, 1.0]\n",
    "    elif \"pad\" in text: params = [0.5, 0.0, 1.0, 0.5, 0.6]\n",
    "    return params\n",
    "\n",
    "data_records = []\n",
    "for archetype, weights in ARCHETYPE_MAP.items():\n",
    "    prompts = [f\"{archetype} tone\", f\"{archetype} pad\", f\"{archetype} pluck\"]\n",
    "    for text in prompts:\n",
    "        dyn = get_dynamics(text)\n",
    "        \n",
    "        # Dummy audio file (needed for AudioEncoder input)\n",
    "        # We generate silence because for Warm Start we map Text -> Params directly.\n",
    "        # The Audio context is less relevant here.\n",
    "        fname = f\"dyn_{archetype}_{text.split()[-1]}.wav\"\n",
    "        path = DATASET_DIR / fname\n",
    "        sf.write(path, np.zeros(SR), SR)\n",
    "        \n",
    "        data_records.append({\n",
    "            \"filepath\": str(path),\n",
    "            \"text\": text,\n",
    "            \"w_sine\": weights[0], \"w_sq\": weights[1], \"w_saw\": weights[2], \"w_tri\": weights[3],\n",
    "            \"a\": dyn[0], \"d\": dyn[1], \"s\": dyn[2], \"r\": dyn[3], \"cutoff\": dyn[4]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data_records)\n",
    "df.to_csv(\"synthetic_dynamic_dataset.csv\", index=False)\n",
    "print(f\"âœ“ Generated {len(df)} synthetic samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722bdb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Warm-starting Generator (Predictor + Decoder)...\n",
      "Epoch 10: Loss 0.0905\n",
      "Epoch 20: Loss 0.0850\n",
      "Epoch 30: Loss 0.0408\n",
      "Epoch 40: Loss 0.0147\n",
      "Epoch 50: Loss 0.0082\n",
      "âœ“ Warm start complete. Generator knows physics!\n"
     ]
    }
   ],
   "source": [
    "# --- SMART WARM START ---\n",
    "# Maps simple CSV weights -> Complex DDSP Parameters\n",
    "\n",
    "class SyntheticDDSPDataset(Dataset):\n",
    "    def __init__(self, csv_path, backbone_model, n_harmonics=64, device='cpu'):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.backbone = backbone_model\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.device = device\n",
    "        \n",
    "        # Pre-calculate Harmonic Series for basic waves\n",
    "        k = torch.arange(1, n_harmonics + 1).float()\n",
    "        self.saw_series = 1.0 / k\n",
    "        self.square_series = (1.0 / k) * (k % 2 != 0).float() # Odd harmonics only\n",
    "        self.tri_series = (1.0 / (k**2)) * (k % 2 != 0).float() # Odd, steeper dropoff\n",
    "        self.sine_series = torch.zeros(n_harmonics); self.sine_series[0] = 1.0\n",
    "        \n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Inputs\n",
    "        text = row['text']\n",
    "        # Dummy audio embedding (zeros)\n",
    "        audio_emb = torch.zeros(self.backbone.embedding_dim)\n",
    "        \n",
    "        # Pre-compute text embedding\n",
    "        with torch.no_grad():\n",
    "            # Use forward() call instead of .encode()\n",
    "            text_input = [text] \n",
    "            text_emb = self.backbone.text_encoder(text_input)\n",
    "            \n",
    "            if isinstance(text_emb, tuple): text_emb = text_emb[0]\n",
    "            if not isinstance(text_emb, torch.Tensor): text_emb = torch.tensor(text_emb)\n",
    "            \n",
    "            text_emb = text_emb.cpu().squeeze()\n",
    "\n",
    "        # 2. Build Targets (Complex Physics)\n",
    "        # Force Python Floats to avoid numpy.float64 pollution\n",
    "        w_sine = float(row['w_sine'])\n",
    "        w_sq = float(row['w_sq'])\n",
    "        w_saw = float(row['w_saw'])\n",
    "        w_tri = float(row['w_tri'])\n",
    "        \n",
    "        target_harmonics = (\n",
    "            w_sine * self.sine_series + \n",
    "            w_sq * self.square_series + \n",
    "            w_saw * self.saw_series + \n",
    "            w_tri * self.tri_series\n",
    "        )\n",
    "        # Normalize harmonics and FORCE FLOAT32\n",
    "        target_harmonics = target_harmonics / (target_harmonics.sum() + 1e-5)\n",
    "        target_harmonics = target_harmonics.float() \n",
    "        \n",
    "        # ADSR Target - FORCE FLOAT32\n",
    "        target_adsr = torch.tensor([row['a'], row['d'], row['s'], row['r']]).float()\n",
    "        \n",
    "        # Noise Target\n",
    "        target_noise = torch.ones(32) * 0.01 \n",
    "        if row['cutoff'] > 0.8: target_noise *= 5.0\n",
    "        target_noise = target_noise.float() # Ensure float32\n",
    "            \n",
    "        return text_emb, audio_emb, target_harmonics, target_noise, target_adsr\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "def run_smart_warm_start(model, epochs=50):\n",
    "    print(\"ðŸ”¥ Warm-starting Generator (Predictor + Decoder)...\")\n",
    "    dataset = SyntheticDDSPDataset(\"synthetic_dynamic_dataset.csv\", model, device='cpu')\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    # Train both Predictor and Decoder\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.predictor.parameters()},\n",
    "        {'params': model.decoder.parameters()}\n",
    "    ], lr=1e-3)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.predictor.train()\n",
    "    model.decoder.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for text_emb, audio_emb, t_harm, t_noise, t_adsr in loader:\n",
    "            # Move to device\n",
    "            text_emb, audio_emb = text_emb.to(device), audio_emb.to(device)\n",
    "            t_harm, t_noise, t_adsr = t_harm.to(device), t_noise.to(device), t_adsr.to(device)\n",
    "            \n",
    "            # Robust dimension check (squeeze 3D -> 2D)\n",
    "            if text_emb.dim() == 3: text_emb = text_emb.squeeze(1)\n",
    "            if audio_emb.dim() == 3: audio_emb = audio_emb.squeeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward: Predict -> Decode\n",
    "            macros = model.predictor(text_emb, audio_emb)\n",
    "            p_harm, p_noise, p_adsr, p_gain = model.decoder(macros['combined'])\n",
    "            \n",
    "            # Loss: Match targets\n",
    "            loss_harm = criterion(p_harm, t_harm)\n",
    "            loss_noise = criterion(p_noise, t_noise)\n",
    "            loss_adsr = criterion(p_adsr, t_adsr)\n",
    "            \n",
    "            loss = loss_harm + loss_noise + loss_adsr\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss {total_loss/len(loader):.4f}\")\n",
    "            \n",
    "    print(\"âœ“ Warm start complete. Generator knows physics!\")\n",
    "\n",
    "run_smart_warm_start(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eec9d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Trainer patched. Ready for feedback.\n"
     ]
    }
   ],
   "source": [
    "class RLHFTrainerMacro:\n",
    "    def __init__(self, model, learning_rate=1e-4):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam([\n",
    "            {'params': model.predictor.parameters()},\n",
    "            {'params': model.decoder.parameters()}\n",
    "        ], lr=learning_rate)\n",
    "        # Use a list as the buffer\n",
    "        self.experience_buffer = [] \n",
    "        \n",
    "    def _ensure_tensor(self, x):\n",
    "        \"\"\"Safely extract tensor from tuple/list if needed\"\"\"\n",
    "        if isinstance(x, (tuple, list)):\n",
    "            return x[0]\n",
    "        return x\n",
    "        \n",
    "    def add_feedback(self, text_emb, audio_emb, rating):\n",
    "        # 1. Unpack tuples if necessary (Robustness Fix)\n",
    "        text_emb = self._ensure_tensor(text_emb)\n",
    "        audio_emb = self._ensure_tensor(audio_emb)\n",
    "        \n",
    "        # 2. Convert 1-5 rating to reward (-1.0 to 1.0)\n",
    "        reward = (rating - 3) / 2.0\n",
    "        \n",
    "        # 3. Store DETACHED tensors\n",
    "        self.experience_buffer.append({\n",
    "            'text': text_emb.detach(),   # Now safe to call .detach()\n",
    "            'audio': audio_emb.detach(), # Now safe to call .detach()\n",
    "            'reward': reward\n",
    "        })\n",
    "        \n",
    "    def update(self, batch_size=4):\n",
    "        if len(self.experience_buffer) < batch_size: return None\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = np.random.choice(self.experience_buffer, batch_size, replace=False)\n",
    "        \n",
    "        loss_accum = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        for item in batch:\n",
    "            # Re-run forward pass to get gradient graph\n",
    "            # Note: We need to squeeze potential extra dims [1, 768] -> [1, 768]\n",
    "            t_emb = item['text'].to(device)\n",
    "            a_emb = item['audio'].to(device)\n",
    "            \n",
    "            # Ensure correct shape for predictor (Batch, Dim)\n",
    "            if t_emb.dim() == 1: t_emb = t_emb.unsqueeze(0)\n",
    "            if a_emb.dim() == 1: a_emb = a_emb.unsqueeze(0)\n",
    "            \n",
    "            # Predict Macros\n",
    "            macros = self.model.predictor(t_emb, a_emb)\n",
    "            latents = macros['combined']\n",
    "            \n",
    "            # Simple Reward-Weighted Regularization Loss\n",
    "            # Pushes latents towards active state if reward > 0\n",
    "            # Pushes latents towards zero if reward < 0\n",
    "            loss = -1.0 * item['reward'] * torch.mean(latents**2) \n",
    "            \n",
    "            loss.backward()\n",
    "            loss_accum += loss.item()\n",
    "            \n",
    "        self.optimizer.step()\n",
    "        return loss_accum / batch_size\n",
    "\n",
    "# Re-Initialize the trainer\n",
    "trainer = RLHFTrainerMacro(model)\n",
    "print(\"âœ“ Trainer patched. Ready for feedback.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8598915f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fff0853f9d4466b731a09d0a83cd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Task: bright, cutting tone'), Button(description='Generate', style=ButtonStyle()),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- WIDGET SETUP ---\n",
    "SR = 44100\n",
    "# Base audio for transformation (Sine Wave Canvas)\n",
    "t = np.linspace(0, 2.0, int(SR*2.0))\n",
    "base_audio = 0.5 * np.sin(2 * np.pi * 440 * t)\n",
    "base_audio_tensor = torch.from_numpy(base_audio).float().unsqueeze(0).to(device)\n",
    "\n",
    "# Tasks\n",
    "tasks = [\n",
    "    \"bright, cutting tone\",\n",
    "    \"warm, smooth melody with gentle sustain\",\n",
    "    \"harsh, digital sound with buzzy edges\",\n",
    "    \"sharp, metallic plucks with quick decay\",\n",
    "    \"crunchy overdriven riffs with grit\",\n",
    "    \"warm, mellow strumming texture\",\n",
    "    \"glassy, chiming harmonics\",\n",
    "    \"thick, muffled rhythm\",\n",
    "    \"raw, gritty tone with expressive bends\",\n",
    "    \"lush, chorus-soaked clean chords\",\n",
    "    \"dark, smoky ensemble in low register\",\n",
    "    \"warm, smooth chords with soft transients\",\n",
    "    \"bright, percussive stabs\",\n",
    "    \"mellow, emotional melody\",\n",
    "    \"dark, muted tone with felt-like texture\",\n",
    "    \"sparkly, bell-like arpeggios\",\n",
    "    \"soulful chords with slow rotary effect\",\n",
    "    \"punchy, percussive riff with sharp attack\",\n",
    "    \"dreamy, detuned hybrid tone with soft transients\",\n",
    "    \"crunchy, slightly distorted tone with bite\",\n",
    "    \"aggressive, bright lead with sharp harmonics\",\n",
    "    \"warm, analog pad with gentle movement\",\n",
    "    \"grainy, lofi lead with texture\",\n",
    "    \"soft, airy lead with gentle brightness\",\n",
    "    \"detuned, wobbling tone with drifting pitch\",\n",
    "    \"raspy, resonant filter-sweep lead\",\n",
    "    \"sparkly, crystalline plucks with short decay\",\n",
    "    \"thick, wide supersaw lead with stereo spread\",\n",
    "    \"hollow, formant-shifted tone with vowel-like quality\",\n",
    "    \"lush, wide pad with dreamy texture\",\n",
    "    \"dark, evolving ambient pad with low rumble\",\n",
    "    \"celestial, shimmering pad with high-frequency sparkle\",\n",
    "    \"hollow, airy pad with subtle modulation\",\n",
    "    \"moody drone with slow-moving harmonics\",\n",
    "    \"detuned, warm pad with analog drift\",\n",
    "    \"ethereal, floating texture with soft overtones\",\n",
    "    \"deep, warm sub-bass with smooth sine texture\",\n",
    "    \"gritty, distorted tone with heavy saturation\",\n",
    "    \"rubbery, bouncy bass with fast transients\",\n",
    "    \"thick, resonant low-end with movement\",\n",
    "    \"clean, round tone with gentle harmonics\",\n",
    "    \"fuzzy, aggressive tone with buzz-saw texture\",\n",
    "    \"burst with bright edges\",\n",
    "    \"grainy, textured sound with soft filtering\",\n",
    "    \"distorted, chaotic bed with harsh peaks\",\n",
    "    \"warm, analog bed with subtle movement\",\n",
    "    \"glitchy, stuttering texture with digital artifacts\",\n",
    "    \"rough, chaotic saw-like texture\",\n",
    "    \"tight, punchy hit with sharp attack\",\n",
    "    \"snappy, bright sound with crisp transient\",\n",
    "    \"warm, rounded hits with soft decay\",\n",
    "    \"bright pattern with metallic shimmer\",\n",
    "    \"airy, crisp groove with stereo shimmer\",\n",
    "    \"tight, dry beat with fast transients\",\n",
    "    \"boomy, cinematic hits\",\n",
    "    \"gritty, overcompressed loop with pumping artifacts\",\n",
    "    \"deep, resonant tone with long sustain\",\n",
    "    \"sharp, percussive hit with strong transient\",\n",
    "    \"bright, resonant melody\",\n",
    "    \"soft, expressive line with airy tone\",\n",
    "    \"warm phrase with smooth transitions\",\n",
    "    \"dark, breathy melody\",\n",
    "    \"bold stabs with powerful attack\",\n",
    "    \"soft, lush strings with gentle swelling\",\n",
    "    \"warm, woody plucks\",\n",
    "    \"smooth section with warm resonance\",\n",
    "    \"bright strikes with clean attack\",\n",
    "    \"glassy, shimmering notes\",\n",
    "    \"warm chords with soft tremolo\",\n",
    "    \"sharp, metallic hits with long decay\",\n",
    "    \"hollow plucks with woody texture\",\n",
    "    \"distant, echoing ambient chords\",\n",
    "    \"soft, hazy reverb-washed tones\",\n",
    "    \"crystalline ambient washes with airy diffusion\",\n",
    "    \"deep, cavernous drone with subharmonics\",\n",
    "    \"slow, swelling cinematic texture\",\n",
    "    \"retro 80s lead with chorus\",\n",
    "    \"dark industrial tone with metallic grit\",\n",
    "    \"lofi, tape-warped acoustic texture\",\n",
    "    \"robotic, vocoder-like synthetic tone\",\n",
    "    \"electronic plucks with rapid transient snap\",\n",
    "    \"warm, resonant plucks with rounded body\",\n",
    "    \"metallic, atonal texture with shifting harmonics\",\n",
    "    \"heavy, saturated resonant tone with compression pump\"\n",
    "]\n",
    "current_task_idx = 0\n",
    "\n",
    "# UI Components\n",
    "lbl_task = Label(f\"Task: {tasks[0]}\")\n",
    "btn_gen = Button(description=\"Generate\")\n",
    "slider = IntSlider(min=1, max=5, value=3, description=\"Rating\")\n",
    "btn_rate = Button(description=\"Submit\")\n",
    "out = Output()\n",
    "\n",
    "# Global variables to store current step data\n",
    "curr_text_emb = None\n",
    "curr_audio_emb = None\n",
    "\n",
    "def on_gen(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        desc = tasks[current_task_idx]\n",
    "        print(f\"Generating for: {desc}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Run Full Model\n",
    "            # generate_audio=True enables the Predictor -> Decoder -> Engine flow\n",
    "            outputs = model([desc], base_audio_tensor, generate_audio=True)\n",
    "            \n",
    "            # Store embeddings for training\n",
    "            global curr_text_emb, curr_audio_emb\n",
    "            curr_text_emb = outputs['text_emb']\n",
    "            curr_audio_emb = outputs['audio_emb']\n",
    "            \n",
    "            # Visualize Macros (32-dim Latents)\n",
    "            macros = outputs['latents']\n",
    "            # Show just the first few dimensions of each group\n",
    "            t_latents = macros['timbre'][0, :4].cpu().numpy().round(2)\n",
    "            e_latents = macros['envelope'][0, :4].cpu().numpy().round(2)\n",
    "            \n",
    "            print(f\"\\nTimbre Latents (first 4):   {t_latents}\")\n",
    "            print(f\"Envelope Latents (first 4): {e_latents}\")\n",
    "            \n",
    "            # Play Audio\n",
    "            audio = outputs['audio_output'].cpu().numpy()[0]\n",
    "            ipd.display(ipd.Audio(audio, rate=SR))\n",
    "\n",
    "def on_rate(b):\n",
    "    global current_task_idx\n",
    "    with out:\n",
    "        rating = slider.value\n",
    "        \n",
    "        # Add to trainer memory\n",
    "        trainer.add_feedback(curr_text_emb, curr_audio_emb, rating)\n",
    "        print(f\"Rated: {rating}. Updating...\")\n",
    "        \n",
    "        # *** FIX: Use 'experience_buffer' instead of 'buffer' ***\n",
    "        if len(trainer.experience_buffer) >= 4:\n",
    "            loss = trainer.update(batch_size=4)\n",
    "            if loss:\n",
    "                print(f\"ðŸ“‰ Model Updated (Loss: {loss:.4f})\")\n",
    "            else:\n",
    "                print(\"Update skipped (insufficient buffer)\")\n",
    "        else:\n",
    "            print(f\"Stored ({len(trainer.experience_buffer)}/4 samples needed for update)\")\n",
    "            \n",
    "        # Advance task\n",
    "        current_task_idx = (current_task_idx + 1) % len(tasks)\n",
    "        lbl_task.value = f\"Task: {tasks[current_task_idx]}\"\n",
    "\n",
    "# Bind buttons\n",
    "btn_gen.on_click(on_gen)\n",
    "btn_rate.on_click(on_rate)\n",
    "\n",
    "# Display UI\n",
    "display(VBox([lbl_task, btn_gen, slider, btn_rate, out]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
