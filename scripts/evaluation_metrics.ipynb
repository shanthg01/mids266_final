{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multimodal Evaluation Metrics for LSTMABAR\n",
    "Implements STS, Spectral Centroid Error, and MFCC Similarity\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from lstmabar_model import LSTMABAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMABAREvaluator:\n",
    "    \"\"\"\n",
    "    Complete evaluation framework for LSTMABAR model\n",
    "    Implements text-side and audio-side metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LSTMABAR,\n",
    "        text_model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        sample_rate: int = 44100\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Text encoder for STS\n",
    "        self.text_encoder = SentenceTransformer(text_model_name)\n",
    "        \n",
    "        # Archetype descriptors for mapping\n",
    "        self.archetype_descriptors = {\n",
    "            'sine': 'smooth pure warm mellow soft gentle flowing',\n",
    "            'square': 'digital harsh buzzy retro electronic robotic',\n",
    "            'sawtooth': 'bright cutting metallic sharp aggressive',\n",
    "            'triangle': 'hollow woody muted filtered organic',\n",
    "            'noise': 'rough textured grainy distorted chaotic'\n",
    "        }\n",
    "        \n",
    "        # Expected spectral centroids for common descriptors (Hz)\n",
    "        self.centroid_map = {\n",
    "            'bright': 3500, 'dark': 800, 'warm': 1000, 'harsh': 4000,\n",
    "            'mellow': 1200, 'sharp': 5000, 'smooth': 1500, 'soft': 1100,\n",
    "            'aggressive': 3800, 'gentle': 1300, 'cutting': 4200\n",
    "        }\n",
    "    \n",
    "    # ==================== TEXT-SIDE EVALUATION (STS) ====================\n",
    "    \n",
    "    def compute_sts(\n",
    "        self,\n",
    "        input_description: str,\n",
    "        predicted_archetype_weights: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Semantic Textual Similarity: Does model understand description?\n",
    "        \n",
    "        Args:\n",
    "            input_description: Original user description\n",
    "            predicted_archetype_weights: Model's predicted archetype weights (5,)\n",
    "        \n",
    "        Returns:\n",
    "            STS score (0-1, higher is better)\n",
    "        \"\"\"\n",
    "        # Convert archetype weights to descriptive text\n",
    "        predicted_description = self._archetypes_to_text(predicted_archetype_weights)\n",
    "        \n",
    "        # Encode both descriptions\n",
    "        emb_input = self.text_encoder.encode(input_description, convert_to_tensor=True)\n",
    "        emb_predicted = self.text_encoder.encode(predicted_description, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        sts_score = util.cos_sim(emb_input, emb_predicted).item()\n",
    "        \n",
    "        return sts_score\n",
    "    \n",
    "    def _archetypes_to_text(\n",
    "        self,\n",
    "        archetype_weights: np.ndarray,\n",
    "        threshold: float = 0.1\n",
    "    ) -> str:\n",
    "        \"\"\"Convert archetype mixture to natural language\"\"\"\n",
    "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
    "        \n",
    "        # Get top contributing archetypes\n",
    "        descriptions = []\n",
    "        for i, (name, weight) in enumerate(zip(archetype_names, archetype_weights)):\n",
    "            if weight > threshold:\n",
    "                descriptions.append(self.archetype_descriptors[name])\n",
    "        \n",
    "        return ' '.join(descriptions) if descriptions else 'neutral sound'\n",
    "    \n",
    "    # ==================== AUDIO-SIDE EVALUATION ====================\n",
    "    \n",
    "    def spectral_centroid_error(\n",
    "        self,\n",
    "        output_audio: np.ndarray,\n",
    "        target_description: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measures brightness accuracy (Hz error)\n",
    "        \n",
    "        Args:\n",
    "            output_audio: Generated audio array\n",
    "            target_description: Target description\n",
    "        \n",
    "        Returns:\n",
    "            Absolute error in Hz (lower is better)\n",
    "        \"\"\"\n",
    "        # Compute actual spectral centroid\n",
    "        centroid = librosa.feature.spectral_centroid(\n",
    "            y=output_audio,\n",
    "            sr=self.sample_rate\n",
    "        )\n",
    "        actual_centroid = np.mean(centroid)\n",
    "        \n",
    "        # Determine expected centroid from description\n",
    "        expected_centroid = self._description_to_centroid(target_description)\n",
    "        \n",
    "        # Calculate error\n",
    "        error_hz = abs(actual_centroid - expected_centroid)\n",
    "        \n",
    "        return error_hz\n",
    "    \n",
    "    def _description_to_centroid(self, description: str) -> float:\n",
    "        \"\"\"Map description to expected spectral centroid\"\"\"\n",
    "        words = description.lower().split()\n",
    "        expected_centroids = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.centroid_map:\n",
    "                expected_centroids.append(self.centroid_map[word])\n",
    "        \n",
    "        if expected_centroids:\n",
    "            return np.mean(expected_centroids)\n",
    "        \n",
    "        return 2000  # Neutral default\n",
    "    \n",
    "    def mfcc_similarity(\n",
    "        self,\n",
    "        output_audio: np.ndarray,\n",
    "        reference_audio: np.ndarray,\n",
    "        n_mfcc: int = 13\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measures timbral similarity (0-1, higher is better)\n",
    "        \n",
    "        Args:\n",
    "            output_audio: Generated audio array\n",
    "            reference_audio: Target/reference audio array\n",
    "            n_mfcc: Number of MFCC coefficients\n",
    "        \n",
    "        Returns:\n",
    "            Cosine similarity (1 is perfect match, >0.8 is good)\n",
    "        \"\"\"\n",
    "        # Extract MFCCs from both signals\n",
    "        mfcc_output = librosa.feature.mfcc(\n",
    "            y=output_audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mfcc=n_mfcc\n",
    "        )\n",
    "        mfcc_reference = librosa.feature.mfcc(\n",
    "            y=reference_audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mfcc=n_mfcc\n",
    "        )\n",
    "        \n",
    "        # Average across time\n",
    "        mfcc_output_mean = np.mean(mfcc_output, axis=1).reshape(1, -1)\n",
    "        mfcc_reference_mean = np.mean(mfcc_reference, axis=1).reshape(1, -1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(mfcc_output_mean, mfcc_reference_mean)[0][0]\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    # ==================== COMPREHENSIVE EVALUATION ====================\n",
    "    \n",
    "    def evaluate_single_transformation(\n",
    "        self,\n",
    "        input_audio: np.ndarray,\n",
    "        input_description: str,\n",
    "        output_audio: np.ndarray,\n",
    "        reference_audio: np.ndarray,\n",
    "        predicted_weights: np.ndarray\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Complete evaluation of a single transformation\n",
    "        \n",
    "        Returns:\n",
    "            Dict with all metrics\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            # Text-side (NLP understanding)\n",
    "            'sts_score': self.compute_sts(input_description, predicted_weights),\n",
    "            \n",
    "            # Audio-side (transformation quality)\n",
    "            'spectral_centroid_error_hz': self.spectral_centroid_error(\n",
    "                output_audio, input_description\n",
    "            ),\n",
    "            'mfcc_similarity': self.mfcc_similarity(output_audio, reference_audio)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        generate_reference: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set\n",
    "        \n",
    "        Args:\n",
    "            test_samples: List of dicts with 'audio', 'description', 'target_weights'\n",
    "            generate_reference: If True, generate reference audio from target weights\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with results for each sample\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        \n",
    "        print(f\"Evaluating model on {len(test_samples)} samples...\")\n",
    "        \n",
    "        for i, sample in enumerate(test_samples):\n",
    "            input_audio = sample['audio']\n",
    "            description = sample['description']\n",
    "            target_weights = sample['target_weights']\n",
    "            \n",
    "            # Convert to tensor\n",
    "            audio_tensor = torch.from_numpy(input_audio).unsqueeze(0).float().to(self.model.device)\n",
    "            \n",
    "            # Model inference\n",
    "            with torch.no_grad():\n",
    "                transformed, metadata = self.model.inference([description], audio_tensor)\n",
    "            \n",
    "            # Convert back to numpy\n",
    "            output_audio = transformed[0].cpu().numpy()\n",
    "            predicted_weights = metadata['predicted_weights'][0]\n",
    "            \n",
    "            # Generate reference audio if needed\n",
    "            if generate_reference:\n",
    "                # Use archetype generator to create ideal sound\n",
    "                reference_audio = self._generate_reference_audio(target_weights)\n",
    "            else:\n",
    "                reference_audio = input_audio  # Fallback\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_single_transformation(\n",
    "                input_audio,\n",
    "                description,\n",
    "                output_audio,\n",
    "                reference_audio,\n",
    "                predicted_weights\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            metrics['description'] = description\n",
    "            metrics['sample_idx'] = i\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i+1}/{len(test_samples)} samples\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _generate_reference_audio(\n",
    "        self,\n",
    "        archetype_weights: np.ndarray,\n",
    "        duration: float = 2.0\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Generate reference audio from archetype mixture\"\"\"\n",
    "        n_samples = int(self.sample_rate * duration)\n",
    "        t = np.linspace(0, duration, n_samples, endpoint=False)\n",
    "        frequency = 440  # A4\n",
    "        \n",
    "        audio = np.zeros(n_samples)\n",
    "        \n",
    "        # Generate each archetype component\n",
    "        archetypes = {\n",
    "            'sine': np.sin(2 * np.pi * frequency * t),\n",
    "            'square': np.sign(np.sin(2 * np.pi * frequency * t)),\n",
    "            'sawtooth': 2 * (t * frequency - np.floor(0.5 + t * frequency)),\n",
    "            'triangle': 2 * np.abs(2 * (t * frequency - np.floor(t * frequency + 0.5))) - 1,\n",
    "            'noise': np.random.randn(n_samples) * 0.3\n",
    "        }\n",
    "        \n",
    "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
    "        \n",
    "        # Mix according to weights\n",
    "        for i, name in enumerate(archetype_names):\n",
    "            audio += archetype_weights[i] * archetypes[name]\n",
    "        \n",
    "        # Normalize\n",
    "        audio = audio / (np.max(np.abs(audio)) + 1e-8) * 0.95\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    def compare_with_baseline(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        baseline_model: Optional[object] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare LSTMABAR with baseline model\n",
    "        \n",
    "        Args:\n",
    "            test_samples: Test samples\n",
    "            baseline_model: Baseline model (if None, uses simple keyword matching)\n",
    "        \n",
    "        Returns:\n",
    "            Comparison DataFrame\n",
    "        \"\"\"\n",
    "        # Evaluate LSTMABAR\n",
    "        lstmabar_results = self.evaluate_model(test_samples)\n",
    "        \n",
    "        # Evaluate baseline\n",
    "        if baseline_model is None:\n",
    "            baseline_results = self._evaluate_keyword_baseline(test_samples)\n",
    "        else:\n",
    "            baseline_results = baseline_model.evaluate(test_samples)\n",
    "        \n",
    "        # Compute statistics\n",
    "        comparison = self._compute_comparison_stats(lstmabar_results, baseline_results)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _evaluate_keyword_baseline(self, test_samples: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate simple keyword-matching baseline\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Simple keyword â†’ archetype mapping\n",
    "        keyword_map = {\n",
    "            'bright': [0.1, 0.1, 0.6, 0.1, 0.1],  # Mostly sawtooth\n",
    "            'warm': [0.6, 0.1, 0.1, 0.2, 0.0],    # Mostly sine\n",
    "            'harsh': [0.1, 0.5, 0.2, 0.1, 0.1],   # Mostly square\n",
    "            'smooth': [0.6, 0.1, 0.1, 0.2, 0.0],  # Mostly sine\n",
    "            'distorted': [0.1, 0.2, 0.2, 0.1, 0.4] # High noise\n",
    "        }\n",
    "        \n",
    "        for i, sample in enumerate(test_samples):\n",
    "            description = sample['description'].lower()\n",
    "            \n",
    "            # Find matching keywords\n",
    "            predicted_weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform default\n",
    "            for keyword, weights in keyword_map.items():\n",
    "                if keyword in description:\n",
    "                    predicted_weights = np.array(weights)\n",
    "                    break\n",
    "            \n",
    "            # Simple transformation (just apply gain based on brightness)\n",
    "            output_audio = sample['audio'].copy()\n",
    "            if 'bright' in description:\n",
    "                # Boost high frequencies (simplified)\n",
    "                output_audio = output_audio * 1.2\n",
    "            \n",
    "            reference_audio = self._generate_reference_audio(sample['target_weights'])\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_single_transformation(\n",
    "                sample['audio'],\n",
    "                description,\n",
    "                output_audio,\n",
    "                reference_audio,\n",
    "                predicted_weights\n",
    "            )\n",
    "            \n",
    "            metrics['description'] = description\n",
    "            metrics['sample_idx'] = i\n",
    "            results.append(metrics)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _compute_comparison_stats(\n",
    "        self,\n",
    "        lstmabar_results: pd.DataFrame,\n",
    "        baseline_results: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute statistical comparison\"\"\"\n",
    "        metrics = ['sts_score', 'spectral_centroid_error_hz', 'mfcc_similarity']\n",
    "        \n",
    "        comparison = {\n",
    "            'Metric': [],\n",
    "            'Baseline_Mean': [],\n",
    "            'Baseline_Std': [],\n",
    "            'LSTMABAR_Mean': [],\n",
    "            'LSTMABAR_Std': [],\n",
    "            'Improvement': [],\n",
    "            'p_value': []\n",
    "        }\n",
    "        \n",
    "        for metric in metrics:\n",
    "            baseline_vals = baseline_results[metric].values\n",
    "            lstmabar_vals = lstmabar_results[metric].values\n",
    "            \n",
    "            baseline_mean = np.mean(baseline_vals)\n",
    "            baseline_std = np.std(baseline_vals)\n",
    "            lstmabar_mean = np.mean(lstmabar_vals)\n",
    "            lstmabar_std = np.std(lstmabar_vals)\n",
    "            \n",
    "            comparison['Metric'].append(metric)\n",
    "            comparison['Baseline_Mean'].append(f\"{baseline_mean:.4f}\")\n",
    "            comparison['Baseline_Std'].append(f\"{baseline_std:.4f}\")\n",
    "            comparison['LSTMABAR_Mean'].append(f\"{lstmabar_mean:.4f}\")\n",
    "            comparison['LSTMABAR_Std'].append(f\"{lstmabar_std:.4f}\")\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if 'error' in metric.lower():\n",
    "                # Lower is better\n",
    "                improvement = ((baseline_mean - lstmabar_mean) / baseline_mean) * 100\n",
    "            else:\n",
    "                # Higher is better\n",
    "                improvement = ((lstmabar_mean - baseline_mean) / baseline_mean) * 100\n",
    "            \n",
    "            comparison['Improvement'].append(f\"{improvement:.2f}%\")\n",
    "            \n",
    "            # Paired t-test\n",
    "            _, p_val = ttest_rel(baseline_vals, lstmabar_vals)\n",
    "            comparison['p_value'].append(f\"{p_val:.4f}\")\n",
    "        \n",
    "        return pd.DataFrame(comparison)\n",
    "    \n",
    "    def generate_evaluation_report(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        save_path: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LSTMABAR MODEL EVALUATION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = self.evaluate_model(test_samples)\n",
    "        \n",
    "        # Compare with baseline\n",
    "        comparison = self.compare_with_baseline(test_samples)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPARISON WITH BASELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(comparison.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INTERPRETATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # STS interpretation\n",
    "        sts_mean = results['sts_score'].mean()\n",
    "        print(f\"\\nðŸ“ TEXT UNDERSTANDING (STS Score):\")\n",
    "        print(f\"   Mean: {sts_mean:.3f}\")\n",
    "        if sts_mean > 0.75:\n",
    "            print(f\"   âœ“ Excellent semantic understanding!\")\n",
    "        elif sts_mean > 0.60:\n",
    "            print(f\"   â†’ Good semantic understanding\")\n",
    "        else:\n",
    "            print(f\"   âš  Needs improvement in NLP comprehension\")\n",
    "        \n",
    "        # Spectral centroid interpretation\n",
    "        sc_mean = results['spectral_centroid_error_hz'].mean()\n",
    "        print(f\"\\nðŸŽµ BRIGHTNESS ACCURACY (Spectral Centroid Error):\")\n",
    "        print(f\"   Mean: {sc_mean:.0f} Hz\")\n",
    "        if sc_mean < 200:\n",
    "            print(f\"   âœ“ Excellent brightness targeting!\")\n",
    "        elif sc_mean < 500:\n",
    "            print(f\"   â†’ Good brightness control\")\n",
    "        else:\n",
    "            print(f\"   âš  Brightness targeting needs improvement\")\n",
    "        \n",
    "        # MFCC interpretation\n",
    "        mfcc_mean = results['mfcc_similarity'].mean()\n",
    "        print(f\"\\nðŸŽ¸ TIMBRE QUALITY (MFCC Similarity):\")\n",
    "        print(f\"   Mean: {mfcc_mean:.3f}\")\n",
    "        if mfcc_mean > 0.80:\n",
    "            print(f\"   âœ“ Excellent timbral matching!\")\n",
    "        elif mfcc_mean > 0.65:\n",
    "            print(f\"   â†’ Good timbral similarity\")\n",
    "        else:\n",
    "            print(f\"   âš  Timbre quality needs improvement\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # Save detailed results\n",
    "        if save_path:\n",
    "            results.to_csv(save_path, index=False)\n",
    "            comparison.to_csv(save_path.replace('.csv', '_comparison.csv'), index=False)\n",
    "            print(f\"\\nDetailed results saved to {save_path}\")\n",
    "\n",
    "    def evaluate_model_on_test_set(\n",
    "        self,\n",
    "        test_data_path: str,\n",
    "        max_samples: Optional[int] = None,\n",
    "        save_results: bool = True,\n",
    "        results_path: str = 'test_results.csv'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set from MusicCaps\n",
    "        \n",
    "        Args:\n",
    "            test_data_path: Path to test .npz file\n",
    "            max_samples: Max samples to evaluate (None = all)\n",
    "            save_results: Whether to save results\n",
    "            results_path: Path to save results\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EVALUATING ON TEST SET\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"Loading test data from {test_data_path}...\")\n",
    "        data = np.load(test_data_path, allow_pickle=True)\n",
    "        \n",
    "        vectors = data['archetype_vectors']\n",
    "        descriptions = data['descriptions'].tolist()\n",
    "        audio_paths = data['audio_paths'].tolist()\n",
    "        \n",
    "        n_samples = min(len(descriptions), max_samples) if max_samples else len(descriptions)\n",
    "        print(f\"Evaluating on {n_samples} test samples\\n\")\n",
    "        \n",
    "        # Prepare test samples\n",
    "        test_samples = []\n",
    "        for i in range(n_samples):\n",
    "            audio_path = audio_paths[i]\n",
    "            \n",
    "            # Check if audio exists\n",
    "            if not Path(audio_path).exists():\n",
    "                print(f\"Skipping {i}: audio file not found\")\n",
    "                continue\n",
    "            \n",
    "            # Load audio\n",
    "            try:\n",
    "                audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=2.0)\n",
    "                \n",
    "                # Pad or trim\n",
    "                target_length = int(self.sample_rate * 2.0)\n",
    "                if len(audio) < target_length:\n",
    "                    audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target_length]\n",
    "                \n",
    "                test_samples.append({\n",
    "                    'audio': audio,\n",
    "                    'description': descriptions[i],\n",
    "                    'target_weights': vectors[i]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {audio_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully loaded {len(test_samples)} test samples\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results_df = self.evaluate_model(test_samples, generate_reference=True)\n",
    "        \n",
    "        # Save results\n",
    "        if save_results:\n",
    "            results_df.to_csv(results_path, index=False)\n",
    "            print(f\"\\nâœ“ Results saved to {results_path}\")\n",
    "        \n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper: load with prefix-compat (for FineTune_B)\n",
    "\n",
    "# --- compatibility remap for old text-encoder prefixes (FineTune_B) ---\n",
    "def _compat_remap_text_encoder_keys(sd: dict) -> dict:\n",
    "    if not any(k.startswith(\"text_encoder.backbone.\") for k in sd.keys()):\n",
    "        return sd\n",
    "    m = [\n",
    "        (\"text_encoder.backbone.embeddings.\", \"text_encoder.sentence_model.0.auto_model.embeddings.\"),\n",
    "        (\"text_encoder.backbone.encoder.\",    \"text_encoder.sentence_model.0.auto_model.encoder.\"),\n",
    "        (\"text_encoder.backbone.pooler.\",     \"text_encoder.sentence_model.0.auto_model.pooler.\"),\n",
    "    ]\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        newk = k\n",
    "        for old, new in m:\n",
    "            if k.startswith(old):\n",
    "                newk = new + k[len(old):]\n",
    "                break\n",
    "        out[newk] = v\n",
    "    return out\n",
    "\n",
    "def load_lstmabar_checkpoint(path: str, device: str = None):\n",
    "    import torch\n",
    "    from lstmabar_model import LSTMABAR\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTMABAR(embedding_dim=768, audio_architecture='resnet', sample_rate=44100, device=device)\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    sd = ckpt.get(\"model_state_dict\", ckpt)\n",
    "    sd = _compat_remap_text_encoder_keys(sd)\n",
    "    incompat = model.load_state_dict(sd, strict=False)\n",
    "    print(f\"Loaded {path} (epoch={ckpt.get('epoch','?')}) | missing={len(incompat.missing_keys)} unexpected={len(incompat.unexpected_keys)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a small shared evaluation set (prefer real val clips, fallback to 3 synthetic)\n",
    "\n",
    "def build_shared_eval_samples(val_npz_path: str, k: int = 73, sample_rate: int = 44100, duration: float = 2.0):\n",
    "    D = np.load(val_npz_path, allow_pickle=True)\n",
    "    descs = D['descriptions'].tolist()\n",
    "    paths = D['audio_paths'].tolist()\n",
    "    vecs  = D['archetype_vectors']\n",
    "\n",
    "    tgt_len = int(sample_rate * duration)\n",
    "    samples = []\n",
    "    for i, p in enumerate(paths):\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                y, sr = librosa.load(p, sr=sample_rate, duration=duration)\n",
    "                if len(y) < tgt_len:\n",
    "                    y = np.pad(y, (0, tgt_len - len(y)))\n",
    "                else:\n",
    "                    y = y[:tgt_len]\n",
    "                samples.append({'audio': y, 'description': descs[i], 'target_weights': vecs[i]})\n",
    "                if len(samples) >= k:\n",
    "                    break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Fallback (if nothing loads)\n",
    "    if not samples:\n",
    "        rng = np.random.default_rng(42)\n",
    "        for desc, vec in [\n",
    "            (\"bright and cutting guitar with metallic tone\", np.array([0.1,0.1,0.6,0.1,0.1])),\n",
    "            (\"warm smooth piano melody with gentle sustain\", np.array([0.6,0.05,0.1,0.2,0.05])),\n",
    "            (\"harsh digital synth with buzzy retro sound\",   np.array([0.1,0.55,0.15,0.1,0.1])),\n",
    "        ]:\n",
    "            y = rng.standard_normal(tgt_len).astype(np.float32)\n",
    "            samples.append({'audio': y, 'description': desc, 'target_weights': vec})\n",
    "    print(f\"Shared eval set size: {len(samples)}\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model-vs-Model comparison on the shared set\n",
    "\n",
    "import itertools\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def evaluate_side_by_side(models: list, samples: list, sample_rate: int = 44100):\n",
    "    \"\"\"\n",
    "    models: list of (label, model)\n",
    "    samples: list of dicts with 'audio', 'description', 'target_weights'\n",
    "    \"\"\"\n",
    "    results_per_model = {}\n",
    "    for label, mdl in models:\n",
    "        print(f\"\\n[Eval] {label}\")\n",
    "        ev = LSTMABAREvaluator(mdl, sample_rate=sample_rate)\n",
    "        # Turn numpy audios into the structure expected by evaluate_model\n",
    "        df = ev.evaluate_model(samples, generate_reference=True)\n",
    "        results_per_model[label] = df\n",
    "\n",
    "    # Summaries\n",
    "    def summarize(df):\n",
    "        return {\n",
    "            \"sts_mean\": df[\"sts_score\"].mean(),\n",
    "            \"sts_std\":  df[\"sts_score\"].std(),\n",
    "            \"centroid_err_mean\": df[\"spectral_centroid_error_hz\"].mean(),\n",
    "            \"centroid_err_std\":  df[\"spectral_centroid_error_hz\"].std(),\n",
    "            \"mfcc_mean\": df[\"mfcc_similarity\"].mean(),\n",
    "            \"mfcc_std\":  df[\"mfcc_similarity\"].std(),\n",
    "            \"n\": len(df),\n",
    "        }\n",
    "\n",
    "    summary_rows = []\n",
    "    for label, df in results_per_model.items():\n",
    "        s = summarize(df)\n",
    "        s[\"model\"] = label\n",
    "        summary_rows.append(s)\n",
    "    means_df = pd.DataFrame(summary_rows).set_index(\"model\").round(4)\n",
    "\n",
    "    # Pairwise deltas + paired t-tests\n",
    "    pairs = []\n",
    "    labels = list(results_per_model.keys())\n",
    "    metrics = [\n",
    "        (\"sts_score\", \"higher_better\"),\n",
    "        (\"spectral_centroid_error_hz\", \"lower_better\"),\n",
    "        (\"mfcc_similarity\", \"higher_better\"),\n",
    "    ]\n",
    "    for a, b in itertools.combinations(labels, 2):\n",
    "        A = results_per_model[a]\n",
    "        B = results_per_model[b]\n",
    "        # align by sample_idx (just in case order changed)\n",
    "        A2 = A.sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        B2 = B.sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        for metric, direction in metrics:\n",
    "            a_vals = A2[metric].values\n",
    "            b_vals = B2[metric].values\n",
    "            # improvement relative to b (positive is better)\n",
    "            if direction == \"higher_better\":\n",
    "                delta = a_vals - b_vals\n",
    "            else:\n",
    "                delta = b_vals - a_vals  # lower error => better\n",
    "            tstat, p = ttest_rel(a_vals, b_vals)\n",
    "            pairs.append({\n",
    "                \"A\": a, \"B\": b, \"Metric\": metric,\n",
    "                \"Î”_mean(A_vs_B)\": np.mean(delta),\n",
    "                \"p_value\": p\n",
    "            })\n",
    "\n",
    "    pairwise_df = pd.DataFrame(pairs)\n",
    "    # Sort for readability\n",
    "    pairwise_df = pairwise_df.sort_values([\"Metric\", \"A\", \"B\"]).reset_index(drop=True)\n",
    "    return means_df, pairwise_df, results_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test-set evaluation (your original path)\n",
    "\n",
    "def eval_on_test_set(model, test_npz, max_samples=None, sample_rate=44100):\n",
    "    ev = LSTMABAREvaluator(model, sample_rate=sample_rate)\n",
    "    df = ev.evaluate_model_on_test_set(\n",
    "        test_data_path=test_npz,\n",
    "        max_samples=max_samples,\n",
    "        save_results=False\n",
    "    )\n",
    "    return {\n",
    "        \"sts_mean\": df[\"sts_score\"].mean(),\n",
    "        \"centroid_err_mean\": df[\"spectral_centroid_error_hz\"].mean(),\n",
    "        \"mfcc_mean\": df[\"mfcc_similarity\"].mean(),\n",
    "        \"n\": len(df)\n",
    "    }, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/best_model.pth (epoch=1) | missing=0 unexpected=0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/fine_tune_A/best_model.pth (epoch=8) | missing=0 unexpected=0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/fine_tune_B/best_model.pth (epoch=6) | missing=0 unexpected=0\n",
      "Shared eval set size: 71\n",
      "\n",
      "[Eval] LSTMABAR_Baseline\n",
      "Evaluating model on 71 samples...\n",
      "  Processed 10/71 samples\n",
      "  Processed 20/71 samples\n",
      "  Processed 30/71 samples\n",
      "  Processed 40/71 samples\n",
      "  Processed 50/71 samples\n",
      "  Processed 60/71 samples\n",
      "  Processed 70/71 samples\n",
      "\n",
      "[Eval] FineTune_A\n",
      "Evaluating model on 71 samples...\n",
      "  Processed 10/71 samples\n",
      "  Processed 20/71 samples\n",
      "  Processed 30/71 samples\n",
      "  Processed 40/71 samples\n",
      "  Processed 50/71 samples\n",
      "  Processed 60/71 samples\n",
      "  Processed 70/71 samples\n",
      "\n",
      "[Eval] FineTune_B\n",
      "Evaluating model on 71 samples...\n",
      "  Processed 10/71 samples\n",
      "  Processed 20/71 samples\n",
      "  Processed 30/71 samples\n",
      "  Processed 40/71 samples\n",
      "  Processed 50/71 samples\n",
      "  Processed 60/71 samples\n",
      "  Processed 70/71 samples\n",
      "Saved head-to-head (incl. keyword baseline): results/model_means_shared.csv, results/pairwise_stats_shared.csv\n",
      "\n",
      "[Test] LSTMABAR_Baseline\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 73 test samples\n",
      "\n",
      "Successfully loaded 73 test samples\n",
      "\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "\n",
      "[Test] FineTune_A\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 73 test samples\n",
      "\n",
      "Successfully loaded 73 test samples\n",
      "\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "\n",
      "[Test] FineTune_B\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 73 test samples\n",
      "\n",
      "Successfully loaded 73 test samples\n",
      "\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "\n",
      "[Test] Keyword_Baseline\n",
      "Saved test means (incl. keyword baseline): results/model_means_test.csv\n",
      "\n",
      "=== Head-to-Head (Shared Set) Means (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sts_mean</th>\n",
       "      <th>sts_std</th>\n",
       "      <th>centroid_err_mean</th>\n",
       "      <th>centroid_err_std</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>mfcc_std</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTMABAR_Baseline</th>\n",
       "      <td>0.2122</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>4827.4214</td>\n",
       "      <td>1655.2142</td>\n",
       "      <td>0.4777</td>\n",
       "      <td>0.2814</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_A</th>\n",
       "      <td>0.2049</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>4725.1752</td>\n",
       "      <td>1693.7753</td>\n",
       "      <td>0.4748</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_B</th>\n",
       "      <td>0.2135</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>5323.9792</td>\n",
       "      <td>1670.1855</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.3299</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword_Baseline</th>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>830.7667</td>\n",
       "      <td>707.9972</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sts_mean  sts_std  centroid_err_mean  centroid_err_std  \\\n",
       "model                                                                       \n",
       "LSTMABAR_Baseline    0.2122   0.0637          4827.4214         1655.2142   \n",
       "FineTune_A           0.2049   0.0656          4725.1752         1693.7753   \n",
       "FineTune_B           0.2135   0.0641          5323.9792         1670.1855   \n",
       "Keyword_Baseline     0.2072   0.0619           830.7667          707.9972   \n",
       "\n",
       "                   mfcc_mean  mfcc_std   n  \n",
       "model                                       \n",
       "LSTMABAR_Baseline     0.4777    0.2814  71  \n",
       "FineTune_A            0.4748    0.2908  71  \n",
       "FineTune_B            0.4032    0.3299  71  \n",
       "Keyword_Baseline      0.5547    0.2384  71  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head-to-Head Pairwise Î” & p-values (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Î”_mean(A_vs_B)</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.071598</td>\n",
       "      <td>6.626610e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.079845</td>\n",
       "      <td>4.833251e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.151442</td>\n",
       "      <td>1.104219e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>6.987086e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.074463</td>\n",
       "      <td>1.202329e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.076979</td>\n",
       "      <td>1.070820e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>598.804058</td>\n",
       "      <td>6.611588e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-3894.408496</td>\n",
       "      <td>1.960572e-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-4493.212554</td>\n",
       "      <td>3.233358e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-102.246256</td>\n",
       "      <td>3.068553e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>496.557803</td>\n",
       "      <td>6.875221e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-3996.654751</td>\n",
       "      <td>7.601251e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.008604</td>\n",
       "      <td>1.018327e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.002339</td>\n",
       "      <td>5.813384e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>4.309090e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>1.864704e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.001327</td>\n",
       "      <td>3.207549e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>7.898056e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    A                 B                      Metric  \\\n",
       "0          FineTune_A        FineTune_B             mfcc_similarity   \n",
       "1          FineTune_A  Keyword_Baseline             mfcc_similarity   \n",
       "2          FineTune_B  Keyword_Baseline             mfcc_similarity   \n",
       "3   LSTMABAR_Baseline        FineTune_A             mfcc_similarity   \n",
       "4   LSTMABAR_Baseline        FineTune_B             mfcc_similarity   \n",
       "5   LSTMABAR_Baseline  Keyword_Baseline             mfcc_similarity   \n",
       "6          FineTune_A        FineTune_B  spectral_centroid_error_hz   \n",
       "7          FineTune_A  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "8          FineTune_B  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "9   LSTMABAR_Baseline        FineTune_A  spectral_centroid_error_hz   \n",
       "10  LSTMABAR_Baseline        FineTune_B  spectral_centroid_error_hz   \n",
       "11  LSTMABAR_Baseline  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "12         FineTune_A        FineTune_B                   sts_score   \n",
       "13         FineTune_A  Keyword_Baseline                   sts_score   \n",
       "14         FineTune_B  Keyword_Baseline                   sts_score   \n",
       "15  LSTMABAR_Baseline        FineTune_A                   sts_score   \n",
       "16  LSTMABAR_Baseline        FineTune_B                   sts_score   \n",
       "17  LSTMABAR_Baseline  Keyword_Baseline                   sts_score   \n",
       "\n",
       "    Î”_mean(A_vs_B)       p_value  \n",
       "0         0.071598  6.626610e-03  \n",
       "1        -0.079845  4.833251e-05  \n",
       "2        -0.151442  1.104219e-05  \n",
       "3         0.002865  6.987086e-01  \n",
       "4         0.074463  1.202329e-03  \n",
       "5        -0.076979  1.070820e-04  \n",
       "6       598.804058  6.611588e-07  \n",
       "7     -3894.408496  1.960572e-28  \n",
       "8     -4493.212554  3.233358e-32  \n",
       "9      -102.246256  3.068553e-01  \n",
       "10      496.557803  6.875221e-12  \n",
       "11    -3996.654751  7.601251e-30  \n",
       "12       -0.008604  1.018327e-02  \n",
       "13       -0.002339  5.813384e-01  \n",
       "14        0.006265  4.309090e-02  \n",
       "15        0.007277  1.864704e-02  \n",
       "16       -0.001327  3.207549e-01  \n",
       "17        0.004938  7.898056e-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Set Means (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sts_mean</th>\n",
       "      <th>centroid_err_mean</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTMABAR_Baseline</th>\n",
       "      <td>0.2140</td>\n",
       "      <td>4876.2111</td>\n",
       "      <td>0.4248</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_A</th>\n",
       "      <td>0.2108</td>\n",
       "      <td>4806.2635</td>\n",
       "      <td>0.4263</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_B</th>\n",
       "      <td>0.2166</td>\n",
       "      <td>5566.6149</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword_Baseline</th>\n",
       "      <td>0.2088</td>\n",
       "      <td>818.7486</td>\n",
       "      <td>0.5468</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sts_mean  centroid_err_mean  mfcc_mean   n\n",
       "model                                                        \n",
       "LSTMABAR_Baseline    0.2140          4876.2111     0.4248  73\n",
       "FineTune_A           0.2108          4806.2635     0.4263  73\n",
       "FineTune_B           0.2166          5566.6149     0.3668  73\n",
       "Keyword_Baseline     0.2088           818.7486     0.5468  73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTIMODAL EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Shared Validation Set (71 samples)\n",
      "================================================================================\n",
      "Comparing all models vs Keyword_Baseline\n",
      "\n",
      "\n",
      "[LSTMABAR_Baseline]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.212  (+2.41% vs baseline)\n",
      "Spectral Centroid Error: 4827 Hz  (-481.08% vs baseline)\n",
      "MFCC Similarity: 0.478  (-13.88% vs baseline)\n",
      "n = 71 samples\n",
      "\n",
      "[FineTune_A]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.205  (-1.11% vs baseline)\n",
      "Spectral Centroid Error: 4725 Hz  (-468.77% vs baseline)\n",
      "MFCC Similarity: 0.475  (-14.40% vs baseline)\n",
      "n = 71 samples\n",
      "\n",
      "[FineTune_B]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.213  (+3.04% vs baseline)\n",
      "Spectral Centroid Error: 5324 Hz  (-540.85% vs baseline)\n",
      "MFCC Similarity: 0.403  (-27.31% vs baseline)\n",
      "n = 71 samples\n",
      "\n",
      "================================================================================\n",
      "PAIRWISE COMPARISONS (Shared Set)\n",
      "================================================================================\n",
      "                A                B                     Metric  Î”_mean(A_vs_B)      p_value\n",
      "       FineTune_A       FineTune_B            mfcc_similarity        0.071598 6.626610e-03\n",
      "       FineTune_A Keyword_Baseline            mfcc_similarity       -0.079845 4.833251e-05\n",
      "       FineTune_B Keyword_Baseline            mfcc_similarity       -0.151442 1.104219e-05\n",
      "LSTMABAR_Baseline       FineTune_A            mfcc_similarity        0.002865 6.987086e-01\n",
      "LSTMABAR_Baseline       FineTune_B            mfcc_similarity        0.074463 1.202329e-03\n",
      "LSTMABAR_Baseline Keyword_Baseline            mfcc_similarity       -0.076979 1.070820e-04\n",
      "       FineTune_A       FineTune_B spectral_centroid_error_hz      598.804058 6.611588e-07\n",
      "       FineTune_A Keyword_Baseline spectral_centroid_error_hz    -3894.408496 1.960572e-28\n",
      "       FineTune_B Keyword_Baseline spectral_centroid_error_hz    -4493.212554 3.233358e-32\n",
      "LSTMABAR_Baseline       FineTune_A spectral_centroid_error_hz     -102.246256 3.068553e-01\n",
      "LSTMABAR_Baseline       FineTune_B spectral_centroid_error_hz      496.557803 6.875221e-12\n",
      "LSTMABAR_Baseline Keyword_Baseline spectral_centroid_error_hz    -3996.654751 7.601251e-30\n",
      "       FineTune_A       FineTune_B                  sts_score       -0.008604 1.018327e-02\n",
      "       FineTune_A Keyword_Baseline                  sts_score       -0.002339 5.813384e-01\n",
      "       FineTune_B Keyword_Baseline                  sts_score        0.006265 4.309090e-02\n",
      "LSTMABAR_Baseline       FineTune_A                  sts_score        0.007277 1.864704e-02\n",
      "LSTMABAR_Baseline       FineTune_B                  sts_score       -0.001327 3.207549e-01\n",
      "LSTMABAR_Baseline Keyword_Baseline                  sts_score        0.004938 7.898056e-02\n",
      "\n",
      "================================================================================\n",
      "TEST SET PERFORMANCE (73 samples)\n",
      "================================================================================\n",
      "All results compared against Keyword_Baseline\n",
      "\n",
      "\n",
      "[LSTMABAR_Baseline]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.214 (+2.49% vs baseline)\n",
      "Spectral Centroid Error: 4876 Hz (-495.57% vs baseline)\n",
      "MFCC Similarity: 0.425 (-22.31% vs baseline)\n",
      "n = 73 samples\n",
      "\n",
      "[FineTune_A]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.211 (+0.96% vs baseline)\n",
      "Spectral Centroid Error: 4806 Hz (-487.03% vs baseline)\n",
      "MFCC Similarity: 0.426 (-22.04% vs baseline)\n",
      "n = 73 samples\n",
      "\n",
      "[FineTune_B]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.217 (+3.74% vs baseline)\n",
      "Spectral Centroid Error: 5567 Hz (-579.89% vs baseline)\n",
      "MFCC Similarity: 0.367 (-32.92% vs baseline)\n",
      "n = 73 samples\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ TEXT UNDERSTANDING (STS)\n",
      " â€¢ â‰¥ 0.75 â†’ Excellent\n",
      " â€¢ 0.60â€“0.75 â†’ Good\n",
      " â€¢ < 0.60 â†’ Needs improvement\n",
      "\n",
      "ðŸŽµ BRIGHTNESS ACCURACY (Spectral Centroid Error)\n",
      " â€¢ < 200 Hz â†’ Excellent\n",
      " â€¢ < 500 Hz â†’ Good\n",
      " â€¢ > 500 Hz â†’ Needs improvement\n",
      "\n",
      "ðŸŽ¸ TIMBRE QUALITY (MFCC Similarity)\n",
      " â€¢ > 0.80 â†’ Excellent\n",
      " â€¢ > 0.65 â†’ Good\n",
      " â€¢ â‰¤ 0.65 â†’ Needs improvement\n",
      "\n",
      "================================================================================\n",
      "âœ“ Detailed CSVs in /results :\n",
      "   â€¢ model_means_shared.csv\n",
      "   â€¢ pairwise_stats_shared.csv\n",
      "   â€¢ model_means_test.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "### Run the eval metrics:\n",
    "\n",
    "# --- Small helper to summarize a results DF (same metrics everywhere) ---\n",
    "def _summarize_df(df: pd.DataFrame) -> dict:\n",
    "    return {\n",
    "        \"sts_mean\": float(df[\"sts_score\"].mean()),\n",
    "        \"sts_std\":  float(df[\"sts_score\"].std()),\n",
    "        \"centroid_err_mean\": float(df[\"spectral_centroid_error_hz\"].mean()),\n",
    "        \"centroid_err_std\":  float(df[\"spectral_centroid_error_hz\"].std()),\n",
    "        \"mfcc_mean\": float(df[\"mfcc_similarity\"].mean()),\n",
    "        \"mfcc_std\":  float(df[\"mfcc_similarity\"].std()),\n",
    "        \"n\": int(len(df)),\n",
    "    }\n",
    "\n",
    "# --- helper to (re)build pairwise stats from a dict of label->df ---\n",
    "def _pairwise_from_frames(frames: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    labels = list(frames.keys())\n",
    "    metrics = [\n",
    "        (\"sts_score\", \"higher_better\"),\n",
    "        (\"spectral_centroid_error_hz\", \"lower_better\"),\n",
    "        (\"mfcc_similarity\", \"higher_better\"),\n",
    "    ]\n",
    "    for a, b in itertools.combinations(labels, 2):\n",
    "        A = frames[a].sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        B = frames[b].sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        for metric, direction in metrics:\n",
    "            a_vals = A[metric].values\n",
    "            b_vals = B[metric].values\n",
    "            # improvement relative to B (positive means A better)\n",
    "            if direction == \"higher_better\":\n",
    "                delta = a_vals - b_vals\n",
    "            else:\n",
    "                delta = b_vals - a_vals  # lower error => better\n",
    "            _, p = ttest_rel(a_vals, b_vals)\n",
    "            rows.append({\n",
    "                \"A\": a, \"B\": b, \"Metric\": metric,\n",
    "                \"Î”_mean(A_vs_B)\": float(np.mean(delta)),\n",
    "                \"p_value\": float(p)\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"Metric\",\"A\",\"B\"]).reset_index(drop=True)\n",
    "\n",
    "# --- helper to load raw samples from an NPZ into a list for baselines ---\n",
    "def _load_samples_from_npz(npz_path: str, max_samples=None, sample_rate=44100, duration=2.0):\n",
    "    D = np.load(npz_path, allow_pickle=True)\n",
    "    descs = D[\"descriptions\"].tolist()\n",
    "    paths = D[\"audio_paths\"].tolist()\n",
    "    vecs  = D[\"archetype_vectors\"]\n",
    "    tgt_len = int(sample_rate * duration)\n",
    "\n",
    "    samples = []\n",
    "    count = 0\n",
    "    for i, p in enumerate(paths):\n",
    "        if max_samples is not None and count >= max_samples:\n",
    "            break\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                y, sr = librosa.load(p, sr=sample_rate, duration=duration)\n",
    "                if len(y) < tgt_len:\n",
    "                    y = np.pad(y, (0, tgt_len - len(y)))\n",
    "                else:\n",
    "                    y = y[:tgt_len]\n",
    "                samples.append({\n",
    "                    \"audio\": y,\n",
    "                    \"description\": descs[i],\n",
    "                    \"target_weights\": vecs[i],\n",
    "                    \"sample_idx\": i\n",
    "                })\n",
    "                count += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    return samples\n",
    "\n",
    "# --- your existing paths ---\n",
    "BASELINE_CKPT   = \"checkpoints/best_model.pth\"\n",
    "FINETUNE_A_CKPT = \"checkpoints/fine_tune_A/best_model.pth\"\n",
    "FINETUNE_B_CKPT = \"checkpoints/fine_tune_B/best_model.pth\"\n",
    "\n",
    "VAL_NPZ  = \"musiccaps_training_data_val.npz\"\n",
    "TEST_NPZ = \"musiccaps_training_data_test.npz\"\n",
    "\n",
    "MAX_SAMPLES_SHARED = 73   # shared head-to-head set (val)\n",
    "MAX_SAMPLES_TEST   = None # full test or cap with an int\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1) Load models (with compat loader)\n",
    "print(\"Loading models...\")\n",
    "m_base = load_lstmabar_checkpoint(BASELINE_CKPT, device=device)\n",
    "m_A    = load_lstmabar_checkpoint(FINETUNE_A_CKPT, device=device)\n",
    "m_B    = load_lstmabar_checkpoint(FINETUNE_B_CKPT, device=device)\n",
    "\n",
    "# 2) Build shared eval set from VAL (or fallback synthetic inside the helper)\n",
    "shared_samples = build_shared_eval_samples(VAL_NPZ, k=MAX_SAMPLES_SHARED, sample_rate=44100)\n",
    "\n",
    "# 3) Model-vs-Model (LSTM-based models only, first pass)\n",
    "models = [(\"LSTMABAR_Baseline\", m_base), (\"FineTune_A\", m_A), (\"FineTune_B\", m_B)]\n",
    "means_df, pairwise_df, per_model_frames = evaluate_side_by_side(models, shared_samples, sample_rate=44100)\n",
    "\n",
    "# 4) Add the Keyword Baseline (non-LSTM) on the SAME shared samples\n",
    "ev_tmp = LSTMABAREvaluator(m_base, sample_rate=44100)  # any LSTMABAR instance just to access evaluator\n",
    "kb_shared_df = ev_tmp._evaluate_keyword_baseline(shared_samples)\n",
    "# ensure sample_idx exists for alignment (added by evaluate_model; add here too)\n",
    "if \"sample_idx\" not in kb_shared_df.columns:\n",
    "    kb_shared_df[\"sample_idx\"] = range(len(kb_shared_df))\n",
    "\n",
    "per_model_frames[\"Keyword_Baseline\"] = kb_shared_df\n",
    "\n",
    "# 5) Recompute head-to-head means/pairwise including Keyword_Baseline\n",
    "summary_rows = []\n",
    "for label, df in per_model_frames.items():\n",
    "    s = _summarize_df(df)\n",
    "    s[\"model\"] = label\n",
    "    summary_rows.append(s)\n",
    "means_df_all = pd.DataFrame(summary_rows).set_index(\"model\").round(4)\n",
    "\n",
    "pairwise_df_all = _pairwise_from_frames(per_model_frames)\n",
    "pairwise_df_all[\"Î”_mean(A_vs_B)\"] = pairwise_df_all[\"Î”_mean(A_vs_B)\"].round(6)\n",
    "pairwise_df_all[\"p_value\"] = pairwise_df_all[\"p_value\"].map(lambda x: float(x))\n",
    "\n",
    "# 6) Save head-to-head (with keyword baseline)\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "means_df_all.to_csv(\"results/model_means_shared.csv\")          # overwrite with the extended table\n",
    "pairwise_df_all.to_csv(\"results/pairwise_stats_shared.csv\")    # overwrite with the extended table\n",
    "print(\"Saved head-to-head (incl. keyword baseline): results/model_means_shared.csv, results/pairwise_stats_shared.csv\")\n",
    "\n",
    "# 7) Evaluate each model on the TEST set (LSTM models)\n",
    "test_rows = []\n",
    "test_frames = {}\n",
    "for label, mdl in models:\n",
    "    print(f\"\\n[Test] {label}\")\n",
    "    summ, df_test = eval_on_test_set(mdl, TEST_NPZ, max_samples=MAX_SAMPLES_TEST, sample_rate=44100)\n",
    "    test_rows.append({\"model\": label,\n",
    "                      \"sts_mean\": round(float(summ[\"sts_mean\"]),4),\n",
    "                      \"centroid_err_mean\": round(float(summ[\"centroid_err_mean\"]),4),\n",
    "                      \"mfcc_mean\": round(float(summ[\"mfcc_mean\"]),4),\n",
    "                      \"n\": int(summ[\"n\"])})\n",
    "    test_frames[label] = df_test\n",
    "\n",
    "# 8) Keyword Baseline on the TEST set (non-LSTM)\n",
    "print(\"\\n[Test] Keyword_Baseline\")\n",
    "test_samples = _load_samples_from_npz(TEST_NPZ, max_samples=MAX_SAMPLES_TEST, sample_rate=44100, duration=2.0)\n",
    "kb_test_df = ev_tmp._evaluate_keyword_baseline(test_samples)\n",
    "test_frames[\"Keyword_Baseline\"] = kb_test_df\n",
    "kb_test_s = _summarize_df(kb_test_df)\n",
    "test_rows.append({\n",
    "    \"model\": \"Keyword_Baseline\",\n",
    "    \"sts_mean\": round(kb_test_s[\"sts_mean\"],4),\n",
    "    \"centroid_err_mean\": round(kb_test_s[\"centroid_err_mean\"],4),\n",
    "    \"mfcc_mean\": round(kb_test_s[\"mfcc_mean\"],4),\n",
    "    \"n\": int(kb_test_s[\"n\"])\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame(test_rows).set_index(\"model\")\n",
    "test_df.to_csv(\"results/model_means_test.csv\")\n",
    "print(\"Saved test means (incl. keyword baseline): results/model_means_test.csv\")\n",
    "\n",
    "# 9) Pretty displays\n",
    "print(\"\\n=== Head-to-Head (Shared Set) Means (incl. Keyword_Baseline) ===\")\n",
    "display(means_df_all)\n",
    "\n",
    "print(\"\\n=== Head-to-Head Pairwise Î” & p-values (incl. Keyword_Baseline) ===\")\n",
    "display(pairwise_df_all)\n",
    "\n",
    "print(\"\\n=== Test Set Means (incl. Keyword_Baseline) ===\")\n",
    "display(test_df)\n",
    "\n",
    "# === 10) Human-readable evaluation report (presentation style) ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIMODAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nShared Validation Set (71 samples)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing all models vs Keyword_Baseline\\n\")\n",
    "\n",
    "base = means_df_all.loc[\"Keyword_Baseline\"]\n",
    "for model in [\"LSTMABAR_Baseline\", \"FineTune_A\", \"FineTune_B\"]:\n",
    "    m = means_df_all.loc[model]\n",
    "    print(f\"\\n[{model}]\")\n",
    "    print(\"-\"*80)\n",
    "    # % changes vs baseline (Keyword_Baseline)\n",
    "    sts_delta = ((m.sts_mean - base.sts_mean) / base.sts_mean) * 100\n",
    "    sc_delta  = ((base.centroid_err_mean - m.centroid_err_mean) / base.centroid_err_mean) * 100\n",
    "    mfcc_delta= ((m.mfcc_mean - base.mfcc_mean) / base.mfcc_mean) * 100\n",
    "    print(f\"STS Mean: {m.sts_mean:.3f}  ({sts_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"Spectral Centroid Error: {m.centroid_err_mean:.0f} Hz  ({sc_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"MFCC Similarity: {m.mfcc_mean:.3f}  ({mfcc_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"n = {int(m.n)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PAIRWISE COMPARISONS (Shared Set)\")\n",
    "print(\"=\"*80)\n",
    "print(pairwise_df_all.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET PERFORMANCE (73 samples)\")\n",
    "print(\"=\"*80)\n",
    "print(\"All results compared against Keyword_Baseline\\n\")\n",
    "\n",
    "tb = test_df.loc[\"Keyword_Baseline\"]\n",
    "for model in [\"LSTMABAR_Baseline\", \"FineTune_A\", \"FineTune_B\"]:\n",
    "    tm = test_df.loc[model]\n",
    "    sts_d = ((tm.sts_mean - tb.sts_mean) / tb.sts_mean) * 100\n",
    "    sc_d  = ((tb.centroid_err_mean - tm.centroid_err_mean) / tb.centroid_err_mean) * 100\n",
    "    mfcc_d= ((tm.mfcc_mean - tb.mfcc_mean) / tb.mfcc_mean) * 100\n",
    "    print(f\"\\n[{model}]\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"STS Mean: {tm.sts_mean:.3f} ({sts_d:+.2f}% vs baseline)\")\n",
    "    print(f\"Spectral Centroid Error: {tm.centroid_err_mean:.0f} Hz ({sc_d:+.2f}% vs baseline)\")\n",
    "    print(f\"MFCC Similarity: {tm.mfcc_mean:.3f} ({mfcc_d:+.2f}% vs baseline)\")\n",
    "    print(f\"n = {int(tm.n)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "ðŸ“ TEXT UNDERSTANDING (STS)\n",
    " â€¢ â‰¥ 0.75 â†’ Excellent\n",
    " â€¢ 0.60â€“0.75 â†’ Good\n",
    " â€¢ < 0.60 â†’ Needs improvement\n",
    "\n",
    "ðŸŽµ BRIGHTNESS ACCURACY (Spectral Centroid Error)\n",
    " â€¢ < 200 Hz â†’ Excellent\n",
    " â€¢ < 500 Hz â†’ Good\n",
    " â€¢ > 500 Hz â†’ Needs improvement\n",
    "\n",
    "ðŸŽ¸ TIMBRE QUALITY (MFCC Similarity)\n",
    " â€¢ > 0.80 â†’ Excellent\n",
    " â€¢ > 0.65 â†’ Good\n",
    " â€¢ â‰¤ 0.65 â†’ Needs improvement\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ Detailed CSVs in /results :\")\n",
    "print(\"   â€¢ model_means_shared.csv\")\n",
    "print(\"   â€¢ pairwise_stats_shared.csv\")\n",
    "print(\"   â€¢ model_means_test.csv\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab3py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
