{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multimodal Evaluation Metrics for LSTMABAR\n",
    "Implements STS, Spectral Centroid Error, and MFCC Similarity\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from lstmabar_model import LSTMABAR\n",
    "from improved_text_encoders import ImprovedTextEncoder, HFBackboneTextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMABAREvaluator:\n",
    "    \"\"\"\n",
    "    Complete evaluation framework for LSTMABAR model\n",
    "    Implements text-side and audio-side metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LSTMABAR,\n",
    "        text_model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        sample_rate: int = 44100\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Text encoder for STS\n",
    "        self.text_encoder = SentenceTransformer(text_model_name)\n",
    "        \n",
    "        # Archetype descriptors for mapping\n",
    "        self.archetype_descriptors = {\n",
    "            'sine': 'smooth pure warm mellow soft gentle flowing',\n",
    "            'square': 'digital harsh buzzy retro electronic robotic',\n",
    "            'sawtooth': 'bright cutting metallic sharp aggressive',\n",
    "            'triangle': 'hollow woody muted filtered organic',\n",
    "            'noise': 'rough textured grainy distorted chaotic'\n",
    "        }\n",
    "        \n",
    "        # Expected spectral centroids for common descriptors (Hz)\n",
    "        self.centroid_map = {\n",
    "            'bright': 3500, 'dark': 800, 'warm': 1000, 'harsh': 4000,\n",
    "            'mellow': 1200, 'sharp': 5000, 'smooth': 1500, 'soft': 1100,\n",
    "            'aggressive': 3800, 'gentle': 1300, 'cutting': 4200\n",
    "        }\n",
    "    \n",
    "    # ==================== TEXT-SIDE EVALUATION (STS) ====================\n",
    "    \n",
    "    def compute_sts(\n",
    "        self,\n",
    "        input_description: str,\n",
    "        predicted_archetype_weights: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Semantic Textual Similarity: Does model understand description?\n",
    "        \n",
    "        Args:\n",
    "            input_description: Original user description\n",
    "            predicted_archetype_weights: Model's predicted archetype weights (5,)\n",
    "        \n",
    "        Returns:\n",
    "            STS score (0-1, higher is better)\n",
    "        \"\"\"\n",
    "        # Convert archetype weights to descriptive text\n",
    "        predicted_description = self._archetypes_to_text(predicted_archetype_weights)\n",
    "        \n",
    "        # Encode both descriptions\n",
    "        emb_input = self.text_encoder.encode(input_description, convert_to_tensor=True)\n",
    "        emb_predicted = self.text_encoder.encode(predicted_description, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        sts_score = util.cos_sim(emb_input, emb_predicted).item()\n",
    "        \n",
    "        return sts_score\n",
    "    \n",
    "    def _archetypes_to_text(\n",
    "        self,\n",
    "        archetype_weights: np.ndarray,\n",
    "        threshold: float = 0.1\n",
    "    ) -> str:\n",
    "        \"\"\"Convert archetype mixture to natural language\"\"\"\n",
    "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
    "        \n",
    "        # Get top contributing archetypes\n",
    "        descriptions = []\n",
    "        for i, (name, weight) in enumerate(zip(archetype_names, archetype_weights)):\n",
    "            if weight > threshold:\n",
    "                descriptions.append(self.archetype_descriptors[name])\n",
    "        \n",
    "        return ' '.join(descriptions) if descriptions else 'neutral sound'\n",
    "    \n",
    "    # ==================== AUDIO-SIDE EVALUATION ====================\n",
    "    \n",
    "    def spectral_centroid_error(\n",
    "        self,\n",
    "        output_audio: np.ndarray,\n",
    "        target_description: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measures brightness accuracy (Hz error)\n",
    "        \n",
    "        Args:\n",
    "            output_audio: Generated audio array\n",
    "            target_description: Target description\n",
    "        \n",
    "        Returns:\n",
    "            Absolute error in Hz (lower is better)\n",
    "        \"\"\"\n",
    "        # Compute actual spectral centroid\n",
    "        centroid = librosa.feature.spectral_centroid(\n",
    "            y=output_audio,\n",
    "            sr=self.sample_rate\n",
    "        )\n",
    "        actual_centroid = np.mean(centroid)\n",
    "        \n",
    "        # Determine expected centroid from description\n",
    "        expected_centroid = self._description_to_centroid(target_description)\n",
    "        \n",
    "        # Calculate error\n",
    "        error_hz = abs(actual_centroid - expected_centroid)\n",
    "        \n",
    "        return error_hz\n",
    "    \n",
    "    def _description_to_centroid(self, description: str) -> float:\n",
    "        \"\"\"Map description to expected spectral centroid\"\"\"\n",
    "        words = description.lower().split()\n",
    "        expected_centroids = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.centroid_map:\n",
    "                expected_centroids.append(self.centroid_map[word])\n",
    "        \n",
    "        if expected_centroids:\n",
    "            return np.mean(expected_centroids)\n",
    "        \n",
    "        return 2000  # Neutral default\n",
    "    \n",
    "    def mfcc_similarity(\n",
    "        self,\n",
    "        output_audio: np.ndarray,\n",
    "        reference_audio: np.ndarray,\n",
    "        n_mfcc: int = 13\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measures timbral similarity (0-1, higher is better)\n",
    "        \n",
    "        Args:\n",
    "            output_audio: Generated audio array\n",
    "            reference_audio: Target/reference audio array\n",
    "            n_mfcc: Number of MFCC coefficients\n",
    "        \n",
    "        Returns:\n",
    "            Cosine similarity (1 is perfect match, >0.8 is good)\n",
    "        \"\"\"\n",
    "        # Extract MFCCs from both signals\n",
    "        mfcc_output = librosa.feature.mfcc(\n",
    "            y=output_audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mfcc=n_mfcc\n",
    "        )\n",
    "        mfcc_reference = librosa.feature.mfcc(\n",
    "            y=reference_audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mfcc=n_mfcc\n",
    "        )\n",
    "        \n",
    "        # Average across time\n",
    "        mfcc_output_mean = np.mean(mfcc_output, axis=1).reshape(1, -1)\n",
    "        mfcc_reference_mean = np.mean(mfcc_reference, axis=1).reshape(1, -1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(mfcc_output_mean, mfcc_reference_mean)[0][0]\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    # ==================== COMPREHENSIVE EVALUATION ====================\n",
    "    \n",
    "    def evaluate_single_transformation(\n",
    "        self,\n",
    "        input_audio: np.ndarray,\n",
    "        input_description: str,\n",
    "        output_audio: np.ndarray,\n",
    "        reference_audio: np.ndarray,\n",
    "        predicted_weights: np.ndarray\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Complete evaluation of a single transformation\n",
    "        \n",
    "        Returns:\n",
    "            Dict with all metrics\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            # Text-side (NLP understanding)\n",
    "            'sts_score': self.compute_sts(input_description, predicted_weights),\n",
    "            \n",
    "            # Audio-side (transformation quality)\n",
    "            'spectral_centroid_error_hz': self.spectral_centroid_error(\n",
    "                output_audio, input_description\n",
    "            ),\n",
    "            'mfcc_similarity': self.mfcc_similarity(output_audio, reference_audio)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        generate_reference: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set\n",
    "        \n",
    "        Args:\n",
    "            test_samples: List of dicts with 'audio', 'description', 'target_weights'\n",
    "            generate_reference: If True, generate reference audio from target weights\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with results for each sample\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        \n",
    "        print(f\"Evaluating model on {len(test_samples)} samples...\")\n",
    "        \n",
    "        for i, sample in enumerate(test_samples):\n",
    "            input_audio = sample['audio']\n",
    "            description = sample['description']\n",
    "            target_weights = sample['target_weights']\n",
    "            \n",
    "            # Convert to tensor\n",
    "            audio_tensor = torch.from_numpy(input_audio).unsqueeze(0).float().to(self.model.device)\n",
    "            \n",
    "            # Model inference\n",
    "            with torch.no_grad():\n",
    "                transformed, metadata = self.model.inference([description], audio_tensor)\n",
    "            \n",
    "            # Convert back to numpy\n",
    "            output_audio = transformed[0].cpu().numpy()\n",
    "            predicted_weights = metadata['predicted_weights'][0]\n",
    "            \n",
    "            # Generate reference audio if needed\n",
    "            if generate_reference:\n",
    "                # Use archetype generator to create ideal sound\n",
    "                reference_audio = self._generate_reference_audio(target_weights)\n",
    "            else:\n",
    "                reference_audio = input_audio  # Fallback\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_single_transformation(\n",
    "                input_audio,\n",
    "                description,\n",
    "                output_audio,\n",
    "                reference_audio,\n",
    "                predicted_weights\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            metrics['description'] = description\n",
    "            metrics['sample_idx'] = i\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i+1}/{len(test_samples)} samples\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _generate_reference_audio(\n",
    "        self,\n",
    "        archetype_weights: np.ndarray,\n",
    "        duration: float = 2.0\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Generate reference audio from archetype mixture\"\"\"\n",
    "        n_samples = int(self.sample_rate * duration)\n",
    "        t = np.linspace(0, duration, n_samples, endpoint=False)\n",
    "        frequency = 440  # A4\n",
    "        \n",
    "        audio = np.zeros(n_samples)\n",
    "        \n",
    "        # Generate each archetype component\n",
    "        archetypes = {\n",
    "            'sine': np.sin(2 * np.pi * frequency * t),\n",
    "            'square': np.sign(np.sin(2 * np.pi * frequency * t)),\n",
    "            'sawtooth': 2 * (t * frequency - np.floor(0.5 + t * frequency)),\n",
    "            'triangle': 2 * np.abs(2 * (t * frequency - np.floor(t * frequency + 0.5))) - 1,\n",
    "            'noise': np.random.randn(n_samples) * 0.3\n",
    "        }\n",
    "        \n",
    "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
    "        \n",
    "        # Mix according to weights\n",
    "        for i, name in enumerate(archetype_names):\n",
    "            audio += archetype_weights[i] * archetypes[name]\n",
    "        \n",
    "        # Normalize\n",
    "        audio = audio / (np.max(np.abs(audio)) + 1e-8) * 0.95\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    def compare_with_baseline(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        baseline_model: Optional[object] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare LSTMABAR with baseline model\n",
    "        \n",
    "        Args:\n",
    "            test_samples: Test samples\n",
    "            baseline_model: Baseline model (if None, uses simple keyword matching)\n",
    "        \n",
    "        Returns:\n",
    "            Comparison DataFrame\n",
    "        \"\"\"\n",
    "        # Evaluate LSTMABAR\n",
    "        lstmabar_results = self.evaluate_model(test_samples)\n",
    "        \n",
    "        # Evaluate baseline\n",
    "        if baseline_model is None:\n",
    "            baseline_results = self._evaluate_keyword_baseline(test_samples)\n",
    "        else:\n",
    "            baseline_results = baseline_model.evaluate(test_samples)\n",
    "        \n",
    "        # Compute statistics\n",
    "        comparison = self._compute_comparison_stats(lstmabar_results, baseline_results)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _evaluate_keyword_baseline(self, test_samples: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate simple keyword-matching baseline\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Simple keyword ‚Üí archetype mapping\n",
    "        keyword_map = {\n",
    "            'bright': [0.1, 0.1, 0.6, 0.1, 0.1],  # Mostly sawtooth\n",
    "            'warm': [0.6, 0.1, 0.1, 0.2, 0.0],    # Mostly sine\n",
    "            'harsh': [0.1, 0.5, 0.2, 0.1, 0.1],   # Mostly square\n",
    "            'smooth': [0.6, 0.1, 0.1, 0.2, 0.0],  # Mostly sine\n",
    "            'distorted': [0.1, 0.2, 0.2, 0.1, 0.4] # High noise\n",
    "        }\n",
    "        \n",
    "        for i, sample in enumerate(test_samples):\n",
    "            description = sample['description'].lower()\n",
    "            \n",
    "            # Find matching keywords\n",
    "            predicted_weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform default\n",
    "            for keyword, weights in keyword_map.items():\n",
    "                if keyword in description:\n",
    "                    predicted_weights = np.array(weights)\n",
    "                    break\n",
    "            \n",
    "            # Simple transformation (just apply gain based on brightness)\n",
    "            output_audio = sample['audio'].copy()\n",
    "            if 'bright' in description:\n",
    "                # Boost high frequencies (simplified)\n",
    "                output_audio = output_audio * 1.2\n",
    "            \n",
    "            reference_audio = self._generate_reference_audio(sample['target_weights'])\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_single_transformation(\n",
    "                sample['audio'],\n",
    "                description,\n",
    "                output_audio,\n",
    "                reference_audio,\n",
    "                predicted_weights\n",
    "            )\n",
    "            \n",
    "            metrics['description'] = description\n",
    "            metrics['sample_idx'] = i\n",
    "            results.append(metrics)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _compute_comparison_stats(\n",
    "        self,\n",
    "        lstmabar_results: pd.DataFrame,\n",
    "        baseline_results: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute statistical comparison\"\"\"\n",
    "        metrics = ['sts_score', 'spectral_centroid_error_hz', 'mfcc_similarity']\n",
    "        \n",
    "        comparison = {\n",
    "            'Metric': [],\n",
    "            'Baseline_Mean': [],\n",
    "            'Baseline_Std': [],\n",
    "            'LSTMABAR_Mean': [],\n",
    "            'LSTMABAR_Std': [],\n",
    "            'Improvement': [],\n",
    "            'p_value': []\n",
    "        }\n",
    "        \n",
    "        for metric in metrics:\n",
    "            baseline_vals = baseline_results[metric].values\n",
    "            lstmabar_vals = lstmabar_results[metric].values\n",
    "            \n",
    "            baseline_mean = np.mean(baseline_vals)\n",
    "            baseline_std = np.std(baseline_vals)\n",
    "            lstmabar_mean = np.mean(lstmabar_vals)\n",
    "            lstmabar_std = np.std(lstmabar_vals)\n",
    "            \n",
    "            comparison['Metric'].append(metric)\n",
    "            comparison['Baseline_Mean'].append(f\"{baseline_mean:.4f}\")\n",
    "            comparison['Baseline_Std'].append(f\"{baseline_std:.4f}\")\n",
    "            comparison['LSTMABAR_Mean'].append(f\"{lstmabar_mean:.4f}\")\n",
    "            comparison['LSTMABAR_Std'].append(f\"{lstmabar_std:.4f}\")\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if 'error' in metric.lower():\n",
    "                # Lower is better\n",
    "                improvement = ((baseline_mean - lstmabar_mean) / baseline_mean) * 100\n",
    "            else:\n",
    "                # Higher is better\n",
    "                improvement = ((lstmabar_mean - baseline_mean) / baseline_mean) * 100\n",
    "            \n",
    "            comparison['Improvement'].append(f\"{improvement:.2f}%\")\n",
    "            \n",
    "            # Paired t-test\n",
    "            _, p_val = ttest_rel(baseline_vals, lstmabar_vals)\n",
    "            comparison['p_value'].append(f\"{p_val:.4f}\")\n",
    "        \n",
    "        return pd.DataFrame(comparison)\n",
    "    \n",
    "    def generate_evaluation_report(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        save_path: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LSTMABAR MODEL EVALUATION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = self.evaluate_model(test_samples)\n",
    "        \n",
    "        # Compare with baseline\n",
    "        comparison = self.compare_with_baseline(test_samples)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPARISON WITH BASELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(comparison.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INTERPRETATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # STS interpretation\n",
    "        sts_mean = results['sts_score'].mean()\n",
    "        print(f\"\\nüìù TEXT UNDERSTANDING (STS Score):\")\n",
    "        print(f\"   Mean: {sts_mean:.3f}\")\n",
    "        if sts_mean > 0.75:\n",
    "            print(f\"   ‚úì Excellent semantic understanding!\")\n",
    "        elif sts_mean > 0.60:\n",
    "            print(f\"   ‚Üí Good semantic understanding\")\n",
    "        else:\n",
    "            print(f\"   ‚ö† Needs improvement in NLP comprehension\")\n",
    "        \n",
    "        # Spectral centroid interpretation\n",
    "        sc_mean = results['spectral_centroid_error_hz'].mean()\n",
    "        print(f\"\\nüéµ BRIGHTNESS ACCURACY (Spectral Centroid Error):\")\n",
    "        print(f\"   Mean: {sc_mean:.0f} Hz\")\n",
    "        if sc_mean < 200:\n",
    "            print(f\"   ‚úì Excellent brightness targeting!\")\n",
    "        elif sc_mean < 500:\n",
    "            print(f\"   ‚Üí Good brightness control\")\n",
    "        else:\n",
    "            print(f\"   ‚ö† Brightness targeting needs improvement\")\n",
    "        \n",
    "        # MFCC interpretation\n",
    "        mfcc_mean = results['mfcc_similarity'].mean()\n",
    "        print(f\"\\nüé∏ TIMBRE QUALITY (MFCC Similarity):\")\n",
    "        print(f\"   Mean: {mfcc_mean:.3f}\")\n",
    "        if mfcc_mean > 0.80:\n",
    "            print(f\"   ‚úì Excellent timbral matching!\")\n",
    "        elif mfcc_mean > 0.65:\n",
    "            print(f\"   ‚Üí Good timbral similarity\")\n",
    "        else:\n",
    "            print(f\"   ‚ö† Timbre quality needs improvement\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # Save detailed results\n",
    "        if save_path:\n",
    "            results.to_csv(save_path, index=False)\n",
    "            comparison.to_csv(save_path.replace('.csv', '_comparison.csv'), index=False)\n",
    "            print(f\"\\nDetailed results saved to {save_path}\")\n",
    "\n",
    "    def evaluate_model_on_test_set(\n",
    "        self,\n",
    "        test_data_path: str,\n",
    "        max_samples: Optional[int] = None,\n",
    "        save_results: bool = True,\n",
    "        results_path: str = 'test_results.csv'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set from MusicCaps\n",
    "        \n",
    "        Args:\n",
    "            test_data_path: Path to test .npz file\n",
    "            max_samples: Max samples to evaluate (None = all)\n",
    "            save_results: Whether to save results\n",
    "            results_path: Path to save results\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EVALUATING ON TEST SET\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"Loading test data from {test_data_path}...\")\n",
    "        data = np.load(test_data_path, allow_pickle=True)\n",
    "        \n",
    "        vectors = data['archetype_vectors']\n",
    "        descriptions = data['descriptions'].tolist()\n",
    "        audio_paths = data['audio_paths'].tolist()\n",
    "        \n",
    "        n_samples = min(len(descriptions), max_samples) if max_samples else len(descriptions)\n",
    "        print(f\"Evaluating on {n_samples} test samples\\n\")\n",
    "        \n",
    "        # Prepare test samples\n",
    "        test_samples = []\n",
    "        for i in range(n_samples):\n",
    "            audio_path = audio_paths[i]\n",
    "            \n",
    "            # Check if audio exists\n",
    "            if not Path(audio_path).exists():\n",
    "                print(f\"Skipping {i}: audio file not found\")\n",
    "                continue\n",
    "            \n",
    "            # Load audio\n",
    "            try:\n",
    "                audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=2.0)\n",
    "                \n",
    "                # Pad or trim\n",
    "                target_length = int(self.sample_rate * 2.0)\n",
    "                if len(audio) < target_length:\n",
    "                    audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target_length]\n",
    "                \n",
    "                test_samples.append({\n",
    "                    'audio': audio,\n",
    "                    'description': descriptions[i],\n",
    "                    'target_weights': vectors[i]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {audio_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully loaded {len(test_samples)} test samples\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results_df = self.evaluate_model(test_samples, generate_reference=True)\n",
    "        \n",
    "        # Save results\n",
    "        if save_results:\n",
    "            results_df.to_csv(results_path, index=False)\n",
    "            print(f\"\\n‚úì Results saved to {results_path}\")\n",
    "        \n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper: load with prefix-compat (for FineTune_B)\n",
    "\n",
    "# --- compatibility remap for old text-encoder prefixes (FineTune_B) ---\n",
    "def _compat_remap_text_encoder_keys(sd: dict) -> dict:\n",
    "    if not any(k.startswith(\"text_encoder.backbone.\") for k in sd.keys()):\n",
    "        return sd\n",
    "    m = [\n",
    "        (\"text_encoder.backbone.embeddings.\", \"text_encoder.sentence_model.0.auto_model.embeddings.\"),\n",
    "        (\"text_encoder.backbone.encoder.\",    \"text_encoder.sentence_model.0.auto_model.encoder.\"),\n",
    "        (\"text_encoder.backbone.pooler.\",     \"text_encoder.sentence_model.0.auto_model.pooler.\"),\n",
    "    ]\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        newk = k\n",
    "        for old, new in m:\n",
    "            if k.startswith(old):\n",
    "                newk = new + k[len(old):]\n",
    "                break\n",
    "        out[newk] = v\n",
    "    return out\n",
    "\n",
    "def load_lstmabar_checkpoint(path: str, device: str = None):\n",
    "    import torch\n",
    "    from lstmabar_model import LSTMABAR\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTMABAR(embedding_dim=768, audio_architecture='resnet', sample_rate=44100, device=device)\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    sd = ckpt.get(\"model_state_dict\", ckpt)\n",
    "    sd = _compat_remap_text_encoder_keys(sd)\n",
    "    incompat = model.load_state_dict(sd, strict=False)\n",
    "    print(f\"Loaded {path} (epoch={ckpt.get('epoch','?')}) | missing={len(incompat.missing_keys)} unexpected={len(incompat.unexpected_keys)}\")\n",
    "    return model\n",
    "\n",
    "def load_improved_c_checkpoint(path: str, device: str = None):\n",
    "    import torch\n",
    "    from lstmabar_model import LSTMABAR\n",
    "    \n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create model with same architecture\n",
    "    model = LSTMABAR(\n",
    "        embedding_dim=768, \n",
    "        audio_architecture='resnet', \n",
    "        sample_rate=44100, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Replace text encoder with improved version\n",
    "    model.text_encoder = ImprovedTextEncoder(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "        embedding_dim=768,\n",
    "        projection_depth='deep',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    sd = ckpt.get(\"model_state_dict\", ckpt)\n",
    "    \n",
    "    # Load with strict=False in case of minor key mismatches\n",
    "    incompat = model.load_state_dict(sd, strict=False)\n",
    "    print(f\"Loaded {path} (epoch={ckpt.get('epoch','?')})\")\n",
    "    print(f\"  Missing keys: {len(incompat.missing_keys)}\")\n",
    "    print(f\"  Unexpected keys: {len(incompat.unexpected_keys)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_approach_d_checkpoint(path: str, device: str = None):\n",
    "    \"\"\"\n",
    "    Load Approach D model (HFBackbone with 4 unfrozen layers) from checkpoint\n",
    "    \"\"\"\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create model with same architecture\n",
    "    model = LSTMABAR(\n",
    "        embedding_dim=768,\n",
    "        audio_architecture='resnet',\n",
    "        sample_rate=44100,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Replace text encoder with HFBackbone version\n",
    "    model.text_encoder = HFBackboneTextEncoder(\n",
    "        model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "        embedding_dim=768,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Configure fine-tuning (same as training)\n",
    "    model.text_encoder.configure_fine_tuning(num_unfrozen_layers=4)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    sd = ckpt.get(\"model_state_dict\", ckpt)\n",
    "    \n",
    "    # Load with strict=False in case of minor key mismatches\n",
    "    incompat = model.load_state_dict(sd, strict=False)\n",
    "    \n",
    "    print(f\"Loaded {path} (epoch={ckpt.get('epoch','?')})\")\n",
    "    print(f\"  Missing keys: {len(incompat.missing_keys)}\")\n",
    "    print(f\"  Unexpected keys: {len(incompat.unexpected_keys)}\")\n",
    "    \n",
    "    if len(incompat.missing_keys) > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  First few missing: {incompat.missing_keys[:3]}\")\n",
    "    if len(incompat.unexpected_keys) > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  First few unexpected: {incompat.unexpected_keys[:3]}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a small shared evaluation set (prefer real val clips, fallback to 3 synthetic)\n",
    "\n",
    "def build_shared_eval_samples(val_npz_path: str, k: int = 73, sample_rate: int = 44100, duration: float = 2.0):\n",
    "    D = np.load(val_npz_path, allow_pickle=True)\n",
    "    descs = D['descriptions'].tolist()\n",
    "    paths = D['audio_paths'].tolist()\n",
    "    vecs  = D['archetype_vectors']\n",
    "\n",
    "    tgt_len = int(sample_rate * duration)\n",
    "    samples = []\n",
    "    for i, p in enumerate(paths):\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                y, sr = librosa.load(p, sr=sample_rate, duration=duration)\n",
    "                if len(y) < tgt_len:\n",
    "                    y = np.pad(y, (0, tgt_len - len(y)))\n",
    "                else:\n",
    "                    y = y[:tgt_len]\n",
    "                samples.append({'audio': y, 'description': descs[i], 'target_weights': vecs[i]})\n",
    "                if len(samples) >= k:\n",
    "                    break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Fallback (if nothing loads)\n",
    "    if not samples:\n",
    "        rng = np.random.default_rng(42)\n",
    "        for desc, vec in [\n",
    "            (\"bright and cutting guitar with metallic tone\", np.array([0.1,0.1,0.6,0.1,0.1])),\n",
    "            (\"warm smooth piano melody with gentle sustain\", np.array([0.6,0.05,0.1,0.2,0.05])),\n",
    "            (\"harsh digital synth with buzzy retro sound\",   np.array([0.1,0.55,0.15,0.1,0.1])),\n",
    "        ]:\n",
    "            y = rng.standard_normal(tgt_len).astype(np.float32)\n",
    "            samples.append({'audio': y, 'description': desc, 'target_weights': vec})\n",
    "    print(f\"Shared eval set size: {len(samples)}\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model-vs-Model comparison on the shared set\n",
    "\n",
    "import itertools\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def evaluate_side_by_side(models: list, samples: list, sample_rate: int = 44100):\n",
    "    \"\"\"\n",
    "    models: list of (label, model)\n",
    "    samples: list of dicts with 'audio', 'description', 'target_weights'\n",
    "    \"\"\"\n",
    "    results_per_model = {}\n",
    "    for label, mdl in models:\n",
    "        print(f\"\\n[Eval] {label}\")\n",
    "        ev = LSTMABAREvaluator(mdl, sample_rate=sample_rate)\n",
    "        # Turn numpy audios into the structure expected by evaluate_model\n",
    "        df = ev.evaluate_model(samples, generate_reference=True)\n",
    "        results_per_model[label] = df\n",
    "\n",
    "    # Summaries\n",
    "    def summarize(df):\n",
    "        return {\n",
    "            \"sts_mean\": df[\"sts_score\"].mean(),\n",
    "            \"sts_std\":  df[\"sts_score\"].std(),\n",
    "            \"centroid_err_mean\": df[\"spectral_centroid_error_hz\"].mean(),\n",
    "            \"centroid_err_std\":  df[\"spectral_centroid_error_hz\"].std(),\n",
    "            \"mfcc_mean\": df[\"mfcc_similarity\"].mean(),\n",
    "            \"mfcc_std\":  df[\"mfcc_similarity\"].std(),\n",
    "            \"n\": len(df),\n",
    "        }\n",
    "\n",
    "    summary_rows = []\n",
    "    for label, df in results_per_model.items():\n",
    "        s = summarize(df)\n",
    "        s[\"model\"] = label\n",
    "        summary_rows.append(s)\n",
    "    means_df = pd.DataFrame(summary_rows).set_index(\"model\").round(4)\n",
    "\n",
    "    # Pairwise deltas + paired t-tests\n",
    "    pairs = []\n",
    "    labels = list(results_per_model.keys())\n",
    "    metrics = [\n",
    "        (\"sts_score\", \"higher_better\"),\n",
    "        (\"spectral_centroid_error_hz\", \"lower_better\"),\n",
    "        (\"mfcc_similarity\", \"higher_better\"),\n",
    "    ]\n",
    "    for a, b in itertools.combinations(labels, 2):\n",
    "        A = results_per_model[a]\n",
    "        B = results_per_model[b]\n",
    "        # align by sample_idx (just in case order changed)\n",
    "        A2 = A.sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        B2 = B.sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        for metric, direction in metrics:\n",
    "            a_vals = A2[metric].values\n",
    "            b_vals = B2[metric].values\n",
    "            # improvement relative to b (positive is better)\n",
    "            if direction == \"higher_better\":\n",
    "                delta = a_vals - b_vals\n",
    "            else:\n",
    "                delta = b_vals - a_vals  # lower error => better\n",
    "            tstat, p = ttest_rel(a_vals, b_vals)\n",
    "            pairs.append({\n",
    "                \"A\": a, \"B\": b, \"Metric\": metric,\n",
    "                \"Œî_mean(A_vs_B)\": np.mean(delta),\n",
    "                \"p_value\": p\n",
    "            })\n",
    "\n",
    "    pairwise_df = pd.DataFrame(pairs)\n",
    "    # Sort for readability\n",
    "    pairwise_df = pairwise_df.sort_values([\"Metric\", \"A\", \"B\"]).reset_index(drop=True)\n",
    "    return means_df, pairwise_df, results_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test-set evaluation (your original path)\n",
    "\n",
    "def eval_on_test_set(model, test_npz, max_samples=None, sample_rate=44100):\n",
    "    ev = LSTMABAREvaluator(model, sample_rate=sample_rate)\n",
    "    df = ev.evaluate_model_on_test_set(\n",
    "        test_data_path=test_npz,\n",
    "        max_samples=max_samples,\n",
    "        save_results=False\n",
    "    )\n",
    "    return {\n",
    "        \"sts_mean\": df[\"sts_score\"].mean(),\n",
    "        \"centroid_err_mean\": df[\"spectral_centroid_error_hz\"].mean(),\n",
    "        \"mfcc_mean\": df[\"mfcc_similarity\"].mean(),\n",
    "        \"n\": len(df)\n",
    "    }, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/best_model.pth (epoch=1) | missing=0 unexpected=0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/fine_tune_A/best_model.pth (epoch=8) | missing=0 unexpected=0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/fine_tune_B/best_model.pth (epoch=15) | missing=0 unexpected=0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loading improved text encoder: sentence-transformers/all-mpnet-base-v2\n",
      "Loaded checkpoints/improved_approach_c/best_model.pth (epoch=16)\n",
      "  Missing keys: 0\n",
      "  Unexpected keys: 0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loading HF model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Unfroze 4 layers: 7,097,856 backbone params trainable\n",
      "Unfroze 4 layers: 7,097,856 backbone params trainable\n",
      "Loaded checkpoints/approach_d_proper/best_model.pth (epoch=18)\n",
      "  Missing keys: 0\n",
      "  Unexpected keys: 0\n",
      "Shared eval set size: 73\n",
      "\n",
      "[Eval] LSTMABAR_Baseline\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "\n",
      "[Eval] FineTune_A\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "\n",
      "[Eval] FineTune_B\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "\n",
      "[Eval] FineTune_C\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "\n",
      "[Eval] FineTune_D\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "Saved head-to-head (incl. keyword baseline): results/model_means_shared.csv, results/pairwise_stats_shared.csv\n",
      "\n",
      "[Test] LSTMABAR_Baseline\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 753 test samples\n",
      "\n",
      "Successfully loaded 753 test samples\n",
      "\n",
      "Evaluating model on 753 samples...\n",
      "  Processed 10/753 samples\n",
      "  Processed 20/753 samples\n",
      "  Processed 30/753 samples\n",
      "  Processed 40/753 samples\n",
      "  Processed 50/753 samples\n",
      "  Processed 60/753 samples\n",
      "  Processed 70/753 samples\n",
      "  Processed 80/753 samples\n",
      "  Processed 90/753 samples\n",
      "  Processed 100/753 samples\n",
      "  Processed 110/753 samples\n",
      "  Processed 120/753 samples\n",
      "  Processed 130/753 samples\n",
      "  Processed 140/753 samples\n",
      "  Processed 150/753 samples\n",
      "  Processed 160/753 samples\n",
      "  Processed 170/753 samples\n",
      "  Processed 180/753 samples\n",
      "  Processed 190/753 samples\n",
      "  Processed 200/753 samples\n",
      "  Processed 210/753 samples\n",
      "  Processed 220/753 samples\n",
      "  Processed 230/753 samples\n",
      "  Processed 240/753 samples\n",
      "  Processed 250/753 samples\n",
      "  Processed 260/753 samples\n",
      "  Processed 270/753 samples\n",
      "  Processed 280/753 samples\n",
      "  Processed 290/753 samples\n",
      "  Processed 300/753 samples\n",
      "  Processed 310/753 samples\n",
      "  Processed 320/753 samples\n",
      "  Processed 330/753 samples\n",
      "  Processed 340/753 samples\n",
      "  Processed 350/753 samples\n",
      "  Processed 360/753 samples\n",
      "  Processed 370/753 samples\n",
      "  Processed 380/753 samples\n",
      "  Processed 390/753 samples\n",
      "  Processed 400/753 samples\n",
      "  Processed 410/753 samples\n",
      "  Processed 420/753 samples\n",
      "  Processed 430/753 samples\n",
      "  Processed 440/753 samples\n",
      "  Processed 450/753 samples\n",
      "  Processed 460/753 samples\n",
      "  Processed 470/753 samples\n",
      "  Processed 480/753 samples\n",
      "  Processed 490/753 samples\n",
      "  Processed 500/753 samples\n",
      "  Processed 510/753 samples\n",
      "  Processed 520/753 samples\n",
      "  Processed 530/753 samples\n",
      "  Processed 540/753 samples\n",
      "  Processed 550/753 samples\n",
      "  Processed 560/753 samples\n",
      "  Processed 570/753 samples\n",
      "  Processed 580/753 samples\n",
      "  Processed 590/753 samples\n",
      "  Processed 600/753 samples\n",
      "  Processed 610/753 samples\n",
      "  Processed 620/753 samples\n",
      "  Processed 630/753 samples\n",
      "  Processed 640/753 samples\n",
      "  Processed 650/753 samples\n",
      "  Processed 660/753 samples\n",
      "  Processed 670/753 samples\n",
      "  Processed 680/753 samples\n",
      "  Processed 690/753 samples\n",
      "  Processed 700/753 samples\n",
      "  Processed 710/753 samples\n",
      "  Processed 720/753 samples\n",
      "  Processed 730/753 samples\n",
      "  Processed 740/753 samples\n",
      "  Processed 750/753 samples\n",
      "\n",
      "[Test] FineTune_A\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 753 test samples\n",
      "\n",
      "Successfully loaded 753 test samples\n",
      "\n",
      "Evaluating model on 753 samples...\n",
      "  Processed 10/753 samples\n",
      "  Processed 20/753 samples\n",
      "  Processed 30/753 samples\n",
      "  Processed 40/753 samples\n",
      "  Processed 50/753 samples\n",
      "  Processed 60/753 samples\n",
      "  Processed 70/753 samples\n",
      "  Processed 80/753 samples\n",
      "  Processed 90/753 samples\n",
      "  Processed 100/753 samples\n",
      "  Processed 110/753 samples\n",
      "  Processed 120/753 samples\n",
      "  Processed 130/753 samples\n",
      "  Processed 140/753 samples\n",
      "  Processed 150/753 samples\n",
      "  Processed 160/753 samples\n",
      "  Processed 170/753 samples\n",
      "  Processed 180/753 samples\n",
      "  Processed 190/753 samples\n",
      "  Processed 200/753 samples\n",
      "  Processed 210/753 samples\n",
      "  Processed 220/753 samples\n",
      "  Processed 230/753 samples\n",
      "  Processed 240/753 samples\n",
      "  Processed 250/753 samples\n",
      "  Processed 260/753 samples\n",
      "  Processed 270/753 samples\n",
      "  Processed 280/753 samples\n",
      "  Processed 290/753 samples\n",
      "  Processed 300/753 samples\n",
      "  Processed 310/753 samples\n",
      "  Processed 320/753 samples\n",
      "  Processed 330/753 samples\n",
      "  Processed 340/753 samples\n",
      "  Processed 350/753 samples\n",
      "  Processed 360/753 samples\n",
      "  Processed 370/753 samples\n",
      "  Processed 380/753 samples\n",
      "  Processed 390/753 samples\n",
      "  Processed 400/753 samples\n",
      "  Processed 410/753 samples\n",
      "  Processed 420/753 samples\n",
      "  Processed 430/753 samples\n",
      "  Processed 440/753 samples\n",
      "  Processed 450/753 samples\n",
      "  Processed 460/753 samples\n",
      "  Processed 470/753 samples\n",
      "  Processed 480/753 samples\n",
      "  Processed 490/753 samples\n",
      "  Processed 500/753 samples\n",
      "  Processed 510/753 samples\n",
      "  Processed 520/753 samples\n",
      "  Processed 530/753 samples\n",
      "  Processed 540/753 samples\n",
      "  Processed 550/753 samples\n",
      "  Processed 560/753 samples\n",
      "  Processed 570/753 samples\n",
      "  Processed 580/753 samples\n",
      "  Processed 590/753 samples\n",
      "  Processed 600/753 samples\n",
      "  Processed 610/753 samples\n",
      "  Processed 620/753 samples\n",
      "  Processed 630/753 samples\n",
      "  Processed 640/753 samples\n",
      "  Processed 650/753 samples\n",
      "  Processed 660/753 samples\n",
      "  Processed 670/753 samples\n",
      "  Processed 680/753 samples\n",
      "  Processed 690/753 samples\n",
      "  Processed 700/753 samples\n",
      "  Processed 710/753 samples\n",
      "  Processed 720/753 samples\n",
      "  Processed 730/753 samples\n",
      "  Processed 740/753 samples\n",
      "  Processed 750/753 samples\n",
      "\n",
      "[Test] FineTune_B\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 753 test samples\n",
      "\n",
      "Successfully loaded 753 test samples\n",
      "\n",
      "Evaluating model on 753 samples...\n",
      "  Processed 10/753 samples\n",
      "  Processed 20/753 samples\n",
      "  Processed 30/753 samples\n",
      "  Processed 40/753 samples\n",
      "  Processed 50/753 samples\n",
      "  Processed 60/753 samples\n",
      "  Processed 70/753 samples\n",
      "  Processed 80/753 samples\n",
      "  Processed 90/753 samples\n",
      "  Processed 100/753 samples\n",
      "  Processed 110/753 samples\n",
      "  Processed 120/753 samples\n",
      "  Processed 130/753 samples\n",
      "  Processed 140/753 samples\n",
      "  Processed 150/753 samples\n",
      "  Processed 160/753 samples\n",
      "  Processed 170/753 samples\n",
      "  Processed 180/753 samples\n",
      "  Processed 190/753 samples\n",
      "  Processed 200/753 samples\n",
      "  Processed 210/753 samples\n",
      "  Processed 220/753 samples\n",
      "  Processed 230/753 samples\n",
      "  Processed 240/753 samples\n",
      "  Processed 250/753 samples\n",
      "  Processed 260/753 samples\n",
      "  Processed 270/753 samples\n",
      "  Processed 280/753 samples\n",
      "  Processed 290/753 samples\n",
      "  Processed 300/753 samples\n",
      "  Processed 310/753 samples\n",
      "  Processed 320/753 samples\n",
      "  Processed 330/753 samples\n",
      "  Processed 340/753 samples\n",
      "  Processed 350/753 samples\n",
      "  Processed 360/753 samples\n",
      "  Processed 370/753 samples\n",
      "  Processed 380/753 samples\n",
      "  Processed 390/753 samples\n",
      "  Processed 400/753 samples\n",
      "  Processed 410/753 samples\n",
      "  Processed 420/753 samples\n",
      "  Processed 430/753 samples\n",
      "  Processed 440/753 samples\n",
      "  Processed 450/753 samples\n",
      "  Processed 460/753 samples\n",
      "  Processed 470/753 samples\n",
      "  Processed 480/753 samples\n",
      "  Processed 490/753 samples\n",
      "  Processed 500/753 samples\n",
      "  Processed 510/753 samples\n",
      "  Processed 520/753 samples\n",
      "  Processed 530/753 samples\n",
      "  Processed 540/753 samples\n",
      "  Processed 550/753 samples\n",
      "  Processed 560/753 samples\n",
      "  Processed 570/753 samples\n",
      "  Processed 580/753 samples\n",
      "  Processed 590/753 samples\n",
      "  Processed 600/753 samples\n",
      "  Processed 610/753 samples\n",
      "  Processed 620/753 samples\n",
      "  Processed 630/753 samples\n",
      "  Processed 640/753 samples\n",
      "  Processed 650/753 samples\n",
      "  Processed 660/753 samples\n",
      "  Processed 670/753 samples\n",
      "  Processed 680/753 samples\n",
      "  Processed 690/753 samples\n",
      "  Processed 700/753 samples\n",
      "  Processed 710/753 samples\n",
      "  Processed 720/753 samples\n",
      "  Processed 730/753 samples\n",
      "  Processed 740/753 samples\n",
      "  Processed 750/753 samples\n",
      "\n",
      "[Test] FineTune_C\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 753 test samples\n",
      "\n",
      "Successfully loaded 753 test samples\n",
      "\n",
      "Evaluating model on 753 samples...\n",
      "  Processed 10/753 samples\n",
      "  Processed 20/753 samples\n",
      "  Processed 30/753 samples\n",
      "  Processed 40/753 samples\n",
      "  Processed 50/753 samples\n",
      "  Processed 60/753 samples\n",
      "  Processed 70/753 samples\n",
      "  Processed 80/753 samples\n",
      "  Processed 90/753 samples\n",
      "  Processed 100/753 samples\n",
      "  Processed 110/753 samples\n",
      "  Processed 120/753 samples\n",
      "  Processed 130/753 samples\n",
      "  Processed 140/753 samples\n",
      "  Processed 150/753 samples\n",
      "  Processed 160/753 samples\n",
      "  Processed 170/753 samples\n",
      "  Processed 180/753 samples\n",
      "  Processed 190/753 samples\n",
      "  Processed 200/753 samples\n",
      "  Processed 210/753 samples\n",
      "  Processed 220/753 samples\n",
      "  Processed 230/753 samples\n",
      "  Processed 240/753 samples\n",
      "  Processed 250/753 samples\n",
      "  Processed 260/753 samples\n",
      "  Processed 270/753 samples\n",
      "  Processed 280/753 samples\n",
      "  Processed 290/753 samples\n",
      "  Processed 300/753 samples\n",
      "  Processed 310/753 samples\n",
      "  Processed 320/753 samples\n",
      "  Processed 330/753 samples\n",
      "  Processed 340/753 samples\n",
      "  Processed 350/753 samples\n",
      "  Processed 360/753 samples\n",
      "  Processed 370/753 samples\n",
      "  Processed 380/753 samples\n",
      "  Processed 390/753 samples\n",
      "  Processed 400/753 samples\n",
      "  Processed 410/753 samples\n",
      "  Processed 420/753 samples\n",
      "  Processed 430/753 samples\n",
      "  Processed 440/753 samples\n",
      "  Processed 450/753 samples\n",
      "  Processed 460/753 samples\n",
      "  Processed 470/753 samples\n",
      "  Processed 480/753 samples\n",
      "  Processed 490/753 samples\n",
      "  Processed 500/753 samples\n",
      "  Processed 510/753 samples\n",
      "  Processed 520/753 samples\n",
      "  Processed 530/753 samples\n",
      "  Processed 540/753 samples\n",
      "  Processed 550/753 samples\n",
      "  Processed 560/753 samples\n",
      "  Processed 570/753 samples\n",
      "  Processed 580/753 samples\n",
      "  Processed 590/753 samples\n",
      "  Processed 600/753 samples\n",
      "  Processed 610/753 samples\n",
      "  Processed 620/753 samples\n",
      "  Processed 630/753 samples\n",
      "  Processed 640/753 samples\n",
      "  Processed 650/753 samples\n",
      "  Processed 660/753 samples\n",
      "  Processed 670/753 samples\n",
      "  Processed 680/753 samples\n",
      "  Processed 690/753 samples\n",
      "  Processed 700/753 samples\n",
      "  Processed 710/753 samples\n",
      "  Processed 720/753 samples\n",
      "  Processed 730/753 samples\n",
      "  Processed 740/753 samples\n",
      "  Processed 750/753 samples\n",
      "\n",
      "[Test] FineTune_D\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 753 test samples\n",
      "\n",
      "Successfully loaded 753 test samples\n",
      "\n",
      "Evaluating model on 753 samples...\n",
      "  Processed 10/753 samples\n",
      "  Processed 20/753 samples\n",
      "  Processed 30/753 samples\n",
      "  Processed 40/753 samples\n",
      "  Processed 50/753 samples\n",
      "  Processed 60/753 samples\n",
      "  Processed 70/753 samples\n",
      "  Processed 80/753 samples\n",
      "  Processed 90/753 samples\n",
      "  Processed 100/753 samples\n",
      "  Processed 110/753 samples\n",
      "  Processed 120/753 samples\n",
      "  Processed 130/753 samples\n",
      "  Processed 140/753 samples\n",
      "  Processed 150/753 samples\n",
      "  Processed 160/753 samples\n",
      "  Processed 170/753 samples\n",
      "  Processed 180/753 samples\n",
      "  Processed 190/753 samples\n",
      "  Processed 200/753 samples\n",
      "  Processed 210/753 samples\n",
      "  Processed 220/753 samples\n",
      "  Processed 230/753 samples\n",
      "  Processed 240/753 samples\n",
      "  Processed 250/753 samples\n",
      "  Processed 260/753 samples\n",
      "  Processed 270/753 samples\n",
      "  Processed 280/753 samples\n",
      "  Processed 290/753 samples\n",
      "  Processed 300/753 samples\n",
      "  Processed 310/753 samples\n",
      "  Processed 320/753 samples\n",
      "  Processed 330/753 samples\n",
      "  Processed 340/753 samples\n",
      "  Processed 350/753 samples\n",
      "  Processed 360/753 samples\n",
      "  Processed 370/753 samples\n",
      "  Processed 380/753 samples\n",
      "  Processed 390/753 samples\n",
      "  Processed 400/753 samples\n",
      "  Processed 410/753 samples\n",
      "  Processed 420/753 samples\n",
      "  Processed 430/753 samples\n",
      "  Processed 440/753 samples\n",
      "  Processed 450/753 samples\n",
      "  Processed 460/753 samples\n",
      "  Processed 470/753 samples\n",
      "  Processed 480/753 samples\n",
      "  Processed 490/753 samples\n",
      "  Processed 500/753 samples\n",
      "  Processed 510/753 samples\n",
      "  Processed 520/753 samples\n",
      "  Processed 530/753 samples\n",
      "  Processed 540/753 samples\n",
      "  Processed 550/753 samples\n",
      "  Processed 560/753 samples\n",
      "  Processed 570/753 samples\n",
      "  Processed 580/753 samples\n",
      "  Processed 590/753 samples\n",
      "  Processed 600/753 samples\n",
      "  Processed 610/753 samples\n",
      "  Processed 620/753 samples\n",
      "  Processed 630/753 samples\n",
      "  Processed 640/753 samples\n",
      "  Processed 650/753 samples\n",
      "  Processed 660/753 samples\n",
      "  Processed 670/753 samples\n",
      "  Processed 680/753 samples\n",
      "  Processed 690/753 samples\n",
      "  Processed 700/753 samples\n",
      "  Processed 710/753 samples\n",
      "  Processed 720/753 samples\n",
      "  Processed 730/753 samples\n",
      "  Processed 740/753 samples\n",
      "  Processed 750/753 samples\n",
      "\n",
      "[Test] Keyword_Baseline\n",
      "Saved test means (incl. keyword baseline): results/model_means_test.csv\n",
      "\n",
      "=== Head-to-Head (Shared Set) Means (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sts_mean</th>\n",
       "      <th>sts_std</th>\n",
       "      <th>centroid_err_mean</th>\n",
       "      <th>centroid_err_std</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>mfcc_std</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTMABAR_Baseline</th>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.0629</td>\n",
       "      <td>5096.1058</td>\n",
       "      <td>1927.5491</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4243</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_A</th>\n",
       "      <td>0.2266</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>5143.6208</td>\n",
       "      <td>2035.4270</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.3965</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_B</th>\n",
       "      <td>0.2287</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>6402.9042</td>\n",
       "      <td>1561.7533</td>\n",
       "      <td>0.2703</td>\n",
       "      <td>0.4193</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_C</th>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>5075.6175</td>\n",
       "      <td>1918.5905</td>\n",
       "      <td>0.3930</td>\n",
       "      <td>0.4045</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_D</th>\n",
       "      <td>0.2271</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>5444.4381</td>\n",
       "      <td>1855.3751</td>\n",
       "      <td>0.3633</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword_Baseline</th>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>953.7882</td>\n",
       "      <td>855.1620</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sts_mean  sts_std  centroid_err_mean  centroid_err_std  \\\n",
       "model                                                                       \n",
       "LSTMABAR_Baseline    0.2272   0.0629          5096.1058         1927.5491   \n",
       "FineTune_A           0.2266   0.0641          5143.6208         2035.4270   \n",
       "FineTune_B           0.2287   0.0650          6402.9042         1561.7533   \n",
       "FineTune_C           0.2234   0.0627          5075.6175         1918.5905   \n",
       "FineTune_D           0.2271   0.0641          5444.4381         1855.3751   \n",
       "Keyword_Baseline     0.2261   0.0640           953.7882          855.1620   \n",
       "\n",
       "                   mfcc_mean  mfcc_std   n  \n",
       "model                                       \n",
       "LSTMABAR_Baseline     0.3765    0.4243  73  \n",
       "FineTune_A            0.3611    0.3965  73  \n",
       "FineTune_B            0.2703    0.4193  73  \n",
       "FineTune_C            0.3930    0.4045  73  \n",
       "FineTune_D            0.3633    0.4066  73  \n",
       "Keyword_Baseline      0.5212    0.4161  73  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head-to-Head Pairwise Œî & p-values (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Œî_mean(A_vs_B)</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.090802</td>\n",
       "      <td>2.192595e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.031917</td>\n",
       "      <td>2.116701e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.002214</td>\n",
       "      <td>9.190897e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.160098</td>\n",
       "      <td>1.996997e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.122719</td>\n",
       "      <td>1.873376e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.093016</td>\n",
       "      <td>4.024400e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.250900</td>\n",
       "      <td>4.005433e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>1.540868e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.128181</td>\n",
       "      <td>1.840986e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.157885</td>\n",
       "      <td>2.111803e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.015329</td>\n",
       "      <td>3.183929e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.106131</td>\n",
       "      <td>3.469148e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.016588</td>\n",
       "      <td>1.565490e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.013116</td>\n",
       "      <td>4.759612e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.144769</td>\n",
       "      <td>2.009922e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>1259.283395</td>\n",
       "      <td>2.413206e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-68.003344</td>\n",
       "      <td>5.832839e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>300.817235</td>\n",
       "      <td>6.873550e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-4189.832619</td>\n",
       "      <td>1.060376e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-1327.286739</td>\n",
       "      <td>2.075030e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-958.466161</td>\n",
       "      <td>1.636262e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-5449.116015</td>\n",
       "      <td>2.227130e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>368.820578</td>\n",
       "      <td>3.191655e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-4121.829276</td>\n",
       "      <td>7.346482e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-4490.649854</td>\n",
       "      <td>5.013637e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>47.515043</td>\n",
       "      <td>6.371083e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>1306.798438</td>\n",
       "      <td>1.376522e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-20.488301</td>\n",
       "      <td>8.146560e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>348.332278</td>\n",
       "      <td>5.922767e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-4142.317576</td>\n",
       "      <td>2.286960e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>4.293152e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>1.299316e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.000483</td>\n",
       "      <td>8.494827e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>8.054689e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>7.540026e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>6.274555e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>3.894739e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.003729</td>\n",
       "      <td>6.579673e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>2.354565e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>7.101068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>7.139470e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.001481</td>\n",
       "      <td>5.122381e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>1.889788e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_D</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>9.794701e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>4.872979e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    A                 B                      Metric  \\\n",
       "0          FineTune_A        FineTune_B             mfcc_similarity   \n",
       "1          FineTune_A        FineTune_C             mfcc_similarity   \n",
       "2          FineTune_A        FineTune_D             mfcc_similarity   \n",
       "3          FineTune_A  Keyword_Baseline             mfcc_similarity   \n",
       "4          FineTune_B        FineTune_C             mfcc_similarity   \n",
       "5          FineTune_B        FineTune_D             mfcc_similarity   \n",
       "6          FineTune_B  Keyword_Baseline             mfcc_similarity   \n",
       "7          FineTune_C        FineTune_D             mfcc_similarity   \n",
       "8          FineTune_C  Keyword_Baseline             mfcc_similarity   \n",
       "9          FineTune_D  Keyword_Baseline             mfcc_similarity   \n",
       "10  LSTMABAR_Baseline        FineTune_A             mfcc_similarity   \n",
       "11  LSTMABAR_Baseline        FineTune_B             mfcc_similarity   \n",
       "12  LSTMABAR_Baseline        FineTune_C             mfcc_similarity   \n",
       "13  LSTMABAR_Baseline        FineTune_D             mfcc_similarity   \n",
       "14  LSTMABAR_Baseline  Keyword_Baseline             mfcc_similarity   \n",
       "15         FineTune_A        FineTune_B  spectral_centroid_error_hz   \n",
       "16         FineTune_A        FineTune_C  spectral_centroid_error_hz   \n",
       "17         FineTune_A        FineTune_D  spectral_centroid_error_hz   \n",
       "18         FineTune_A  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "19         FineTune_B        FineTune_C  spectral_centroid_error_hz   \n",
       "20         FineTune_B        FineTune_D  spectral_centroid_error_hz   \n",
       "21         FineTune_B  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "22         FineTune_C        FineTune_D  spectral_centroid_error_hz   \n",
       "23         FineTune_C  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "24         FineTune_D  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "25  LSTMABAR_Baseline        FineTune_A  spectral_centroid_error_hz   \n",
       "26  LSTMABAR_Baseline        FineTune_B  spectral_centroid_error_hz   \n",
       "27  LSTMABAR_Baseline        FineTune_C  spectral_centroid_error_hz   \n",
       "28  LSTMABAR_Baseline        FineTune_D  spectral_centroid_error_hz   \n",
       "29  LSTMABAR_Baseline  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "30         FineTune_A        FineTune_B                   sts_score   \n",
       "31         FineTune_A        FineTune_C                   sts_score   \n",
       "32         FineTune_A        FineTune_D                   sts_score   \n",
       "33         FineTune_A  Keyword_Baseline                   sts_score   \n",
       "34         FineTune_B        FineTune_C                   sts_score   \n",
       "35         FineTune_B        FineTune_D                   sts_score   \n",
       "36         FineTune_B  Keyword_Baseline                   sts_score   \n",
       "37         FineTune_C        FineTune_D                   sts_score   \n",
       "38         FineTune_C  Keyword_Baseline                   sts_score   \n",
       "39         FineTune_D  Keyword_Baseline                   sts_score   \n",
       "40  LSTMABAR_Baseline        FineTune_A                   sts_score   \n",
       "41  LSTMABAR_Baseline        FineTune_B                   sts_score   \n",
       "42  LSTMABAR_Baseline        FineTune_C                   sts_score   \n",
       "43  LSTMABAR_Baseline        FineTune_D                   sts_score   \n",
       "44  LSTMABAR_Baseline  Keyword_Baseline                   sts_score   \n",
       "\n",
       "    Œî_mean(A_vs_B)       p_value  \n",
       "0         0.090802  2.192595e-03  \n",
       "1        -0.031917  2.116701e-02  \n",
       "2        -0.002214  9.190897e-01  \n",
       "3        -0.160098  1.996997e-06  \n",
       "4        -0.122719  1.873376e-05  \n",
       "5        -0.093016  4.024400e-05  \n",
       "6        -0.250900  4.005433e-07  \n",
       "7         0.029703  1.540868e-01  \n",
       "8        -0.128181  1.840986e-05  \n",
       "9        -0.157885  2.111803e-04  \n",
       "10        0.015329  3.183929e-01  \n",
       "11        0.106131  3.469148e-05  \n",
       "12       -0.016588  1.565490e-01  \n",
       "13        0.013116  4.759612e-01  \n",
       "14       -0.144769  2.009922e-05  \n",
       "15     1259.283395  2.413206e-15  \n",
       "16      -68.003344  5.832839e-01  \n",
       "17      300.817235  6.873550e-03  \n",
       "18    -4189.832619  1.060376e-25  \n",
       "19    -1327.286739  2.075030e-15  \n",
       "20     -958.466161  1.636262e-10  \n",
       "21    -5449.116015  2.227130e-38  \n",
       "22      368.820578  3.191655e-04  \n",
       "23    -4121.829276  7.346482e-27  \n",
       "24    -4490.649854  5.013637e-30  \n",
       "25       47.515043  6.371083e-01  \n",
       "26     1306.798438  1.376522e-20  \n",
       "27      -20.488301  8.146560e-01  \n",
       "28      348.332278  5.922767e-04  \n",
       "29    -4142.317576  2.286960e-27  \n",
       "30       -0.002020  4.293152e-01  \n",
       "31        0.003245  1.299316e-01  \n",
       "32       -0.000483  8.494827e-01  \n",
       "33        0.000516  8.054689e-01  \n",
       "34        0.005265  7.540026e-02  \n",
       "35        0.001536  6.274555e-01  \n",
       "36        0.002536  3.894739e-01  \n",
       "37       -0.003729  6.579673e-02  \n",
       "38       -0.002729  2.354565e-01  \n",
       "39        0.001000  7.101068e-01  \n",
       "40        0.000539  7.139470e-01  \n",
       "41       -0.001481  5.122381e-01  \n",
       "42        0.003784  1.889788e-02  \n",
       "43        0.000055  9.794701e-01  \n",
       "44        0.001055  4.872979e-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Set Means (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sts_mean</th>\n",
       "      <th>centroid_err_mean</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTMABAR_Baseline</th>\n",
       "      <td>0.2199</td>\n",
       "      <td>5149.0363</td>\n",
       "      <td>0.3901</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_A</th>\n",
       "      <td>0.2174</td>\n",
       "      <td>5138.9973</td>\n",
       "      <td>0.3890</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_B</th>\n",
       "      <td>0.2216</td>\n",
       "      <td>6242.5751</td>\n",
       "      <td>0.3091</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_C</th>\n",
       "      <td>0.2178</td>\n",
       "      <td>5214.1119</td>\n",
       "      <td>0.3837</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_D</th>\n",
       "      <td>0.2179</td>\n",
       "      <td>5496.3787</td>\n",
       "      <td>0.3657</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword_Baseline</th>\n",
       "      <td>0.2152</td>\n",
       "      <td>903.4466</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sts_mean  centroid_err_mean  mfcc_mean    n\n",
       "model                                                         \n",
       "LSTMABAR_Baseline    0.2199          5149.0363     0.3901  753\n",
       "FineTune_A           0.2174          5138.9973     0.3890  753\n",
       "FineTune_B           0.2216          6242.5751     0.3091  753\n",
       "FineTune_C           0.2178          5214.1119     0.3837  753\n",
       "FineTune_D           0.2179          5496.3787     0.3657  753\n",
       "Keyword_Baseline     0.2152           903.4466     0.5079  753"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTIMODAL EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Shared Validation Set (71 samples)\n",
      "================================================================================\n",
      "Comparing all models vs Keyword_Baseline\n",
      "\n",
      "\n",
      "[LSTMABAR_Baseline]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.227  (+0.49% vs baseline)\n",
      "Spectral Centroid Error: 5096 Hz  (-434.30% vs baseline)\n",
      "MFCC Similarity: 0.377  (-27.76% vs baseline)\n",
      "n = 73 samples\n",
      "\n",
      "[FineTune_A]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.227  (+0.22% vs baseline)\n",
      "Spectral Centroid Error: 5144 Hz  (-439.28% vs baseline)\n",
      "MFCC Similarity: 0.361  (-30.72% vs baseline)\n",
      "n = 73 samples\n",
      "\n",
      "[FineTune_B]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.229  (+1.15% vs baseline)\n",
      "Spectral Centroid Error: 6403 Hz  (-571.31% vs baseline)\n",
      "MFCC Similarity: 0.270  (-48.14% vs baseline)\n",
      "n = 73 samples\n",
      "\n",
      "[FineTune_C]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.223  (-1.19% vs baseline)\n",
      "Spectral Centroid Error: 5076 Hz  (-432.15% vs baseline)\n",
      "MFCC Similarity: 0.393  (-24.60% vs baseline)\n",
      "n = 73 samples\n",
      "\n",
      "[FineTune_D]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.227  (+0.44% vs baseline)\n",
      "Spectral Centroid Error: 5444 Hz  (-470.82% vs baseline)\n",
      "MFCC Similarity: 0.363  (-30.30% vs baseline)\n",
      "n = 73 samples\n",
      "\n",
      "================================================================================\n",
      "PAIRWISE COMPARISONS (Shared Set)\n",
      "================================================================================\n",
      "                A                B                     Metric  Œî_mean(A_vs_B)      p_value\n",
      "       FineTune_A       FineTune_B            mfcc_similarity        0.090802 2.192595e-03\n",
      "       FineTune_A       FineTune_C            mfcc_similarity       -0.031917 2.116701e-02\n",
      "       FineTune_A       FineTune_D            mfcc_similarity       -0.002214 9.190897e-01\n",
      "       FineTune_A Keyword_Baseline            mfcc_similarity       -0.160098 1.996997e-06\n",
      "       FineTune_B       FineTune_C            mfcc_similarity       -0.122719 1.873376e-05\n",
      "       FineTune_B       FineTune_D            mfcc_similarity       -0.093016 4.024400e-05\n",
      "       FineTune_B Keyword_Baseline            mfcc_similarity       -0.250900 4.005433e-07\n",
      "       FineTune_C       FineTune_D            mfcc_similarity        0.029703 1.540868e-01\n",
      "       FineTune_C Keyword_Baseline            mfcc_similarity       -0.128181 1.840986e-05\n",
      "       FineTune_D Keyword_Baseline            mfcc_similarity       -0.157885 2.111803e-04\n",
      "LSTMABAR_Baseline       FineTune_A            mfcc_similarity        0.015329 3.183929e-01\n",
      "LSTMABAR_Baseline       FineTune_B            mfcc_similarity        0.106131 3.469148e-05\n",
      "LSTMABAR_Baseline       FineTune_C            mfcc_similarity       -0.016588 1.565490e-01\n",
      "LSTMABAR_Baseline       FineTune_D            mfcc_similarity        0.013116 4.759612e-01\n",
      "LSTMABAR_Baseline Keyword_Baseline            mfcc_similarity       -0.144769 2.009922e-05\n",
      "       FineTune_A       FineTune_B spectral_centroid_error_hz     1259.283395 2.413206e-15\n",
      "       FineTune_A       FineTune_C spectral_centroid_error_hz      -68.003344 5.832839e-01\n",
      "       FineTune_A       FineTune_D spectral_centroid_error_hz      300.817235 6.873550e-03\n",
      "       FineTune_A Keyword_Baseline spectral_centroid_error_hz    -4189.832619 1.060376e-25\n",
      "       FineTune_B       FineTune_C spectral_centroid_error_hz    -1327.286739 2.075030e-15\n",
      "       FineTune_B       FineTune_D spectral_centroid_error_hz     -958.466161 1.636262e-10\n",
      "       FineTune_B Keyword_Baseline spectral_centroid_error_hz    -5449.116015 2.227130e-38\n",
      "       FineTune_C       FineTune_D spectral_centroid_error_hz      368.820578 3.191655e-04\n",
      "       FineTune_C Keyword_Baseline spectral_centroid_error_hz    -4121.829276 7.346482e-27\n",
      "       FineTune_D Keyword_Baseline spectral_centroid_error_hz    -4490.649854 5.013637e-30\n",
      "LSTMABAR_Baseline       FineTune_A spectral_centroid_error_hz       47.515043 6.371083e-01\n",
      "LSTMABAR_Baseline       FineTune_B spectral_centroid_error_hz     1306.798438 1.376522e-20\n",
      "LSTMABAR_Baseline       FineTune_C spectral_centroid_error_hz      -20.488301 8.146560e-01\n",
      "LSTMABAR_Baseline       FineTune_D spectral_centroid_error_hz      348.332278 5.922767e-04\n",
      "LSTMABAR_Baseline Keyword_Baseline spectral_centroid_error_hz    -4142.317576 2.286960e-27\n",
      "       FineTune_A       FineTune_B                  sts_score       -0.002020 4.293152e-01\n",
      "       FineTune_A       FineTune_C                  sts_score        0.003245 1.299316e-01\n",
      "       FineTune_A       FineTune_D                  sts_score       -0.000483 8.494827e-01\n",
      "       FineTune_A Keyword_Baseline                  sts_score        0.000516 8.054689e-01\n",
      "       FineTune_B       FineTune_C                  sts_score        0.005265 7.540026e-02\n",
      "       FineTune_B       FineTune_D                  sts_score        0.001536 6.274555e-01\n",
      "       FineTune_B Keyword_Baseline                  sts_score        0.002536 3.894739e-01\n",
      "       FineTune_C       FineTune_D                  sts_score       -0.003729 6.579673e-02\n",
      "       FineTune_C Keyword_Baseline                  sts_score       -0.002729 2.354565e-01\n",
      "       FineTune_D Keyword_Baseline                  sts_score        0.001000 7.101068e-01\n",
      "LSTMABAR_Baseline       FineTune_A                  sts_score        0.000539 7.139470e-01\n",
      "LSTMABAR_Baseline       FineTune_B                  sts_score       -0.001481 5.122381e-01\n",
      "LSTMABAR_Baseline       FineTune_C                  sts_score        0.003784 1.889788e-02\n",
      "LSTMABAR_Baseline       FineTune_D                  sts_score        0.000055 9.794701e-01\n",
      "LSTMABAR_Baseline Keyword_Baseline                  sts_score        0.001055 4.872979e-01\n",
      "\n",
      "================================================================================\n",
      "TEST SET PERFORMANCE (73 samples)\n",
      "================================================================================\n",
      "All results compared against Keyword_Baseline\n",
      "\n",
      "\n",
      "[LSTMABAR_Baseline]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.220 (+2.18% vs baseline)\n",
      "Spectral Centroid Error: 5149 Hz (-469.93% vs baseline)\n",
      "MFCC Similarity: 0.390 (-23.19% vs baseline)\n",
      "n = 753 samples\n",
      "\n",
      "[FineTune_A]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.217 (+1.02% vs baseline)\n",
      "Spectral Centroid Error: 5139 Hz (-468.82% vs baseline)\n",
      "MFCC Similarity: 0.389 (-23.41% vs baseline)\n",
      "n = 753 samples\n",
      "\n",
      "[FineTune_B]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.222 (+2.97% vs baseline)\n",
      "Spectral Centroid Error: 6243 Hz (-590.97% vs baseline)\n",
      "MFCC Similarity: 0.309 (-39.14% vs baseline)\n",
      "n = 753 samples\n",
      "\n",
      "[FineTune_C]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.218 (+1.21% vs baseline)\n",
      "Spectral Centroid Error: 5214 Hz (-477.14% vs baseline)\n",
      "MFCC Similarity: 0.384 (-24.45% vs baseline)\n",
      "n = 753 samples\n",
      "\n",
      "[FineTune_D]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.218 (+1.25% vs baseline)\n",
      "Spectral Centroid Error: 5496 Hz (-508.38% vs baseline)\n",
      "MFCC Similarity: 0.366 (-28.00% vs baseline)\n",
      "n = 753 samples\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "üìù TEXT UNDERSTANDING (STS)\n",
      " ‚Ä¢ ‚â• 0.75 ‚Üí Excellent\n",
      " ‚Ä¢ 0.60‚Äì0.75 ‚Üí Good\n",
      " ‚Ä¢ < 0.60 ‚Üí Needs improvement\n",
      "\n",
      "üéµ BRIGHTNESS ACCURACY (Spectral Centroid Error)\n",
      " ‚Ä¢ < 200 Hz ‚Üí Excellent\n",
      " ‚Ä¢ < 500 Hz ‚Üí Good\n",
      " ‚Ä¢ > 500 Hz ‚Üí Needs improvement\n",
      "\n",
      "üé∏ TIMBRE QUALITY (MFCC Similarity)\n",
      " ‚Ä¢ > 0.80 ‚Üí Excellent\n",
      " ‚Ä¢ > 0.65 ‚Üí Good\n",
      " ‚Ä¢ ‚â§ 0.65 ‚Üí Needs improvement\n",
      "\n",
      "================================================================================\n",
      "‚úì Detailed CSVs in /results :\n",
      "   ‚Ä¢ model_means_shared.csv\n",
      "   ‚Ä¢ pairwise_stats_shared.csv\n",
      "   ‚Ä¢ model_means_test.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "### Run the eval metrics:\n",
    "\n",
    "# --- Small helper to summarize a results DF (same metrics everywhere) ---\n",
    "def _summarize_df(df: pd.DataFrame) -> dict:\n",
    "    return {\n",
    "        \"sts_mean\": float(df[\"sts_score\"].mean()),\n",
    "        \"sts_std\":  float(df[\"sts_score\"].std()),\n",
    "        \"centroid_err_mean\": float(df[\"spectral_centroid_error_hz\"].mean()),\n",
    "        \"centroid_err_std\":  float(df[\"spectral_centroid_error_hz\"].std()),\n",
    "        \"mfcc_mean\": float(df[\"mfcc_similarity\"].mean()),\n",
    "        \"mfcc_std\":  float(df[\"mfcc_similarity\"].std()),\n",
    "        \"n\": int(len(df)),\n",
    "    }\n",
    "\n",
    "# --- helper to (re)build pairwise stats from a dict of label->df ---\n",
    "def _pairwise_from_frames(frames: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    labels = list(frames.keys())\n",
    "    metrics = [\n",
    "        (\"sts_score\", \"higher_better\"),\n",
    "        (\"spectral_centroid_error_hz\", \"lower_better\"),\n",
    "        (\"mfcc_similarity\", \"higher_better\"),\n",
    "    ]\n",
    "    for a, b in itertools.combinations(labels, 2):\n",
    "        A = frames[a].sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        B = frames[b].sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        for metric, direction in metrics:\n",
    "            a_vals = A[metric].values\n",
    "            b_vals = B[metric].values\n",
    "            # improvement relative to B (positive means A better)\n",
    "            if direction == \"higher_better\":\n",
    "                delta = a_vals - b_vals\n",
    "            else:\n",
    "                delta = b_vals - a_vals  # lower error => better\n",
    "            _, p = ttest_rel(a_vals, b_vals)\n",
    "            rows.append({\n",
    "                \"A\": a, \"B\": b, \"Metric\": metric,\n",
    "                \"Œî_mean(A_vs_B)\": float(np.mean(delta)),\n",
    "                \"p_value\": float(p)\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"Metric\",\"A\",\"B\"]).reset_index(drop=True)\n",
    "\n",
    "# --- helper to load raw samples from an NPZ into a list for baselines ---\n",
    "def _load_samples_from_npz(npz_path: str, max_samples=None, sample_rate=44100, duration=2.0):\n",
    "    D = np.load(npz_path, allow_pickle=True)\n",
    "    descs = D[\"descriptions\"].tolist()\n",
    "    paths = D[\"audio_paths\"].tolist()\n",
    "    vecs  = D[\"archetype_vectors\"]\n",
    "    tgt_len = int(sample_rate * duration)\n",
    "\n",
    "    samples = []\n",
    "    count = 0\n",
    "    for i, p in enumerate(paths):\n",
    "        if max_samples is not None and count >= max_samples:\n",
    "            break\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                y, sr = librosa.load(p, sr=sample_rate, duration=duration)\n",
    "                if len(y) < tgt_len:\n",
    "                    y = np.pad(y, (0, tgt_len - len(y)))\n",
    "                else:\n",
    "                    y = y[:tgt_len]\n",
    "                samples.append({\n",
    "                    \"audio\": y,\n",
    "                    \"description\": descs[i],\n",
    "                    \"target_weights\": vecs[i],\n",
    "                    \"sample_idx\": i\n",
    "                })\n",
    "                count += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    return samples\n",
    "\n",
    "# --- your existing paths ---\n",
    "BASELINE_CKPT   = \"checkpoints/best_model.pth\"\n",
    "FINETUNE_A_CKPT = \"checkpoints/fine_tune_A/best_model.pth\"\n",
    "FINETUNE_B_CKPT = \"checkpoints/fine_tune_B/best_model.pth\"\n",
    "FINETUNE_C_CKPT = \"checkpoints/improved_approach_c/best_model.pth\"\n",
    "FINETUNE_D_CKPT = \"checkpoints/approach_d_proper/best_model.pth\"\n",
    "\n",
    "VAL_NPZ  = \"musiccaps_training_data_val.npz\"\n",
    "TEST_NPZ = \"musiccaps_training_data_test.npz\"\n",
    "\n",
    "MAX_SAMPLES_SHARED = 73   # shared head-to-head set (val)\n",
    "MAX_SAMPLES_TEST   = None # full test or cap with an int\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1) Load models (with compat loader)\n",
    "print(\"Loading models...\")\n",
    "m_base = load_lstmabar_checkpoint(BASELINE_CKPT, device=device)\n",
    "m_A    = load_lstmabar_checkpoint(FINETUNE_A_CKPT, device=device)\n",
    "m_B    = load_lstmabar_checkpoint(FINETUNE_B_CKPT, device=device)\n",
    "m_C    = load_improved_c_checkpoint(FINETUNE_C_CKPT, device=device)\n",
    "m_D    = load_approach_d_checkpoint(FINETUNE_D_CKPT, device=device)\n",
    "\n",
    "# 2) Build shared eval set from VAL (or fallback synthetic inside the helper)\n",
    "shared_samples = build_shared_eval_samples(VAL_NPZ, k=MAX_SAMPLES_SHARED, sample_rate=44100)\n",
    "\n",
    "# 3) Model-vs-Model (LSTM-based models only, first pass)\n",
    "models = [\n",
    "    (\"LSTMABAR_Baseline\", m_base), \n",
    "    (\"FineTune_A\", m_A), \n",
    "    (\"FineTune_B\", m_B), \n",
    "    (\"FineTune_C\", m_C),\n",
    "    (\"FineTune_D\", m_D)  # NEW\n",
    "]\n",
    "\n",
    "means_df, pairwise_df, per_model_frames = evaluate_side_by_side(models, shared_samples, sample_rate=44100)\n",
    "\n",
    "# 4) Add the Keyword Baseline (non-LSTM) on the SAME shared samples\n",
    "ev_tmp = LSTMABAREvaluator(m_base, sample_rate=44100)  # any LSTMABAR instance just to access evaluator\n",
    "kb_shared_df = ev_tmp._evaluate_keyword_baseline(shared_samples)\n",
    "# ensure sample_idx exists for alignment (added by evaluate_model; add here too)\n",
    "if \"sample_idx\" not in kb_shared_df.columns:\n",
    "    kb_shared_df[\"sample_idx\"] = range(len(kb_shared_df))\n",
    "\n",
    "per_model_frames[\"Keyword_Baseline\"] = kb_shared_df\n",
    "\n",
    "# 5) Recompute head-to-head means/pairwise including Keyword_Baseline\n",
    "summary_rows = []\n",
    "for label, df in per_model_frames.items():\n",
    "    s = _summarize_df(df)\n",
    "    s[\"model\"] = label\n",
    "    summary_rows.append(s)\n",
    "means_df_all = pd.DataFrame(summary_rows).set_index(\"model\").round(4)\n",
    "\n",
    "pairwise_df_all = _pairwise_from_frames(per_model_frames)\n",
    "pairwise_df_all[\"Œî_mean(A_vs_B)\"] = pairwise_df_all[\"Œî_mean(A_vs_B)\"].round(6)\n",
    "pairwise_df_all[\"p_value\"] = pairwise_df_all[\"p_value\"].map(lambda x: float(x))\n",
    "\n",
    "# 6) Save head-to-head (with keyword baseline)\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "means_df_all.to_csv(\"results/model_means_shared.csv\")          # overwrite with the extended table\n",
    "pairwise_df_all.to_csv(\"results/pairwise_stats_shared.csv\")    # overwrite with the extended table\n",
    "print(\"Saved head-to-head (incl. keyword baseline): results/model_means_shared.csv, results/pairwise_stats_shared.csv\")\n",
    "\n",
    "# 7) Evaluate each model on the TEST set (LSTM models)\n",
    "test_rows = []\n",
    "test_frames = {}\n",
    "for label, mdl in models:\n",
    "    print(f\"\\n[Test] {label}\")\n",
    "    summ, df_test = eval_on_test_set(mdl, TEST_NPZ, max_samples=MAX_SAMPLES_TEST, sample_rate=44100)\n",
    "    test_rows.append({\"model\": label,\n",
    "                      \"sts_mean\": round(float(summ[\"sts_mean\"]),4),\n",
    "                      \"centroid_err_mean\": round(float(summ[\"centroid_err_mean\"]),4),\n",
    "                      \"mfcc_mean\": round(float(summ[\"mfcc_mean\"]),4),\n",
    "                      \"n\": int(summ[\"n\"])})\n",
    "    test_frames[label] = df_test\n",
    "\n",
    "# 8) Keyword Baseline on the TEST set (non-LSTM)\n",
    "print(\"\\n[Test] Keyword_Baseline\")\n",
    "test_samples = _load_samples_from_npz(TEST_NPZ, max_samples=MAX_SAMPLES_TEST, sample_rate=44100, duration=2.0)\n",
    "kb_test_df = ev_tmp._evaluate_keyword_baseline(test_samples)\n",
    "test_frames[\"Keyword_Baseline\"] = kb_test_df\n",
    "kb_test_s = _summarize_df(kb_test_df)\n",
    "test_rows.append({\n",
    "    \"model\": \"Keyword_Baseline\",\n",
    "    \"sts_mean\": round(kb_test_s[\"sts_mean\"],4),\n",
    "    \"centroid_err_mean\": round(kb_test_s[\"centroid_err_mean\"],4),\n",
    "    \"mfcc_mean\": round(kb_test_s[\"mfcc_mean\"],4),\n",
    "    \"n\": int(kb_test_s[\"n\"])\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame(test_rows).set_index(\"model\")\n",
    "test_df.to_csv(\"results/model_means_test.csv\")\n",
    "print(\"Saved test means (incl. keyword baseline): results/model_means_test.csv\")\n",
    "\n",
    "# 9) Pretty displays\n",
    "print(\"\\n=== Head-to-Head (Shared Set) Means (incl. Keyword_Baseline) ===\")\n",
    "display(means_df_all)\n",
    "\n",
    "print(\"\\n=== Head-to-Head Pairwise Œî & p-values (incl. Keyword_Baseline) ===\")\n",
    "display(pairwise_df_all)\n",
    "\n",
    "print(\"\\n=== Test Set Means (incl. Keyword_Baseline) ===\")\n",
    "display(test_df)\n",
    "\n",
    "# === 10) Human-readable evaluation report (presentation style) ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIMODAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nShared Validation Set (71 samples)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing all models vs Keyword_Baseline\\n\")\n",
    "\n",
    "base = means_df_all.loc[\"Keyword_Baseline\"]\n",
    "for model in [\"LSTMABAR_Baseline\", \"FineTune_A\", \"FineTune_B\", \"FineTune_C\", \"FineTune_D\"]:\n",
    "    m = means_df_all.loc[model]\n",
    "    print(f\"\\n[{model}]\")\n",
    "    print(\"-\"*80)\n",
    "    # % changes vs baseline (Keyword_Baseline)\n",
    "    sts_delta = ((m.sts_mean - base.sts_mean) / base.sts_mean) * 100\n",
    "    sc_delta  = ((base.centroid_err_mean - m.centroid_err_mean) / base.centroid_err_mean) * 100\n",
    "    mfcc_delta= ((m.mfcc_mean - base.mfcc_mean) / base.mfcc_mean) * 100\n",
    "    print(f\"STS Mean: {m.sts_mean:.3f}  ({sts_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"Spectral Centroid Error: {m.centroid_err_mean:.0f} Hz  ({sc_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"MFCC Similarity: {m.mfcc_mean:.3f}  ({mfcc_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"n = {int(m.n)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PAIRWISE COMPARISONS (Shared Set)\")\n",
    "print(\"=\"*80)\n",
    "print(pairwise_df_all.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET PERFORMANCE (73 samples)\")\n",
    "print(\"=\"*80)\n",
    "print(\"All results compared against Keyword_Baseline\\n\")\n",
    "\n",
    "tb = test_df.loc[\"Keyword_Baseline\"]\n",
    "for model in [\"LSTMABAR_Baseline\", \"FineTune_A\", \"FineTune_B\", \"FineTune_C\", \"FineTune_D\"]:\n",
    "    tm = test_df.loc[model]\n",
    "    sts_d = ((tm.sts_mean - tb.sts_mean) / tb.sts_mean) * 100\n",
    "    sc_d  = ((tb.centroid_err_mean - tm.centroid_err_mean) / tb.centroid_err_mean) * 100\n",
    "    mfcc_d= ((tm.mfcc_mean - tb.mfcc_mean) / tb.mfcc_mean) * 100\n",
    "    print(f\"\\n[{model}]\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"STS Mean: {tm.sts_mean:.3f} ({sts_d:+.2f}% vs baseline)\")\n",
    "    print(f\"Spectral Centroid Error: {tm.centroid_err_mean:.0f} Hz ({sc_d:+.2f}% vs baseline)\")\n",
    "    print(f\"MFCC Similarity: {tm.mfcc_mean:.3f} ({mfcc_d:+.2f}% vs baseline)\")\n",
    "    print(f\"n = {int(tm.n)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "üìù TEXT UNDERSTANDING (STS)\n",
    " ‚Ä¢ ‚â• 0.75 ‚Üí Excellent\n",
    " ‚Ä¢ 0.60‚Äì0.75 ‚Üí Good\n",
    " ‚Ä¢ < 0.60 ‚Üí Needs improvement\n",
    "\n",
    "üéµ BRIGHTNESS ACCURACY (Spectral Centroid Error)\n",
    " ‚Ä¢ < 200 Hz ‚Üí Excellent\n",
    " ‚Ä¢ < 500 Hz ‚Üí Good\n",
    " ‚Ä¢ > 500 Hz ‚Üí Needs improvement\n",
    "\n",
    "üé∏ TIMBRE QUALITY (MFCC Similarity)\n",
    " ‚Ä¢ > 0.80 ‚Üí Excellent\n",
    " ‚Ä¢ > 0.65 ‚Üí Good\n",
    " ‚Ä¢ ‚â§ 0.65 ‚Üí Needs improvement\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Detailed CSVs in /results :\")\n",
    "print(\"   ‚Ä¢ model_means_shared.csv\")\n",
    "print(\"   ‚Ä¢ pairwise_stats_shared.csv\")\n",
    "print(\"   ‚Ä¢ model_means_test.csv\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab3py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
