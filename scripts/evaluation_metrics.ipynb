{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multimodal Evaluation Metrics for LSTMABAR\n",
    "Implements STS, Spectral Centroid Error, and MFCC Similarity\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from lstmabar_model import LSTMABAR\n",
    "from improved_text_encoders import ImprovedTextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMABAREvaluator:\n",
    "    \"\"\"\n",
    "    Complete evaluation framework for LSTMABAR model\n",
    "    Implements text-side and audio-side metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LSTMABAR,\n",
    "        text_model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        sample_rate: int = 44100\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Text encoder for STS\n",
    "        self.text_encoder = SentenceTransformer(text_model_name)\n",
    "        \n",
    "        # Archetype descriptors for mapping\n",
    "        self.archetype_descriptors = {\n",
    "            'sine': 'smooth pure warm mellow soft gentle flowing',\n",
    "            'square': 'digital harsh buzzy retro electronic robotic',\n",
    "            'sawtooth': 'bright cutting metallic sharp aggressive',\n",
    "            'triangle': 'hollow woody muted filtered organic',\n",
    "            'noise': 'rough textured grainy distorted chaotic'\n",
    "        }\n",
    "        \n",
    "        # Expected spectral centroids for common descriptors (Hz)\n",
    "        self.centroid_map = {\n",
    "            'bright': 3500, 'dark': 800, 'warm': 1000, 'harsh': 4000,\n",
    "            'mellow': 1200, 'sharp': 5000, 'smooth': 1500, 'soft': 1100,\n",
    "            'aggressive': 3800, 'gentle': 1300, 'cutting': 4200\n",
    "        }\n",
    "    \n",
    "    # ==================== TEXT-SIDE EVALUATION (STS) ====================\n",
    "    \n",
    "    def compute_sts(\n",
    "        self,\n",
    "        input_description: str,\n",
    "        predicted_archetype_weights: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Semantic Textual Similarity: Does model understand description?\n",
    "        \n",
    "        Args:\n",
    "            input_description: Original user description\n",
    "            predicted_archetype_weights: Model's predicted archetype weights (5,)\n",
    "        \n",
    "        Returns:\n",
    "            STS score (0-1, higher is better)\n",
    "        \"\"\"\n",
    "        # Convert archetype weights to descriptive text\n",
    "        predicted_description = self._archetypes_to_text(predicted_archetype_weights)\n",
    "        \n",
    "        # Encode both descriptions\n",
    "        emb_input = self.text_encoder.encode(input_description, convert_to_tensor=True)\n",
    "        emb_predicted = self.text_encoder.encode(predicted_description, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        sts_score = util.cos_sim(emb_input, emb_predicted).item()\n",
    "        \n",
    "        return sts_score\n",
    "    \n",
    "    def _archetypes_to_text(\n",
    "        self,\n",
    "        archetype_weights: np.ndarray,\n",
    "        threshold: float = 0.1\n",
    "    ) -> str:\n",
    "        \"\"\"Convert archetype mixture to natural language\"\"\"\n",
    "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
    "        \n",
    "        # Get top contributing archetypes\n",
    "        descriptions = []\n",
    "        for i, (name, weight) in enumerate(zip(archetype_names, archetype_weights)):\n",
    "            if weight > threshold:\n",
    "                descriptions.append(self.archetype_descriptors[name])\n",
    "        \n",
    "        return ' '.join(descriptions) if descriptions else 'neutral sound'\n",
    "    \n",
    "    # ==================== AUDIO-SIDE EVALUATION ====================\n",
    "    \n",
    "    def spectral_centroid_error(\n",
    "        self,\n",
    "        output_audio: np.ndarray,\n",
    "        target_description: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measures brightness accuracy (Hz error)\n",
    "        \n",
    "        Args:\n",
    "            output_audio: Generated audio array\n",
    "            target_description: Target description\n",
    "        \n",
    "        Returns:\n",
    "            Absolute error in Hz (lower is better)\n",
    "        \"\"\"\n",
    "        # Compute actual spectral centroid\n",
    "        centroid = librosa.feature.spectral_centroid(\n",
    "            y=output_audio,\n",
    "            sr=self.sample_rate\n",
    "        )\n",
    "        actual_centroid = np.mean(centroid)\n",
    "        \n",
    "        # Determine expected centroid from description\n",
    "        expected_centroid = self._description_to_centroid(target_description)\n",
    "        \n",
    "        # Calculate error\n",
    "        error_hz = abs(actual_centroid - expected_centroid)\n",
    "        \n",
    "        return error_hz\n",
    "    \n",
    "    def _description_to_centroid(self, description: str) -> float:\n",
    "        \"\"\"Map description to expected spectral centroid\"\"\"\n",
    "        words = description.lower().split()\n",
    "        expected_centroids = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.centroid_map:\n",
    "                expected_centroids.append(self.centroid_map[word])\n",
    "        \n",
    "        if expected_centroids:\n",
    "            return np.mean(expected_centroids)\n",
    "        \n",
    "        return 2000  # Neutral default\n",
    "    \n",
    "    def mfcc_similarity(\n",
    "        self,\n",
    "        output_audio: np.ndarray,\n",
    "        reference_audio: np.ndarray,\n",
    "        n_mfcc: int = 13\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measures timbral similarity (0-1, higher is better)\n",
    "        \n",
    "        Args:\n",
    "            output_audio: Generated audio array\n",
    "            reference_audio: Target/reference audio array\n",
    "            n_mfcc: Number of MFCC coefficients\n",
    "        \n",
    "        Returns:\n",
    "            Cosine similarity (1 is perfect match, >0.8 is good)\n",
    "        \"\"\"\n",
    "        # Extract MFCCs from both signals\n",
    "        mfcc_output = librosa.feature.mfcc(\n",
    "            y=output_audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mfcc=n_mfcc\n",
    "        )\n",
    "        mfcc_reference = librosa.feature.mfcc(\n",
    "            y=reference_audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mfcc=n_mfcc\n",
    "        )\n",
    "        \n",
    "        # Average across time\n",
    "        mfcc_output_mean = np.mean(mfcc_output, axis=1).reshape(1, -1)\n",
    "        mfcc_reference_mean = np.mean(mfcc_reference, axis=1).reshape(1, -1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(mfcc_output_mean, mfcc_reference_mean)[0][0]\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    # ==================== COMPREHENSIVE EVALUATION ====================\n",
    "    \n",
    "    def evaluate_single_transformation(\n",
    "        self,\n",
    "        input_audio: np.ndarray,\n",
    "        input_description: str,\n",
    "        output_audio: np.ndarray,\n",
    "        reference_audio: np.ndarray,\n",
    "        predicted_weights: np.ndarray\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Complete evaluation of a single transformation\n",
    "        \n",
    "        Returns:\n",
    "            Dict with all metrics\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            # Text-side (NLP understanding)\n",
    "            'sts_score': self.compute_sts(input_description, predicted_weights),\n",
    "            \n",
    "            # Audio-side (transformation quality)\n",
    "            'spectral_centroid_error_hz': self.spectral_centroid_error(\n",
    "                output_audio, input_description\n",
    "            ),\n",
    "            'mfcc_similarity': self.mfcc_similarity(output_audio, reference_audio)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        generate_reference: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set\n",
    "        \n",
    "        Args:\n",
    "            test_samples: List of dicts with 'audio', 'description', 'target_weights'\n",
    "            generate_reference: If True, generate reference audio from target weights\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with results for each sample\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        \n",
    "        print(f\"Evaluating model on {len(test_samples)} samples...\")\n",
    "        \n",
    "        for i, sample in enumerate(test_samples):\n",
    "            input_audio = sample['audio']\n",
    "            description = sample['description']\n",
    "            target_weights = sample['target_weights']\n",
    "            \n",
    "            # Convert to tensor\n",
    "            audio_tensor = torch.from_numpy(input_audio).unsqueeze(0).float().to(self.model.device)\n",
    "            \n",
    "            # Model inference\n",
    "            with torch.no_grad():\n",
    "                transformed, metadata = self.model.inference([description], audio_tensor)\n",
    "            \n",
    "            # Convert back to numpy\n",
    "            output_audio = transformed[0].cpu().numpy()\n",
    "            predicted_weights = metadata['predicted_weights'][0]\n",
    "            \n",
    "            # Generate reference audio if needed\n",
    "            if generate_reference:\n",
    "                # Use archetype generator to create ideal sound\n",
    "                reference_audio = self._generate_reference_audio(target_weights)\n",
    "            else:\n",
    "                reference_audio = input_audio  # Fallback\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_single_transformation(\n",
    "                input_audio,\n",
    "                description,\n",
    "                output_audio,\n",
    "                reference_audio,\n",
    "                predicted_weights\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            metrics['description'] = description\n",
    "            metrics['sample_idx'] = i\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i+1}/{len(test_samples)} samples\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _generate_reference_audio(\n",
    "        self,\n",
    "        archetype_weights: np.ndarray,\n",
    "        duration: float = 2.0\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Generate reference audio from archetype mixture\"\"\"\n",
    "        n_samples = int(self.sample_rate * duration)\n",
    "        t = np.linspace(0, duration, n_samples, endpoint=False)\n",
    "        frequency = 440  # A4\n",
    "        \n",
    "        audio = np.zeros(n_samples)\n",
    "        \n",
    "        # Generate each archetype component\n",
    "        archetypes = {\n",
    "            'sine': np.sin(2 * np.pi * frequency * t),\n",
    "            'square': np.sign(np.sin(2 * np.pi * frequency * t)),\n",
    "            'sawtooth': 2 * (t * frequency - np.floor(0.5 + t * frequency)),\n",
    "            'triangle': 2 * np.abs(2 * (t * frequency - np.floor(t * frequency + 0.5))) - 1,\n",
    "            'noise': np.random.randn(n_samples) * 0.3\n",
    "        }\n",
    "        \n",
    "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
    "        \n",
    "        # Mix according to weights\n",
    "        for i, name in enumerate(archetype_names):\n",
    "            audio += archetype_weights[i] * archetypes[name]\n",
    "        \n",
    "        # Normalize\n",
    "        audio = audio / (np.max(np.abs(audio)) + 1e-8) * 0.95\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    def compare_with_baseline(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        baseline_model: Optional[object] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare LSTMABAR with baseline model\n",
    "        \n",
    "        Args:\n",
    "            test_samples: Test samples\n",
    "            baseline_model: Baseline model (if None, uses simple keyword matching)\n",
    "        \n",
    "        Returns:\n",
    "            Comparison DataFrame\n",
    "        \"\"\"\n",
    "        # Evaluate LSTMABAR\n",
    "        lstmabar_results = self.evaluate_model(test_samples)\n",
    "        \n",
    "        # Evaluate baseline\n",
    "        if baseline_model is None:\n",
    "            baseline_results = self._evaluate_keyword_baseline(test_samples)\n",
    "        else:\n",
    "            baseline_results = baseline_model.evaluate(test_samples)\n",
    "        \n",
    "        # Compute statistics\n",
    "        comparison = self._compute_comparison_stats(lstmabar_results, baseline_results)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _evaluate_keyword_baseline(self, test_samples: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate simple keyword-matching baseline\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Simple keyword â†’ archetype mapping\n",
    "        keyword_map = {\n",
    "            'bright': [0.1, 0.1, 0.6, 0.1, 0.1],  # Mostly sawtooth\n",
    "            'warm': [0.6, 0.1, 0.1, 0.2, 0.0],    # Mostly sine\n",
    "            'harsh': [0.1, 0.5, 0.2, 0.1, 0.1],   # Mostly square\n",
    "            'smooth': [0.6, 0.1, 0.1, 0.2, 0.0],  # Mostly sine\n",
    "            'distorted': [0.1, 0.2, 0.2, 0.1, 0.4] # High noise\n",
    "        }\n",
    "        \n",
    "        for i, sample in enumerate(test_samples):\n",
    "            description = sample['description'].lower()\n",
    "            \n",
    "            # Find matching keywords\n",
    "            predicted_weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform default\n",
    "            for keyword, weights in keyword_map.items():\n",
    "                if keyword in description:\n",
    "                    predicted_weights = np.array(weights)\n",
    "                    break\n",
    "            \n",
    "            # Simple transformation (just apply gain based on brightness)\n",
    "            output_audio = sample['audio'].copy()\n",
    "            if 'bright' in description:\n",
    "                # Boost high frequencies (simplified)\n",
    "                output_audio = output_audio * 1.2\n",
    "            \n",
    "            reference_audio = self._generate_reference_audio(sample['target_weights'])\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_single_transformation(\n",
    "                sample['audio'],\n",
    "                description,\n",
    "                output_audio,\n",
    "                reference_audio,\n",
    "                predicted_weights\n",
    "            )\n",
    "            \n",
    "            metrics['description'] = description\n",
    "            metrics['sample_idx'] = i\n",
    "            results.append(metrics)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _compute_comparison_stats(\n",
    "        self,\n",
    "        lstmabar_results: pd.DataFrame,\n",
    "        baseline_results: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute statistical comparison\"\"\"\n",
    "        metrics = ['sts_score', 'spectral_centroid_error_hz', 'mfcc_similarity']\n",
    "        \n",
    "        comparison = {\n",
    "            'Metric': [],\n",
    "            'Baseline_Mean': [],\n",
    "            'Baseline_Std': [],\n",
    "            'LSTMABAR_Mean': [],\n",
    "            'LSTMABAR_Std': [],\n",
    "            'Improvement': [],\n",
    "            'p_value': []\n",
    "        }\n",
    "        \n",
    "        for metric in metrics:\n",
    "            baseline_vals = baseline_results[metric].values\n",
    "            lstmabar_vals = lstmabar_results[metric].values\n",
    "            \n",
    "            baseline_mean = np.mean(baseline_vals)\n",
    "            baseline_std = np.std(baseline_vals)\n",
    "            lstmabar_mean = np.mean(lstmabar_vals)\n",
    "            lstmabar_std = np.std(lstmabar_vals)\n",
    "            \n",
    "            comparison['Metric'].append(metric)\n",
    "            comparison['Baseline_Mean'].append(f\"{baseline_mean:.4f}\")\n",
    "            comparison['Baseline_Std'].append(f\"{baseline_std:.4f}\")\n",
    "            comparison['LSTMABAR_Mean'].append(f\"{lstmabar_mean:.4f}\")\n",
    "            comparison['LSTMABAR_Std'].append(f\"{lstmabar_std:.4f}\")\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if 'error' in metric.lower():\n",
    "                # Lower is better\n",
    "                improvement = ((baseline_mean - lstmabar_mean) / baseline_mean) * 100\n",
    "            else:\n",
    "                # Higher is better\n",
    "                improvement = ((lstmabar_mean - baseline_mean) / baseline_mean) * 100\n",
    "            \n",
    "            comparison['Improvement'].append(f\"{improvement:.2f}%\")\n",
    "            \n",
    "            # Paired t-test\n",
    "            _, p_val = ttest_rel(baseline_vals, lstmabar_vals)\n",
    "            comparison['p_value'].append(f\"{p_val:.4f}\")\n",
    "        \n",
    "        return pd.DataFrame(comparison)\n",
    "    \n",
    "    def generate_evaluation_report(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        save_path: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LSTMABAR MODEL EVALUATION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = self.evaluate_model(test_samples)\n",
    "        \n",
    "        # Compare with baseline\n",
    "        comparison = self.compare_with_baseline(test_samples)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPARISON WITH BASELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(comparison.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INTERPRETATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # STS interpretation\n",
    "        sts_mean = results['sts_score'].mean()\n",
    "        print(f\"\\nðŸ“ TEXT UNDERSTANDING (STS Score):\")\n",
    "        print(f\"   Mean: {sts_mean:.3f}\")\n",
    "        if sts_mean > 0.75:\n",
    "            print(f\"   âœ“ Excellent semantic understanding!\")\n",
    "        elif sts_mean > 0.60:\n",
    "            print(f\"   â†’ Good semantic understanding\")\n",
    "        else:\n",
    "            print(f\"   âš  Needs improvement in NLP comprehension\")\n",
    "        \n",
    "        # Spectral centroid interpretation\n",
    "        sc_mean = results['spectral_centroid_error_hz'].mean()\n",
    "        print(f\"\\nðŸŽµ BRIGHTNESS ACCURACY (Spectral Centroid Error):\")\n",
    "        print(f\"   Mean: {sc_mean:.0f} Hz\")\n",
    "        if sc_mean < 200:\n",
    "            print(f\"   âœ“ Excellent brightness targeting!\")\n",
    "        elif sc_mean < 500:\n",
    "            print(f\"   â†’ Good brightness control\")\n",
    "        else:\n",
    "            print(f\"   âš  Brightness targeting needs improvement\")\n",
    "        \n",
    "        # MFCC interpretation\n",
    "        mfcc_mean = results['mfcc_similarity'].mean()\n",
    "        print(f\"\\nðŸŽ¸ TIMBRE QUALITY (MFCC Similarity):\")\n",
    "        print(f\"   Mean: {mfcc_mean:.3f}\")\n",
    "        if mfcc_mean > 0.80:\n",
    "            print(f\"   âœ“ Excellent timbral matching!\")\n",
    "        elif mfcc_mean > 0.65:\n",
    "            print(f\"   â†’ Good timbral similarity\")\n",
    "        else:\n",
    "            print(f\"   âš  Timbre quality needs improvement\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # Save detailed results\n",
    "        if save_path:\n",
    "            results.to_csv(save_path, index=False)\n",
    "            comparison.to_csv(save_path.replace('.csv', '_comparison.csv'), index=False)\n",
    "            print(f\"\\nDetailed results saved to {save_path}\")\n",
    "\n",
    "    def evaluate_model_on_test_set(\n",
    "        self,\n",
    "        test_data_path: str,\n",
    "        max_samples: Optional[int] = None,\n",
    "        save_results: bool = True,\n",
    "        results_path: str = 'test_results.csv'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set from MusicCaps\n",
    "        \n",
    "        Args:\n",
    "            test_data_path: Path to test .npz file\n",
    "            max_samples: Max samples to evaluate (None = all)\n",
    "            save_results: Whether to save results\n",
    "            results_path: Path to save results\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EVALUATING ON TEST SET\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"Loading test data from {test_data_path}...\")\n",
    "        data = np.load(test_data_path, allow_pickle=True)\n",
    "        \n",
    "        vectors = data['archetype_vectors']\n",
    "        descriptions = data['descriptions'].tolist()\n",
    "        audio_paths = data['audio_paths'].tolist()\n",
    "        \n",
    "        n_samples = min(len(descriptions), max_samples) if max_samples else len(descriptions)\n",
    "        print(f\"Evaluating on {n_samples} test samples\\n\")\n",
    "        \n",
    "        # Prepare test samples\n",
    "        test_samples = []\n",
    "        for i in range(n_samples):\n",
    "            audio_path = audio_paths[i]\n",
    "            \n",
    "            # Check if audio exists\n",
    "            if not Path(audio_path).exists():\n",
    "                print(f\"Skipping {i}: audio file not found\")\n",
    "                continue\n",
    "            \n",
    "            # Load audio\n",
    "            try:\n",
    "                audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=2.0)\n",
    "                \n",
    "                # Pad or trim\n",
    "                target_length = int(self.sample_rate * 2.0)\n",
    "                if len(audio) < target_length:\n",
    "                    audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target_length]\n",
    "                \n",
    "                test_samples.append({\n",
    "                    'audio': audio,\n",
    "                    'description': descriptions[i],\n",
    "                    'target_weights': vectors[i]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {audio_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully loaded {len(test_samples)} test samples\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results_df = self.evaluate_model(test_samples, generate_reference=True)\n",
    "        \n",
    "        # Save results\n",
    "        if save_results:\n",
    "            results_df.to_csv(results_path, index=False)\n",
    "            print(f\"\\nâœ“ Results saved to {results_path}\")\n",
    "        \n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper: load with prefix-compat (for FineTune_B)\n",
    "\n",
    "# --- compatibility remap for old text-encoder prefixes (FineTune_B) ---\n",
    "def _compat_remap_text_encoder_keys(sd: dict) -> dict:\n",
    "    if not any(k.startswith(\"text_encoder.backbone.\") for k in sd.keys()):\n",
    "        return sd\n",
    "    m = [\n",
    "        (\"text_encoder.backbone.embeddings.\", \"text_encoder.sentence_model.0.auto_model.embeddings.\"),\n",
    "        (\"text_encoder.backbone.encoder.\",    \"text_encoder.sentence_model.0.auto_model.encoder.\"),\n",
    "        (\"text_encoder.backbone.pooler.\",     \"text_encoder.sentence_model.0.auto_model.pooler.\"),\n",
    "    ]\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        newk = k\n",
    "        for old, new in m:\n",
    "            if k.startswith(old):\n",
    "                newk = new + k[len(old):]\n",
    "                break\n",
    "        out[newk] = v\n",
    "    return out\n",
    "\n",
    "def load_lstmabar_checkpoint(path: str, device: str = None):\n",
    "    import torch\n",
    "    from lstmabar_model import LSTMABAR\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTMABAR(embedding_dim=768, audio_architecture='resnet', sample_rate=44100, device=device)\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    sd = ckpt.get(\"model_state_dict\", ckpt)\n",
    "    sd = _compat_remap_text_encoder_keys(sd)\n",
    "    incompat = model.load_state_dict(sd, strict=False)\n",
    "    print(f\"Loaded {path} (epoch={ckpt.get('epoch','?')}) | missing={len(incompat.missing_keys)} unexpected={len(incompat.unexpected_keys)}\")\n",
    "    return model\n",
    "\n",
    "def load_improved_c_checkpoint(path: str, device: str = None):\n",
    "    import torch\n",
    "    from lstmabar_model import LSTMABAR\n",
    "    \n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create model with same architecture\n",
    "    model = LSTMABAR(\n",
    "        embedding_dim=768, \n",
    "        audio_architecture='resnet', \n",
    "        sample_rate=44100, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Replace text encoder with improved version\n",
    "    model.text_encoder = ImprovedTextEncoder(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "        embedding_dim=768,\n",
    "        projection_depth='deep',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    sd = ckpt.get(\"model_state_dict\", ckpt)\n",
    "    \n",
    "    # Load with strict=False in case of minor key mismatches\n",
    "    incompat = model.load_state_dict(sd, strict=False)\n",
    "    print(f\"Loaded {path} (epoch={ckpt.get('epoch','?')})\")\n",
    "    print(f\"  Missing keys: {len(incompat.missing_keys)}\")\n",
    "    print(f\"  Unexpected keys: {len(incompat.unexpected_keys)}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a small shared evaluation set (prefer real val clips, fallback to 3 synthetic)\n",
    "\n",
    "def build_shared_eval_samples(val_npz_path: str, k: int = 73, sample_rate: int = 44100, duration: float = 2.0):\n",
    "    D = np.load(val_npz_path, allow_pickle=True)\n",
    "    descs = D['descriptions'].tolist()\n",
    "    paths = D['audio_paths'].tolist()\n",
    "    vecs  = D['archetype_vectors']\n",
    "\n",
    "    tgt_len = int(sample_rate * duration)\n",
    "    samples = []\n",
    "    for i, p in enumerate(paths):\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                y, sr = librosa.load(p, sr=sample_rate, duration=duration)\n",
    "                if len(y) < tgt_len:\n",
    "                    y = np.pad(y, (0, tgt_len - len(y)))\n",
    "                else:\n",
    "                    y = y[:tgt_len]\n",
    "                samples.append({'audio': y, 'description': descs[i], 'target_weights': vecs[i]})\n",
    "                if len(samples) >= k:\n",
    "                    break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Fallback (if nothing loads)\n",
    "    if not samples:\n",
    "        rng = np.random.default_rng(42)\n",
    "        for desc, vec in [\n",
    "            (\"bright and cutting guitar with metallic tone\", np.array([0.1,0.1,0.6,0.1,0.1])),\n",
    "            (\"warm smooth piano melody with gentle sustain\", np.array([0.6,0.05,0.1,0.2,0.05])),\n",
    "            (\"harsh digital synth with buzzy retro sound\",   np.array([0.1,0.55,0.15,0.1,0.1])),\n",
    "        ]:\n",
    "            y = rng.standard_normal(tgt_len).astype(np.float32)\n",
    "            samples.append({'audio': y, 'description': desc, 'target_weights': vec})\n",
    "    print(f\"Shared eval set size: {len(samples)}\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model-vs-Model comparison on the shared set\n",
    "\n",
    "import itertools\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def evaluate_side_by_side(models: list, samples: list, sample_rate: int = 44100):\n",
    "    \"\"\"\n",
    "    models: list of (label, model)\n",
    "    samples: list of dicts with 'audio', 'description', 'target_weights'\n",
    "    \"\"\"\n",
    "    results_per_model = {}\n",
    "    for label, mdl in models:\n",
    "        print(f\"\\n[Eval] {label}\")\n",
    "        ev = LSTMABAREvaluator(mdl, sample_rate=sample_rate)\n",
    "        # Turn numpy audios into the structure expected by evaluate_model\n",
    "        df = ev.evaluate_model(samples, generate_reference=True)\n",
    "        results_per_model[label] = df\n",
    "\n",
    "    # Summaries\n",
    "    def summarize(df):\n",
    "        return {\n",
    "            \"sts_mean\": df[\"sts_score\"].mean(),\n",
    "            \"sts_std\":  df[\"sts_score\"].std(),\n",
    "            \"centroid_err_mean\": df[\"spectral_centroid_error_hz\"].mean(),\n",
    "            \"centroid_err_std\":  df[\"spectral_centroid_error_hz\"].std(),\n",
    "            \"mfcc_mean\": df[\"mfcc_similarity\"].mean(),\n",
    "            \"mfcc_std\":  df[\"mfcc_similarity\"].std(),\n",
    "            \"n\": len(df),\n",
    "        }\n",
    "\n",
    "    summary_rows = []\n",
    "    for label, df in results_per_model.items():\n",
    "        s = summarize(df)\n",
    "        s[\"model\"] = label\n",
    "        summary_rows.append(s)\n",
    "    means_df = pd.DataFrame(summary_rows).set_index(\"model\").round(4)\n",
    "\n",
    "    # Pairwise deltas + paired t-tests\n",
    "    pairs = []\n",
    "    labels = list(results_per_model.keys())\n",
    "    metrics = [\n",
    "        (\"sts_score\", \"higher_better\"),\n",
    "        (\"spectral_centroid_error_hz\", \"lower_better\"),\n",
    "        (\"mfcc_similarity\", \"higher_better\"),\n",
    "    ]\n",
    "    for a, b in itertools.combinations(labels, 2):\n",
    "        A = results_per_model[a]\n",
    "        B = results_per_model[b]\n",
    "        # align by sample_idx (just in case order changed)\n",
    "        A2 = A.sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        B2 = B.sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        for metric, direction in metrics:\n",
    "            a_vals = A2[metric].values\n",
    "            b_vals = B2[metric].values\n",
    "            # improvement relative to b (positive is better)\n",
    "            if direction == \"higher_better\":\n",
    "                delta = a_vals - b_vals\n",
    "            else:\n",
    "                delta = b_vals - a_vals  # lower error => better\n",
    "            tstat, p = ttest_rel(a_vals, b_vals)\n",
    "            pairs.append({\n",
    "                \"A\": a, \"B\": b, \"Metric\": metric,\n",
    "                \"Î”_mean(A_vs_B)\": np.mean(delta),\n",
    "                \"p_value\": p\n",
    "            })\n",
    "\n",
    "    pairwise_df = pd.DataFrame(pairs)\n",
    "    # Sort for readability\n",
    "    pairwise_df = pairwise_df.sort_values([\"Metric\", \"A\", \"B\"]).reset_index(drop=True)\n",
    "    return means_df, pairwise_df, results_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test-set evaluation (your original path)\n",
    "\n",
    "def eval_on_test_set(model, test_npz, max_samples=None, sample_rate=44100):\n",
    "    ev = LSTMABAREvaluator(model, sample_rate=sample_rate)\n",
    "    df = ev.evaluate_model_on_test_set(\n",
    "        test_data_path=test_npz,\n",
    "        max_samples=max_samples,\n",
    "        save_results=False\n",
    "    )\n",
    "    return {\n",
    "        \"sts_mean\": df[\"sts_score\"].mean(),\n",
    "        \"centroid_err_mean\": df[\"spectral_centroid_error_hz\"].mean(),\n",
    "        \"mfcc_mean\": df[\"mfcc_similarity\"].mean(),\n",
    "        \"n\": len(df)\n",
    "    }, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/best_model.pth (epoch=1) | missing=0 unexpected=0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/fine_tune_A/best_model.pth (epoch=8) | missing=0 unexpected=0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loaded checkpoints/fine_tune_B/best_model.pth (epoch=15) | missing=0 unexpected=0\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loading improved text encoder: sentence-transformers/all-mpnet-base-v2\n",
      "Loaded checkpoints/improved_approach_c/best_model.pth (epoch=16)\n",
      "  Missing keys: 0\n",
      "  Unexpected keys: 0\n",
      "Shared eval set size: 10\n",
      "\n",
      "[Eval] LSTMABAR_Baseline\n",
      "Evaluating model on 10 samples...\n",
      "  Processed 10/10 samples\n",
      "\n",
      "[Eval] FineTune_A\n",
      "Evaluating model on 10 samples...\n",
      "  Processed 10/10 samples\n",
      "\n",
      "[Eval] FineTune_B\n",
      "Evaluating model on 10 samples...\n",
      "  Processed 10/10 samples\n",
      "\n",
      "[Eval] FineTune_C\n",
      "Evaluating model on 10 samples...\n",
      "  Processed 10/10 samples\n",
      "Saved head-to-head (incl. keyword baseline): results/model_means_shared.csv, results/pairwise_stats_shared.csv\n",
      "\n",
      "[Test] LSTMABAR_Baseline\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 145 test samples\n",
      "\n",
      "Successfully loaded 145 test samples\n",
      "\n",
      "Evaluating model on 145 samples...\n",
      "  Processed 10/145 samples\n",
      "  Processed 20/145 samples\n",
      "  Processed 30/145 samples\n",
      "  Processed 40/145 samples\n",
      "  Processed 50/145 samples\n",
      "  Processed 60/145 samples\n",
      "  Processed 70/145 samples\n",
      "  Processed 80/145 samples\n",
      "  Processed 90/145 samples\n",
      "  Processed 100/145 samples\n",
      "  Processed 110/145 samples\n",
      "  Processed 120/145 samples\n",
      "  Processed 130/145 samples\n",
      "  Processed 140/145 samples\n",
      "\n",
      "[Test] FineTune_A\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 145 test samples\n",
      "\n",
      "Successfully loaded 145 test samples\n",
      "\n",
      "Evaluating model on 145 samples...\n",
      "  Processed 10/145 samples\n",
      "  Processed 20/145 samples\n",
      "  Processed 30/145 samples\n",
      "  Processed 40/145 samples\n",
      "  Processed 50/145 samples\n",
      "  Processed 60/145 samples\n",
      "  Processed 70/145 samples\n",
      "  Processed 80/145 samples\n",
      "  Processed 90/145 samples\n",
      "  Processed 100/145 samples\n",
      "  Processed 110/145 samples\n",
      "  Processed 120/145 samples\n",
      "  Processed 130/145 samples\n",
      "  Processed 140/145 samples\n",
      "\n",
      "[Test] FineTune_B\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 145 test samples\n",
      "\n",
      "Successfully loaded 145 test samples\n",
      "\n",
      "Evaluating model on 145 samples...\n",
      "  Processed 10/145 samples\n",
      "  Processed 20/145 samples\n",
      "  Processed 30/145 samples\n",
      "  Processed 40/145 samples\n",
      "  Processed 50/145 samples\n",
      "  Processed 60/145 samples\n",
      "  Processed 70/145 samples\n",
      "  Processed 80/145 samples\n",
      "  Processed 90/145 samples\n",
      "  Processed 100/145 samples\n",
      "  Processed 110/145 samples\n",
      "  Processed 120/145 samples\n",
      "  Processed 130/145 samples\n",
      "  Processed 140/145 samples\n",
      "\n",
      "[Test] FineTune_C\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 145 test samples\n",
      "\n",
      "Successfully loaded 145 test samples\n",
      "\n",
      "Evaluating model on 145 samples...\n",
      "  Processed 10/145 samples\n",
      "  Processed 20/145 samples\n",
      "  Processed 30/145 samples\n",
      "  Processed 40/145 samples\n",
      "  Processed 50/145 samples\n",
      "  Processed 60/145 samples\n",
      "  Processed 70/145 samples\n",
      "  Processed 80/145 samples\n",
      "  Processed 90/145 samples\n",
      "  Processed 100/145 samples\n",
      "  Processed 110/145 samples\n",
      "  Processed 120/145 samples\n",
      "  Processed 130/145 samples\n",
      "  Processed 140/145 samples\n",
      "\n",
      "[Test] Keyword_Baseline\n",
      "Saved test means (incl. keyword baseline): results/model_means_test.csv\n",
      "\n",
      "=== Head-to-Head (Shared Set) Means (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sts_mean</th>\n",
       "      <th>sts_std</th>\n",
       "      <th>centroid_err_mean</th>\n",
       "      <th>centroid_err_std</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>mfcc_std</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTMABAR_Baseline</th>\n",
       "      <td>0.2043</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>4964.0342</td>\n",
       "      <td>1514.1472</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.3744</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_A</th>\n",
       "      <td>0.2022</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>5254.7170</td>\n",
       "      <td>1758.3301</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>0.3785</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_B</th>\n",
       "      <td>0.2135</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>6217.0833</td>\n",
       "      <td>1653.5339</td>\n",
       "      <td>0.4204</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_C</th>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>4722.9145</td>\n",
       "      <td>1530.5992</td>\n",
       "      <td>0.5434</td>\n",
       "      <td>0.4221</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword_Baseline</th>\n",
       "      <td>0.1997</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>822.0181</td>\n",
       "      <td>901.7607</td>\n",
       "      <td>0.6441</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sts_mean  sts_std  centroid_err_mean  centroid_err_std  \\\n",
       "model                                                                       \n",
       "LSTMABAR_Baseline    0.2043   0.0333          4964.0342         1514.1472   \n",
       "FineTune_A           0.2022   0.0347          5254.7170         1758.3301   \n",
       "FineTune_B           0.2135   0.0483          6217.0833         1653.5339   \n",
       "FineTune_C           0.1975   0.0404          4722.9145         1530.5992   \n",
       "Keyword_Baseline     0.1997   0.0375           822.0181          901.7607   \n",
       "\n",
       "                   mfcc_mean  mfcc_std   n  \n",
       "model                                       \n",
       "LSTMABAR_Baseline     0.5598    0.3744  10  \n",
       "FineTune_A            0.5272    0.3785  10  \n",
       "FineTune_B            0.4204    0.4625  10  \n",
       "FineTune_C            0.5434    0.4221  10  \n",
       "Keyword_Baseline      0.6441    0.2424  10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head-to-Head Pairwise Î” & p-values (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Î”_mean(A_vs_B)</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.106746</td>\n",
       "      <td>0.022973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.016172</td>\n",
       "      <td>0.729517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.116924</td>\n",
       "      <td>0.134191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.122918</td>\n",
       "      <td>0.075227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.223670</td>\n",
       "      <td>0.062536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.100752</td>\n",
       "      <td>0.195883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.032611</td>\n",
       "      <td>0.259825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.139357</td>\n",
       "      <td>0.038848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>0.016440</td>\n",
       "      <td>0.532038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>mfcc_similarity</td>\n",
       "      <td>-0.084313</td>\n",
       "      <td>0.184738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>962.366365</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-531.802479</td>\n",
       "      <td>0.135025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-4432.698855</td>\n",
       "      <td>0.000157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-1494.168844</td>\n",
       "      <td>0.002364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-5395.065220</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-3900.896376</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>290.682774</td>\n",
       "      <td>0.206434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>1253.049139</td>\n",
       "      <td>0.002816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-241.119704</td>\n",
       "      <td>0.392804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>spectral_centroid_error_hz</td>\n",
       "      <td>-4142.016081</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.011276</td>\n",
       "      <td>0.300465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.004781</td>\n",
       "      <td>0.534455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.823401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.016057</td>\n",
       "      <td>0.361957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>0.357276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.002249</td>\n",
       "      <td>0.866551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_A</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.343436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_B</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>-0.009185</td>\n",
       "      <td>0.393936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>FineTune_C</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.343436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LSTMABAR_Baseline</td>\n",
       "      <td>Keyword_Baseline</td>\n",
       "      <td>sts_score</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.676418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    A                 B                      Metric  \\\n",
       "0          FineTune_A        FineTune_B             mfcc_similarity   \n",
       "1          FineTune_A        FineTune_C             mfcc_similarity   \n",
       "2          FineTune_A  Keyword_Baseline             mfcc_similarity   \n",
       "3          FineTune_B        FineTune_C             mfcc_similarity   \n",
       "4          FineTune_B  Keyword_Baseline             mfcc_similarity   \n",
       "5          FineTune_C  Keyword_Baseline             mfcc_similarity   \n",
       "6   LSTMABAR_Baseline        FineTune_A             mfcc_similarity   \n",
       "7   LSTMABAR_Baseline        FineTune_B             mfcc_similarity   \n",
       "8   LSTMABAR_Baseline        FineTune_C             mfcc_similarity   \n",
       "9   LSTMABAR_Baseline  Keyword_Baseline             mfcc_similarity   \n",
       "10         FineTune_A        FineTune_B  spectral_centroid_error_hz   \n",
       "11         FineTune_A        FineTune_C  spectral_centroid_error_hz   \n",
       "12         FineTune_A  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "13         FineTune_B        FineTune_C  spectral_centroid_error_hz   \n",
       "14         FineTune_B  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "15         FineTune_C  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "16  LSTMABAR_Baseline        FineTune_A  spectral_centroid_error_hz   \n",
       "17  LSTMABAR_Baseline        FineTune_B  spectral_centroid_error_hz   \n",
       "18  LSTMABAR_Baseline        FineTune_C  spectral_centroid_error_hz   \n",
       "19  LSTMABAR_Baseline  Keyword_Baseline  spectral_centroid_error_hz   \n",
       "20         FineTune_A        FineTune_B                   sts_score   \n",
       "21         FineTune_A        FineTune_C                   sts_score   \n",
       "22         FineTune_A  Keyword_Baseline                   sts_score   \n",
       "23         FineTune_B        FineTune_C                   sts_score   \n",
       "24         FineTune_B  Keyword_Baseline                   sts_score   \n",
       "25         FineTune_C  Keyword_Baseline                   sts_score   \n",
       "26  LSTMABAR_Baseline        FineTune_A                   sts_score   \n",
       "27  LSTMABAR_Baseline        FineTune_B                   sts_score   \n",
       "28  LSTMABAR_Baseline        FineTune_C                   sts_score   \n",
       "29  LSTMABAR_Baseline  Keyword_Baseline                   sts_score   \n",
       "\n",
       "    Î”_mean(A_vs_B)   p_value  \n",
       "0         0.106746  0.022973  \n",
       "1        -0.016172  0.729517  \n",
       "2        -0.116924  0.134191  \n",
       "3        -0.122918  0.075227  \n",
       "4        -0.223670  0.062536  \n",
       "5        -0.100752  0.195883  \n",
       "6         0.032611  0.259825  \n",
       "7         0.139357  0.038848  \n",
       "8         0.016440  0.532038  \n",
       "9        -0.084313  0.184738  \n",
       "10      962.366365  0.001242  \n",
       "11     -531.802479  0.135025  \n",
       "12    -4432.698855  0.000157  \n",
       "13    -1494.168844  0.002364  \n",
       "14    -5395.065220  0.000027  \n",
       "15    -3900.896376  0.000212  \n",
       "16      290.682774  0.206434  \n",
       "17     1253.049139  0.002816  \n",
       "18     -241.119704  0.392804  \n",
       "19    -4142.016081  0.000088  \n",
       "20       -0.011276  0.300465  \n",
       "21        0.004781  0.534455  \n",
       "22        0.002532  0.823401  \n",
       "23        0.016057  0.361957  \n",
       "24        0.013808  0.357276  \n",
       "25       -0.002249  0.866551  \n",
       "26        0.002091  0.343436  \n",
       "27       -0.009185  0.393936  \n",
       "28        0.006872  0.343436  \n",
       "29        0.004623  0.676418  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Set Means (incl. Keyword_Baseline) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sts_mean</th>\n",
       "      <th>centroid_err_mean</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTMABAR_Baseline</th>\n",
       "      <td>0.2178</td>\n",
       "      <td>4935.4675</td>\n",
       "      <td>0.4206</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_A</th>\n",
       "      <td>0.2129</td>\n",
       "      <td>4858.5802</td>\n",
       "      <td>0.4388</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_B</th>\n",
       "      <td>0.2184</td>\n",
       "      <td>6240.8304</td>\n",
       "      <td>0.3332</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FineTune_C</th>\n",
       "      <td>0.2163</td>\n",
       "      <td>4931.3675</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword_Baseline</th>\n",
       "      <td>0.2133</td>\n",
       "      <td>936.6808</td>\n",
       "      <td>0.5287</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sts_mean  centroid_err_mean  mfcc_mean    n\n",
       "model                                                         \n",
       "LSTMABAR_Baseline    0.2178          4935.4675     0.4206  145\n",
       "FineTune_A           0.2129          4858.5802     0.4388  145\n",
       "FineTune_B           0.2184          6240.8304     0.3332  145\n",
       "FineTune_C           0.2163          4931.3675     0.4337  145\n",
       "Keyword_Baseline     0.2133           936.6808     0.5287  145"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTIMODAL EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Shared Validation Set (71 samples)\n",
      "================================================================================\n",
      "Comparing all models vs Keyword_Baseline\n",
      "\n",
      "\n",
      "[LSTMABAR_Baseline]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.204  (+2.30% vs baseline)\n",
      "Spectral Centroid Error: 4964 Hz  (-503.88% vs baseline)\n",
      "MFCC Similarity: 0.560  (-13.09% vs baseline)\n",
      "n = 10 samples\n",
      "\n",
      "[FineTune_A]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.202  (+1.25% vs baseline)\n",
      "Spectral Centroid Error: 5255 Hz  (-539.25% vs baseline)\n",
      "MFCC Similarity: 0.527  (-18.15% vs baseline)\n",
      "n = 10 samples\n",
      "\n",
      "[FineTune_B]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.213  (+6.91% vs baseline)\n",
      "Spectral Centroid Error: 6217 Hz  (-656.32% vs baseline)\n",
      "MFCC Similarity: 0.420  (-34.73% vs baseline)\n",
      "n = 10 samples\n",
      "\n",
      "================================================================================\n",
      "PAIRWISE COMPARISONS (Shared Set)\n",
      "================================================================================\n",
      "                A                B                     Metric  Î”_mean(A_vs_B)  p_value\n",
      "       FineTune_A       FineTune_B            mfcc_similarity        0.106746 0.022973\n",
      "       FineTune_A       FineTune_C            mfcc_similarity       -0.016172 0.729517\n",
      "       FineTune_A Keyword_Baseline            mfcc_similarity       -0.116924 0.134191\n",
      "       FineTune_B       FineTune_C            mfcc_similarity       -0.122918 0.075227\n",
      "       FineTune_B Keyword_Baseline            mfcc_similarity       -0.223670 0.062536\n",
      "       FineTune_C Keyword_Baseline            mfcc_similarity       -0.100752 0.195883\n",
      "LSTMABAR_Baseline       FineTune_A            mfcc_similarity        0.032611 0.259825\n",
      "LSTMABAR_Baseline       FineTune_B            mfcc_similarity        0.139357 0.038848\n",
      "LSTMABAR_Baseline       FineTune_C            mfcc_similarity        0.016440 0.532038\n",
      "LSTMABAR_Baseline Keyword_Baseline            mfcc_similarity       -0.084313 0.184738\n",
      "       FineTune_A       FineTune_B spectral_centroid_error_hz      962.366365 0.001242\n",
      "       FineTune_A       FineTune_C spectral_centroid_error_hz     -531.802479 0.135025\n",
      "       FineTune_A Keyword_Baseline spectral_centroid_error_hz    -4432.698855 0.000157\n",
      "       FineTune_B       FineTune_C spectral_centroid_error_hz    -1494.168844 0.002364\n",
      "       FineTune_B Keyword_Baseline spectral_centroid_error_hz    -5395.065220 0.000027\n",
      "       FineTune_C Keyword_Baseline spectral_centroid_error_hz    -3900.896376 0.000212\n",
      "LSTMABAR_Baseline       FineTune_A spectral_centroid_error_hz      290.682774 0.206434\n",
      "LSTMABAR_Baseline       FineTune_B spectral_centroid_error_hz     1253.049139 0.002816\n",
      "LSTMABAR_Baseline       FineTune_C spectral_centroid_error_hz     -241.119704 0.392804\n",
      "LSTMABAR_Baseline Keyword_Baseline spectral_centroid_error_hz    -4142.016081 0.000088\n",
      "       FineTune_A       FineTune_B                  sts_score       -0.011276 0.300465\n",
      "       FineTune_A       FineTune_C                  sts_score        0.004781 0.534455\n",
      "       FineTune_A Keyword_Baseline                  sts_score        0.002532 0.823401\n",
      "       FineTune_B       FineTune_C                  sts_score        0.016057 0.361957\n",
      "       FineTune_B Keyword_Baseline                  sts_score        0.013808 0.357276\n",
      "       FineTune_C Keyword_Baseline                  sts_score       -0.002249 0.866551\n",
      "LSTMABAR_Baseline       FineTune_A                  sts_score        0.002091 0.343436\n",
      "LSTMABAR_Baseline       FineTune_B                  sts_score       -0.009185 0.393936\n",
      "LSTMABAR_Baseline       FineTune_C                  sts_score        0.006872 0.343436\n",
      "LSTMABAR_Baseline Keyword_Baseline                  sts_score        0.004623 0.676418\n",
      "\n",
      "================================================================================\n",
      "TEST SET PERFORMANCE (73 samples)\n",
      "================================================================================\n",
      "All results compared against Keyword_Baseline\n",
      "\n",
      "\n",
      "[LSTMABAR_Baseline]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.218 (+2.11% vs baseline)\n",
      "Spectral Centroid Error: 4935 Hz (-426.91% vs baseline)\n",
      "MFCC Similarity: 0.421 (-20.45% vs baseline)\n",
      "n = 145 samples\n",
      "\n",
      "[FineTune_A]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.213 (-0.19% vs baseline)\n",
      "Spectral Centroid Error: 4859 Hz (-418.70% vs baseline)\n",
      "MFCC Similarity: 0.439 (-17.00% vs baseline)\n",
      "n = 145 samples\n",
      "\n",
      "[FineTune_B]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.218 (+2.39% vs baseline)\n",
      "Spectral Centroid Error: 6241 Hz (-566.27% vs baseline)\n",
      "MFCC Similarity: 0.333 (-36.98% vs baseline)\n",
      "n = 145 samples\n",
      "\n",
      "[FineTune_C]\n",
      "--------------------------------------------------------------------------------\n",
      "STS Mean: 0.216 (+1.41% vs baseline)\n",
      "Spectral Centroid Error: 4931 Hz (-426.47% vs baseline)\n",
      "MFCC Similarity: 0.434 (-17.97% vs baseline)\n",
      "n = 145 samples\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ TEXT UNDERSTANDING (STS)\n",
      " â€¢ â‰¥ 0.75 â†’ Excellent\n",
      " â€¢ 0.60â€“0.75 â†’ Good\n",
      " â€¢ < 0.60 â†’ Needs improvement\n",
      "\n",
      "ðŸŽµ BRIGHTNESS ACCURACY (Spectral Centroid Error)\n",
      " â€¢ < 200 Hz â†’ Excellent\n",
      " â€¢ < 500 Hz â†’ Good\n",
      " â€¢ > 500 Hz â†’ Needs improvement\n",
      "\n",
      "ðŸŽ¸ TIMBRE QUALITY (MFCC Similarity)\n",
      " â€¢ > 0.80 â†’ Excellent\n",
      " â€¢ > 0.65 â†’ Good\n",
      " â€¢ â‰¤ 0.65 â†’ Needs improvement\n",
      "\n",
      "================================================================================\n",
      "âœ“ Detailed CSVs in /results :\n",
      "   â€¢ model_means_shared.csv\n",
      "   â€¢ pairwise_stats_shared.csv\n",
      "   â€¢ model_means_test.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "### Run the eval metrics:\n",
    "\n",
    "# --- Small helper to summarize a results DF (same metrics everywhere) ---\n",
    "def _summarize_df(df: pd.DataFrame) -> dict:\n",
    "    return {\n",
    "        \"sts_mean\": float(df[\"sts_score\"].mean()),\n",
    "        \"sts_std\":  float(df[\"sts_score\"].std()),\n",
    "        \"centroid_err_mean\": float(df[\"spectral_centroid_error_hz\"].mean()),\n",
    "        \"centroid_err_std\":  float(df[\"spectral_centroid_error_hz\"].std()),\n",
    "        \"mfcc_mean\": float(df[\"mfcc_similarity\"].mean()),\n",
    "        \"mfcc_std\":  float(df[\"mfcc_similarity\"].std()),\n",
    "        \"n\": int(len(df)),\n",
    "    }\n",
    "\n",
    "# --- helper to (re)build pairwise stats from a dict of label->df ---\n",
    "def _pairwise_from_frames(frames: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    labels = list(frames.keys())\n",
    "    metrics = [\n",
    "        (\"sts_score\", \"higher_better\"),\n",
    "        (\"spectral_centroid_error_hz\", \"lower_better\"),\n",
    "        (\"mfcc_similarity\", \"higher_better\"),\n",
    "    ]\n",
    "    for a, b in itertools.combinations(labels, 2):\n",
    "        A = frames[a].sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        B = frames[b].sort_values(\"sample_idx\").reset_index(drop=True)\n",
    "        for metric, direction in metrics:\n",
    "            a_vals = A[metric].values\n",
    "            b_vals = B[metric].values\n",
    "            # improvement relative to B (positive means A better)\n",
    "            if direction == \"higher_better\":\n",
    "                delta = a_vals - b_vals\n",
    "            else:\n",
    "                delta = b_vals - a_vals  # lower error => better\n",
    "            _, p = ttest_rel(a_vals, b_vals)\n",
    "            rows.append({\n",
    "                \"A\": a, \"B\": b, \"Metric\": metric,\n",
    "                \"Î”_mean(A_vs_B)\": float(np.mean(delta)),\n",
    "                \"p_value\": float(p)\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"Metric\",\"A\",\"B\"]).reset_index(drop=True)\n",
    "\n",
    "# --- helper to load raw samples from an NPZ into a list for baselines ---\n",
    "def _load_samples_from_npz(npz_path: str, max_samples=None, sample_rate=44100, duration=2.0):\n",
    "    D = np.load(npz_path, allow_pickle=True)\n",
    "    descs = D[\"descriptions\"].tolist()\n",
    "    paths = D[\"audio_paths\"].tolist()\n",
    "    vecs  = D[\"archetype_vectors\"]\n",
    "    tgt_len = int(sample_rate * duration)\n",
    "\n",
    "    samples = []\n",
    "    count = 0\n",
    "    for i, p in enumerate(paths):\n",
    "        if max_samples is not None and count >= max_samples:\n",
    "            break\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                y, sr = librosa.load(p, sr=sample_rate, duration=duration)\n",
    "                if len(y) < tgt_len:\n",
    "                    y = np.pad(y, (0, tgt_len - len(y)))\n",
    "                else:\n",
    "                    y = y[:tgt_len]\n",
    "                samples.append({\n",
    "                    \"audio\": y,\n",
    "                    \"description\": descs[i],\n",
    "                    \"target_weights\": vecs[i],\n",
    "                    \"sample_idx\": i\n",
    "                })\n",
    "                count += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    return samples\n",
    "\n",
    "# --- your existing paths ---\n",
    "BASELINE_CKPT   = \"checkpoints/best_model.pth\"\n",
    "FINETUNE_A_CKPT = \"checkpoints/fine_tune_A/best_model.pth\"\n",
    "FINETUNE_B_CKPT = \"checkpoints/fine_tune_B/best_model.pth\"\n",
    "FINETUNE_C_CKPT = \"checkpoints/improved_approach_c/best_model.pth\"\n",
    "\n",
    "VAL_NPZ  = \"musiccaps_training_data_val.npz\"\n",
    "TEST_NPZ = \"musiccaps_training_data_test.npz\"\n",
    "\n",
    "MAX_SAMPLES_SHARED = 10   # shared head-to-head set (val)\n",
    "MAX_SAMPLES_TEST   = None # full test or cap with an int\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1) Load models (with compat loader)\n",
    "print(\"Loading models...\")\n",
    "m_base = load_lstmabar_checkpoint(BASELINE_CKPT, device=device)\n",
    "m_A    = load_lstmabar_checkpoint(FINETUNE_A_CKPT, device=device)\n",
    "m_B    = load_lstmabar_checkpoint(FINETUNE_B_CKPT, device=device)\n",
    "m_C    = load_improved_c_checkpoint(FINETUNE_C_CKPT, device=device)\n",
    "\n",
    "# 2) Build shared eval set from VAL (or fallback synthetic inside the helper)\n",
    "shared_samples = build_shared_eval_samples(VAL_NPZ, k=MAX_SAMPLES_SHARED, sample_rate=44100)\n",
    "\n",
    "# 3) Model-vs-Model (LSTM-based models only, first pass)\n",
    "models = [(\"LSTMABAR_Baseline\", m_base), (\"FineTune_A\", m_A), (\"FineTune_B\", m_B), (\"FineTune_C\", m_C)]\n",
    "means_df, pairwise_df, per_model_frames = evaluate_side_by_side(models, shared_samples, sample_rate=44100)\n",
    "\n",
    "# 4) Add the Keyword Baseline (non-LSTM) on the SAME shared samples\n",
    "ev_tmp = LSTMABAREvaluator(m_base, sample_rate=44100)  # any LSTMABAR instance just to access evaluator\n",
    "kb_shared_df = ev_tmp._evaluate_keyword_baseline(shared_samples)\n",
    "# ensure sample_idx exists for alignment (added by evaluate_model; add here too)\n",
    "if \"sample_idx\" not in kb_shared_df.columns:\n",
    "    kb_shared_df[\"sample_idx\"] = range(len(kb_shared_df))\n",
    "\n",
    "per_model_frames[\"Keyword_Baseline\"] = kb_shared_df\n",
    "\n",
    "# 5) Recompute head-to-head means/pairwise including Keyword_Baseline\n",
    "summary_rows = []\n",
    "for label, df in per_model_frames.items():\n",
    "    s = _summarize_df(df)\n",
    "    s[\"model\"] = label\n",
    "    summary_rows.append(s)\n",
    "means_df_all = pd.DataFrame(summary_rows).set_index(\"model\").round(4)\n",
    "\n",
    "pairwise_df_all = _pairwise_from_frames(per_model_frames)\n",
    "pairwise_df_all[\"Î”_mean(A_vs_B)\"] = pairwise_df_all[\"Î”_mean(A_vs_B)\"].round(6)\n",
    "pairwise_df_all[\"p_value\"] = pairwise_df_all[\"p_value\"].map(lambda x: float(x))\n",
    "\n",
    "# 6) Save head-to-head (with keyword baseline)\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "means_df_all.to_csv(\"results/model_means_shared.csv\")          # overwrite with the extended table\n",
    "pairwise_df_all.to_csv(\"results/pairwise_stats_shared.csv\")    # overwrite with the extended table\n",
    "print(\"Saved head-to-head (incl. keyword baseline): results/model_means_shared.csv, results/pairwise_stats_shared.csv\")\n",
    "\n",
    "# 7) Evaluate each model on the TEST set (LSTM models)\n",
    "test_rows = []\n",
    "test_frames = {}\n",
    "for label, mdl in models:\n",
    "    print(f\"\\n[Test] {label}\")\n",
    "    summ, df_test = eval_on_test_set(mdl, TEST_NPZ, max_samples=MAX_SAMPLES_TEST, sample_rate=44100)\n",
    "    test_rows.append({\"model\": label,\n",
    "                      \"sts_mean\": round(float(summ[\"sts_mean\"]),4),\n",
    "                      \"centroid_err_mean\": round(float(summ[\"centroid_err_mean\"]),4),\n",
    "                      \"mfcc_mean\": round(float(summ[\"mfcc_mean\"]),4),\n",
    "                      \"n\": int(summ[\"n\"])})\n",
    "    test_frames[label] = df_test\n",
    "\n",
    "# 8) Keyword Baseline on the TEST set (non-LSTM)\n",
    "print(\"\\n[Test] Keyword_Baseline\")\n",
    "test_samples = _load_samples_from_npz(TEST_NPZ, max_samples=MAX_SAMPLES_TEST, sample_rate=44100, duration=2.0)\n",
    "kb_test_df = ev_tmp._evaluate_keyword_baseline(test_samples)\n",
    "test_frames[\"Keyword_Baseline\"] = kb_test_df\n",
    "kb_test_s = _summarize_df(kb_test_df)\n",
    "test_rows.append({\n",
    "    \"model\": \"Keyword_Baseline\",\n",
    "    \"sts_mean\": round(kb_test_s[\"sts_mean\"],4),\n",
    "    \"centroid_err_mean\": round(kb_test_s[\"centroid_err_mean\"],4),\n",
    "    \"mfcc_mean\": round(kb_test_s[\"mfcc_mean\"],4),\n",
    "    \"n\": int(kb_test_s[\"n\"])\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame(test_rows).set_index(\"model\")\n",
    "test_df.to_csv(\"results/model_means_test.csv\")\n",
    "print(\"Saved test means (incl. keyword baseline): results/model_means_test.csv\")\n",
    "\n",
    "# 9) Pretty displays\n",
    "print(\"\\n=== Head-to-Head (Shared Set) Means (incl. Keyword_Baseline) ===\")\n",
    "display(means_df_all)\n",
    "\n",
    "print(\"\\n=== Head-to-Head Pairwise Î” & p-values (incl. Keyword_Baseline) ===\")\n",
    "display(pairwise_df_all)\n",
    "\n",
    "print(\"\\n=== Test Set Means (incl. Keyword_Baseline) ===\")\n",
    "display(test_df)\n",
    "\n",
    "# === 10) Human-readable evaluation report (presentation style) ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIMODAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nShared Validation Set (71 samples)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing all models vs Keyword_Baseline\\n\")\n",
    "\n",
    "base = means_df_all.loc[\"Keyword_Baseline\"]\n",
    "for model in [\"LSTMABAR_Baseline\", \"FineTune_A\", \"FineTune_B\"]:\n",
    "    m = means_df_all.loc[model]\n",
    "    print(f\"\\n[{model}]\")\n",
    "    print(\"-\"*80)\n",
    "    # % changes vs baseline (Keyword_Baseline)\n",
    "    sts_delta = ((m.sts_mean - base.sts_mean) / base.sts_mean) * 100\n",
    "    sc_delta  = ((base.centroid_err_mean - m.centroid_err_mean) / base.centroid_err_mean) * 100\n",
    "    mfcc_delta= ((m.mfcc_mean - base.mfcc_mean) / base.mfcc_mean) * 100\n",
    "    print(f\"STS Mean: {m.sts_mean:.3f}  ({sts_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"Spectral Centroid Error: {m.centroid_err_mean:.0f} Hz  ({sc_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"MFCC Similarity: {m.mfcc_mean:.3f}  ({mfcc_delta:+.2f}% vs baseline)\")\n",
    "    print(f\"n = {int(m.n)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PAIRWISE COMPARISONS (Shared Set)\")\n",
    "print(\"=\"*80)\n",
    "print(pairwise_df_all.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET PERFORMANCE (73 samples)\")\n",
    "print(\"=\"*80)\n",
    "print(\"All results compared against Keyword_Baseline\\n\")\n",
    "\n",
    "tb = test_df.loc[\"Keyword_Baseline\"]\n",
    "for model in [\"LSTMABAR_Baseline\", \"FineTune_A\", \"FineTune_B\", \"FineTune_C\"]:\n",
    "    tm = test_df.loc[model]\n",
    "    sts_d = ((tm.sts_mean - tb.sts_mean) / tb.sts_mean) * 100\n",
    "    sc_d  = ((tb.centroid_err_mean - tm.centroid_err_mean) / tb.centroid_err_mean) * 100\n",
    "    mfcc_d= ((tm.mfcc_mean - tb.mfcc_mean) / tb.mfcc_mean) * 100\n",
    "    print(f\"\\n[{model}]\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"STS Mean: {tm.sts_mean:.3f} ({sts_d:+.2f}% vs baseline)\")\n",
    "    print(f\"Spectral Centroid Error: {tm.centroid_err_mean:.0f} Hz ({sc_d:+.2f}% vs baseline)\")\n",
    "    print(f\"MFCC Similarity: {tm.mfcc_mean:.3f} ({mfcc_d:+.2f}% vs baseline)\")\n",
    "    print(f\"n = {int(tm.n)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "ðŸ“ TEXT UNDERSTANDING (STS)\n",
    " â€¢ â‰¥ 0.75 â†’ Excellent\n",
    " â€¢ 0.60â€“0.75 â†’ Good\n",
    " â€¢ < 0.60 â†’ Needs improvement\n",
    "\n",
    "ðŸŽµ BRIGHTNESS ACCURACY (Spectral Centroid Error)\n",
    " â€¢ < 200 Hz â†’ Excellent\n",
    " â€¢ < 500 Hz â†’ Good\n",
    " â€¢ > 500 Hz â†’ Needs improvement\n",
    "\n",
    "ðŸŽ¸ TIMBRE QUALITY (MFCC Similarity)\n",
    " â€¢ > 0.80 â†’ Excellent\n",
    " â€¢ > 0.65 â†’ Good\n",
    " â€¢ â‰¤ 0.65 â†’ Needs improvement\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ Detailed CSVs in /results :\")\n",
    "print(\"   â€¢ model_means_shared.csv\")\n",
    "print(\"   â€¢ pairwise_stats_shared.csv\")\n",
    "print(\"   â€¢ model_means_test.csv\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab3py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
