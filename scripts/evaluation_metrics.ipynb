{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multimodal Evaluation Metrics for LSTMABAR\n",
    "Implements STS, Spectral Centroid Error, and MFCC Similarity\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from lstmabar_model import LSTMABAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMABAREvaluator:\n",
    "    \"\"\"\n",
    "    Complete evaluation framework for LSTMABAR model\n",
    "    Implements text-side and audio-side metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: LSTMABAR,\n",
    "        text_model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        sample_rate: int = 44100\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Text encoder for STS\n",
    "        self.text_encoder = SentenceTransformer(text_model_name)\n",
    "        \n",
    "        # Archetype descriptors for mapping\n",
    "        self.archetype_descriptors = {\n",
    "            'sine': 'smooth pure warm mellow soft gentle flowing',\n",
    "            'square': 'digital harsh buzzy retro electronic robotic',\n",
    "            'sawtooth': 'bright cutting metallic sharp aggressive',\n",
    "            'triangle': 'hollow woody muted filtered organic',\n",
    "            'noise': 'rough textured grainy distorted chaotic'\n",
    "        }\n",
    "        \n",
    "        # Expected spectral centroids for common descriptors (Hz)\n",
    "        self.centroid_map = {\n",
    "            'bright': 3500, 'dark': 800, 'warm': 1000, 'harsh': 4000,\n",
    "            'mellow': 1200, 'sharp': 5000, 'smooth': 1500, 'soft': 1100,\n",
    "            'aggressive': 3800, 'gentle': 1300, 'cutting': 4200\n",
    "        }\n",
    "    \n",
    "    # ==================== TEXT-SIDE EVALUATION (STS) ====================\n",
    "    \n",
    "    def compute_sts(\n",
    "        self,\n",
    "        input_description: str,\n",
    "        predicted_archetype_weights: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Semantic Textual Similarity: Does model understand description?\n",
    "        \n",
    "        Args:\n",
    "            input_description: Original user description\n",
    "            predicted_archetype_weights: Model's predicted archetype weights (5,)\n",
    "        \n",
    "        Returns:\n",
    "            STS score (0-1, higher is better)\n",
    "        \"\"\"\n",
    "        # Convert archetype weights to descriptive text\n",
    "        predicted_description = self._archetypes_to_text(predicted_archetype_weights)\n",
    "        \n",
    "        # Encode both descriptions\n",
    "        emb_input = self.text_encoder.encode(input_description, convert_to_tensor=True)\n",
    "        emb_predicted = self.text_encoder.encode(predicted_description, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        sts_score = util.cos_sim(emb_input, emb_predicted).item()\n",
    "        \n",
    "        return sts_score\n",
    "    \n",
    "    def _archetypes_to_text(\n",
    "        self,\n",
    "        archetype_weights: np.ndarray,\n",
    "        threshold: float = 0.1\n",
    "    ) -> str:\n",
    "        \"\"\"Convert archetype mixture to natural language\"\"\"\n",
    "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
    "        \n",
    "        # Get top contributing archetypes\n",
    "        descriptions = []\n",
    "        for i, (name, weight) in enumerate(zip(archetype_names, archetype_weights)):\n",
    "            if weight > threshold:\n",
    "                descriptions.append(self.archetype_descriptors[name])\n",
    "        \n",
    "        return ' '.join(descriptions) if descriptions else 'neutral sound'\n",
    "    \n",
    "    # ==================== AUDIO-SIDE EVALUATION ====================\n",
    "    \n",
    "    def spectral_centroid_error(\n",
    "        self,\n",
    "        output_audio: np.ndarray,\n",
    "        target_description: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measures brightness accuracy (Hz error)\n",
    "        \n",
    "        Args:\n",
    "            output_audio: Generated audio array\n",
    "            target_description: Target description\n",
    "        \n",
    "        Returns:\n",
    "            Absolute error in Hz (lower is better)\n",
    "        \"\"\"\n",
    "        # Compute actual spectral centroid\n",
    "        centroid = librosa.feature.spectral_centroid(\n",
    "            y=output_audio,\n",
    "            sr=self.sample_rate\n",
    "        )\n",
    "        actual_centroid = np.mean(centroid)\n",
    "        \n",
    "        # Determine expected centroid from description\n",
    "        expected_centroid = self._description_to_centroid(target_description)\n",
    "        \n",
    "        # Calculate error\n",
    "        error_hz = abs(actual_centroid - expected_centroid)\n",
    "        \n",
    "        return error_hz\n",
    "    \n",
    "    def _description_to_centroid(self, description: str) -> float:\n",
    "        \"\"\"Map description to expected spectral centroid\"\"\"\n",
    "        words = description.lower().split()\n",
    "        expected_centroids = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.centroid_map:\n",
    "                expected_centroids.append(self.centroid_map[word])\n",
    "        \n",
    "        if expected_centroids:\n",
    "            return np.mean(expected_centroids)\n",
    "        \n",
    "        return 2000  # Neutral default\n",
    "    \n",
    "    def mfcc_similarity(\n",
    "        self,\n",
    "        output_audio: np.ndarray,\n",
    "        reference_audio: np.ndarray,\n",
    "        n_mfcc: int = 13\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Measures timbral similarity (0-1, higher is better)\n",
    "        \n",
    "        Args:\n",
    "            output_audio: Generated audio array\n",
    "            reference_audio: Target/reference audio array\n",
    "            n_mfcc: Number of MFCC coefficients\n",
    "        \n",
    "        Returns:\n",
    "            Cosine similarity (1 is perfect match, >0.8 is good)\n",
    "        \"\"\"\n",
    "        # Extract MFCCs from both signals\n",
    "        mfcc_output = librosa.feature.mfcc(\n",
    "            y=output_audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mfcc=n_mfcc\n",
    "        )\n",
    "        mfcc_reference = librosa.feature.mfcc(\n",
    "            y=reference_audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mfcc=n_mfcc\n",
    "        )\n",
    "        \n",
    "        # Average across time\n",
    "        mfcc_output_mean = np.mean(mfcc_output, axis=1).reshape(1, -1)\n",
    "        mfcc_reference_mean = np.mean(mfcc_reference, axis=1).reshape(1, -1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(mfcc_output_mean, mfcc_reference_mean)[0][0]\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    # ==================== COMPREHENSIVE EVALUATION ====================\n",
    "    \n",
    "    def evaluate_single_transformation(\n",
    "        self,\n",
    "        input_audio: np.ndarray,\n",
    "        input_description: str,\n",
    "        output_audio: np.ndarray,\n",
    "        reference_audio: np.ndarray,\n",
    "        predicted_weights: np.ndarray\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Complete evaluation of a single transformation\n",
    "        \n",
    "        Returns:\n",
    "            Dict with all metrics\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            # Text-side (NLP understanding)\n",
    "            'sts_score': self.compute_sts(input_description, predicted_weights),\n",
    "            \n",
    "            # Audio-side (transformation quality)\n",
    "            'spectral_centroid_error_hz': self.spectral_centroid_error(\n",
    "                output_audio, input_description\n",
    "            ),\n",
    "            'mfcc_similarity': self.mfcc_similarity(output_audio, reference_audio)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        generate_reference: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set\n",
    "        \n",
    "        Args:\n",
    "            test_samples: List of dicts with 'audio', 'description', 'target_weights'\n",
    "            generate_reference: If True, generate reference audio from target weights\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with results for each sample\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        \n",
    "        print(f\"Evaluating model on {len(test_samples)} samples...\")\n",
    "        \n",
    "        for i, sample in enumerate(test_samples):\n",
    "            input_audio = sample['audio']\n",
    "            description = sample['description']\n",
    "            target_weights = sample['target_weights']\n",
    "            \n",
    "            # Convert to tensor\n",
    "            audio_tensor = torch.from_numpy(input_audio).unsqueeze(0).float().to(self.model.device)\n",
    "            \n",
    "            # Model inference\n",
    "            with torch.no_grad():\n",
    "                transformed, metadata = self.model.inference([description], audio_tensor)\n",
    "            \n",
    "            # Convert back to numpy\n",
    "            output_audio = transformed[0].cpu().numpy()\n",
    "            predicted_weights = metadata['predicted_weights'][0]\n",
    "            \n",
    "            # Generate reference audio if needed\n",
    "            if generate_reference:\n",
    "                # Use archetype generator to create ideal sound\n",
    "                reference_audio = self._generate_reference_audio(target_weights)\n",
    "            else:\n",
    "                reference_audio = input_audio  # Fallback\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_single_transformation(\n",
    "                input_audio,\n",
    "                description,\n",
    "                output_audio,\n",
    "                reference_audio,\n",
    "                predicted_weights\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            metrics['description'] = description\n",
    "            metrics['sample_idx'] = i\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i+1}/{len(test_samples)} samples\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _generate_reference_audio(\n",
    "        self,\n",
    "        archetype_weights: np.ndarray,\n",
    "        duration: float = 2.0\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Generate reference audio from archetype mixture\"\"\"\n",
    "        n_samples = int(self.sample_rate * duration)\n",
    "        t = np.linspace(0, duration, n_samples, endpoint=False)\n",
    "        frequency = 440  # A4\n",
    "        \n",
    "        audio = np.zeros(n_samples)\n",
    "        \n",
    "        # Generate each archetype component\n",
    "        archetypes = {\n",
    "            'sine': np.sin(2 * np.pi * frequency * t),\n",
    "            'square': np.sign(np.sin(2 * np.pi * frequency * t)),\n",
    "            'sawtooth': 2 * (t * frequency - np.floor(0.5 + t * frequency)),\n",
    "            'triangle': 2 * np.abs(2 * (t * frequency - np.floor(t * frequency + 0.5))) - 1,\n",
    "            'noise': np.random.randn(n_samples) * 0.3\n",
    "        }\n",
    "        \n",
    "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
    "        \n",
    "        # Mix according to weights\n",
    "        for i, name in enumerate(archetype_names):\n",
    "            audio += archetype_weights[i] * archetypes[name]\n",
    "        \n",
    "        # Normalize\n",
    "        audio = audio / (np.max(np.abs(audio)) + 1e-8) * 0.95\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    def compare_with_baseline(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        baseline_model: Optional[object] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare LSTMABAR with baseline model\n",
    "        \n",
    "        Args:\n",
    "            test_samples: Test samples\n",
    "            baseline_model: Baseline model (if None, uses simple keyword matching)\n",
    "        \n",
    "        Returns:\n",
    "            Comparison DataFrame\n",
    "        \"\"\"\n",
    "        # Evaluate LSTMABAR\n",
    "        lstmabar_results = self.evaluate_model(test_samples)\n",
    "        \n",
    "        # Evaluate baseline\n",
    "        if baseline_model is None:\n",
    "            baseline_results = self._evaluate_keyword_baseline(test_samples)\n",
    "        else:\n",
    "            baseline_results = baseline_model.evaluate(test_samples)\n",
    "        \n",
    "        # Compute statistics\n",
    "        comparison = self._compute_comparison_stats(lstmabar_results, baseline_results)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _evaluate_keyword_baseline(self, test_samples: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate simple keyword-matching baseline\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Simple keyword â†’ archetype mapping\n",
    "        keyword_map = {\n",
    "            'bright': [0.1, 0.1, 0.6, 0.1, 0.1],  # Mostly sawtooth\n",
    "            'warm': [0.6, 0.1, 0.1, 0.2, 0.0],    # Mostly sine\n",
    "            'harsh': [0.1, 0.5, 0.2, 0.1, 0.1],   # Mostly square\n",
    "            'smooth': [0.6, 0.1, 0.1, 0.2, 0.0],  # Mostly sine\n",
    "            'distorted': [0.1, 0.2, 0.2, 0.1, 0.4] # High noise\n",
    "        }\n",
    "        \n",
    "        for i, sample in enumerate(test_samples):\n",
    "            description = sample['description'].lower()\n",
    "            \n",
    "            # Find matching keywords\n",
    "            predicted_weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform default\n",
    "            for keyword, weights in keyword_map.items():\n",
    "                if keyword in description:\n",
    "                    predicted_weights = np.array(weights)\n",
    "                    break\n",
    "            \n",
    "            # Simple transformation (just apply gain based on brightness)\n",
    "            output_audio = sample['audio'].copy()\n",
    "            if 'bright' in description:\n",
    "                # Boost high frequencies (simplified)\n",
    "                output_audio = output_audio * 1.2\n",
    "            \n",
    "            reference_audio = self._generate_reference_audio(sample['target_weights'])\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_single_transformation(\n",
    "                sample['audio'],\n",
    "                description,\n",
    "                output_audio,\n",
    "                reference_audio,\n",
    "                predicted_weights\n",
    "            )\n",
    "            \n",
    "            metrics['description'] = description\n",
    "            metrics['sample_idx'] = i\n",
    "            results.append(metrics)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _compute_comparison_stats(\n",
    "        self,\n",
    "        lstmabar_results: pd.DataFrame,\n",
    "        baseline_results: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute statistical comparison\"\"\"\n",
    "        metrics = ['sts_score', 'spectral_centroid_error_hz', 'mfcc_similarity']\n",
    "        \n",
    "        comparison = {\n",
    "            'Metric': [],\n",
    "            'Baseline_Mean': [],\n",
    "            'Baseline_Std': [],\n",
    "            'LSTMABAR_Mean': [],\n",
    "            'LSTMABAR_Std': [],\n",
    "            'Improvement': [],\n",
    "            'p_value': []\n",
    "        }\n",
    "        \n",
    "        for metric in metrics:\n",
    "            baseline_vals = baseline_results[metric].values\n",
    "            lstmabar_vals = lstmabar_results[metric].values\n",
    "            \n",
    "            baseline_mean = np.mean(baseline_vals)\n",
    "            baseline_std = np.std(baseline_vals)\n",
    "            lstmabar_mean = np.mean(lstmabar_vals)\n",
    "            lstmabar_std = np.std(lstmabar_vals)\n",
    "            \n",
    "            comparison['Metric'].append(metric)\n",
    "            comparison['Baseline_Mean'].append(f\"{baseline_mean:.4f}\")\n",
    "            comparison['Baseline_Std'].append(f\"{baseline_std:.4f}\")\n",
    "            comparison['LSTMABAR_Mean'].append(f\"{lstmabar_mean:.4f}\")\n",
    "            comparison['LSTMABAR_Std'].append(f\"{lstmabar_std:.4f}\")\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if 'error' in metric.lower():\n",
    "                # Lower is better\n",
    "                improvement = ((baseline_mean - lstmabar_mean) / baseline_mean) * 100\n",
    "            else:\n",
    "                # Higher is better\n",
    "                improvement = ((lstmabar_mean - baseline_mean) / baseline_mean) * 100\n",
    "            \n",
    "            comparison['Improvement'].append(f\"{improvement:.2f}%\")\n",
    "            \n",
    "            # Paired t-test\n",
    "            _, p_val = ttest_rel(baseline_vals, lstmabar_vals)\n",
    "            comparison['p_value'].append(f\"{p_val:.4f}\")\n",
    "        \n",
    "        return pd.DataFrame(comparison)\n",
    "    \n",
    "    def generate_evaluation_report(\n",
    "        self,\n",
    "        test_samples: List[Dict],\n",
    "        save_path: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LSTMABAR MODEL EVALUATION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = self.evaluate_model(test_samples)\n",
    "        \n",
    "        # Compare with baseline\n",
    "        comparison = self.compare_with_baseline(test_samples)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPARISON WITH BASELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(comparison.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INTERPRETATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # STS interpretation\n",
    "        sts_mean = results['sts_score'].mean()\n",
    "        print(f\"\\nðŸ“ TEXT UNDERSTANDING (STS Score):\")\n",
    "        print(f\"   Mean: {sts_mean:.3f}\")\n",
    "        if sts_mean > 0.75:\n",
    "            print(f\"   âœ“ Excellent semantic understanding!\")\n",
    "        elif sts_mean > 0.60:\n",
    "            print(f\"   â†’ Good semantic understanding\")\n",
    "        else:\n",
    "            print(f\"   âš  Needs improvement in NLP comprehension\")\n",
    "        \n",
    "        # Spectral centroid interpretation\n",
    "        sc_mean = results['spectral_centroid_error_hz'].mean()\n",
    "        print(f\"\\nðŸŽµ BRIGHTNESS ACCURACY (Spectral Centroid Error):\")\n",
    "        print(f\"   Mean: {sc_mean:.0f} Hz\")\n",
    "        if sc_mean < 200:\n",
    "            print(f\"   âœ“ Excellent brightness targeting!\")\n",
    "        elif sc_mean < 500:\n",
    "            print(f\"   â†’ Good brightness control\")\n",
    "        else:\n",
    "            print(f\"   âš  Brightness targeting needs improvement\")\n",
    "        \n",
    "        # MFCC interpretation\n",
    "        mfcc_mean = results['mfcc_similarity'].mean()\n",
    "        print(f\"\\nðŸŽ¸ TIMBRE QUALITY (MFCC Similarity):\")\n",
    "        print(f\"   Mean: {mfcc_mean:.3f}\")\n",
    "        if mfcc_mean > 0.80:\n",
    "            print(f\"   âœ“ Excellent timbral matching!\")\n",
    "        elif mfcc_mean > 0.65:\n",
    "            print(f\"   â†’ Good timbral similarity\")\n",
    "        else:\n",
    "            print(f\"   âš  Timbre quality needs improvement\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # Save detailed results\n",
    "        if save_path:\n",
    "            results.to_csv(save_path, index=False)\n",
    "            comparison.to_csv(save_path.replace('.csv', '_comparison.csv'), index=False)\n",
    "            print(f\"\\nDetailed results saved to {save_path}\")\n",
    "\n",
    "    def evaluate_model_on_test_set(\n",
    "        self,\n",
    "        test_data_path: str,\n",
    "        max_samples: Optional[int] = None,\n",
    "        save_results: bool = True,\n",
    "        results_path: str = 'test_results.csv'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate model on test set from MusicCaps\n",
    "        \n",
    "        Args:\n",
    "            test_data_path: Path to test .npz file\n",
    "            max_samples: Max samples to evaluate (None = all)\n",
    "            save_results: Whether to save results\n",
    "            results_path: Path to save results\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EVALUATING ON TEST SET\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"Loading test data from {test_data_path}...\")\n",
    "        data = np.load(test_data_path, allow_pickle=True)\n",
    "        \n",
    "        vectors = data['archetype_vectors']\n",
    "        descriptions = data['descriptions'].tolist()\n",
    "        audio_paths = data['audio_paths'].tolist()\n",
    "        \n",
    "        n_samples = min(len(descriptions), max_samples) if max_samples else len(descriptions)\n",
    "        print(f\"Evaluating on {n_samples} test samples\\n\")\n",
    "        \n",
    "        # Prepare test samples\n",
    "        test_samples = []\n",
    "        for i in range(n_samples):\n",
    "            audio_path = audio_paths[i]\n",
    "            \n",
    "            # Check if audio exists\n",
    "            if not Path(audio_path).exists():\n",
    "                print(f\"Skipping {i}: audio file not found\")\n",
    "                continue\n",
    "            \n",
    "            # Load audio\n",
    "            try:\n",
    "                audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=2.0)\n",
    "                \n",
    "                # Pad or trim\n",
    "                target_length = int(self.sample_rate * 2.0)\n",
    "                if len(audio) < target_length:\n",
    "                    audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target_length]\n",
    "                \n",
    "                test_samples.append({\n",
    "                    'audio': audio,\n",
    "                    'description': descriptions[i],\n",
    "                    'target_weights': vectors[i]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {audio_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully loaded {len(test_samples)} test samples\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results_df = self.evaluate_model(test_samples, generate_reference=True)\n",
    "        \n",
    "        # Save results\n",
    "        if save_results:\n",
    "            results_df.to_csv(results_path, index=False)\n",
    "            print(f\"\\nâœ“ Results saved to {results_path}\")\n",
    "        \n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LSTMABAR model...\n",
      "Loading text encoder: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from lstmabar_model import LSTMABAR\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load trained model\n",
    "print(\"Loading LSTMABAR model...\")\n",
    "model = LSTMABAR(\n",
    "    embedding_dim=768,\n",
    "    audio_architecture='resnet',\n",
    "    sample_rate=44100,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model.pth (epoch 1)\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "model.load_checkpoint('checkpoints/best_model.pth')\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = LSTMABAREvaluator(model, sample_rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating test samples...\n"
     ]
    }
   ],
   "source": [
    "# Create test samples\n",
    "print(\"\\nCreating test samples...\")\n",
    "test_samples = []\n",
    "\n",
    "# Example 1: Bright guitar\n",
    "audio1 = np.random.randn(44100 * 2)  # 2 seconds\n",
    "test_samples.append({\n",
    "    'audio': audio1,\n",
    "    'description': 'bright and cutting guitar with metallic tone',\n",
    "    'target_weights': np.array([0.1, 0.1, 0.6, 0.1, 0.1])\n",
    "})\n",
    "\n",
    "# Example 2: Warm piano\n",
    "audio2 = np.random.randn(44100 * 2)\n",
    "test_samples.append({\n",
    "    'audio': audio2,\n",
    "    'description': 'warm smooth piano melody with gentle sustain',\n",
    "    'target_weights': np.array([0.6, 0.05, 0.1, 0.2, 0.05])\n",
    "})\n",
    "\n",
    "# Example 3: Digital synth\n",
    "audio3 = np.random.randn(44100 * 2)\n",
    "test_samples.append({\n",
    "    'audio': audio3,\n",
    "    'description': 'harsh digital synth with buzzy retro sound',\n",
    "    'target_weights': np.array([0.1, 0.55, 0.15, 0.1, 0.1])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n",
      "\n",
      "================================================================================\n",
      "LSTMABAR MODEL EVALUATION REPORT\n",
      "================================================================================\n",
      "Evaluating model on 3 samples...\n",
      "Evaluating model on 3 samples...\n",
      "\n",
      "================================================================================\n",
      "COMPARISON WITH BASELINE\n",
      "================================================================================\n",
      "                    Metric Baseline_Mean Baseline_Std LSTMABAR_Mean LSTMABAR_Std Improvement p_value\n",
      "                 sts_score        0.5507       0.1440        0.4061       0.1241     -26.26%  0.3797\n",
      "spectral_centroid_error_hz     7992.9986    1249.6807     7252.6950    1247.1636       9.26%  0.0000\n",
      "           mfcc_similarity       -0.6586       0.1760       -0.6140       0.1816      -6.77%  0.0497\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ TEXT UNDERSTANDING (STS Score):\n",
      "   Mean: 0.406\n",
      "   âš  Needs improvement in NLP comprehension\n",
      "\n",
      "ðŸŽµ BRIGHTNESS ACCURACY (Spectral Centroid Error):\n",
      "   Mean: 7253 Hz\n",
      "   âš  Brightness targeting needs improvement\n",
      "\n",
      "ðŸŽ¸ TIMBRE QUALITY (MFCC Similarity):\n",
      "   Mean: -0.613\n",
      "   âš  Timbre quality needs improvement\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Detailed results saved to evaluation_results.csv\n",
      "\n",
      "âœ“ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"\\nRunning evaluation...\")\n",
    "evaluator.generate_evaluation_report(\n",
    "    test_samples,\n",
    "    save_path='evaluation_results.csv'\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING ON TEST SET\n",
      "================================================================================\n",
      "\n",
      "Loading test data from musiccaps_training_data_test.npz...\n",
      "Evaluating on 73 test samples\n",
      "\n",
      "Successfully loaded 73 test samples\n",
      "\n",
      "Evaluating model on 73 samples...\n",
      "  Processed 10/73 samples\n",
      "  Processed 20/73 samples\n",
      "  Processed 30/73 samples\n",
      "  Processed 40/73 samples\n",
      "  Processed 50/73 samples\n",
      "  Processed 60/73 samples\n",
      "  Processed 70/73 samples\n",
      "\n",
      "âœ“ Results saved to test_results.csv\n",
      "\n",
      "Test Performance:\n",
      "  STS: 0.214\n",
      "  SC Error: 5533 Hz\n",
      "  MFCC: 0.384\n"
     ]
    }
   ],
   "source": [
    "test_path = 'musiccaps_training_data_test.npz'\n",
    "\n",
    "test_results = evaluator.evaluate_model_on_test_set(\n",
    "    test_data_path=test_path,\n",
    "    max_samples=100,\n",
    "    save_results=True,\n",
    "    results_path='test_results.csv'\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  STS: {test_results['sts_score'].mean():.3f}\")\n",
    "print(f\"  SC Error: {test_results['spectral_centroid_error_hz'].mean():.0f} Hz\")\n",
    "print(f\"  MFCC: {test_results['mfcc_similarity'].mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
