{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Audio Tower - Spectrogram-Aware Acoustic Encoding\n",
    "Encodes audio spectrograms into acoustic embeddings\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio tower for encoding spectrograms into acoustic embeddings\n",
    "    \n",
    "    Architecture options:\n",
    "    - ResNet-18 for image-like spectrogram processing\n",
    "    - Audio Spectrogram Transformer (AST) for patch-based encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=768,\n",
    "        architecture='resnet',  # 'resnet' or 'ast'\n",
    "        sample_rate=44100,\n",
    "        n_mels=128,\n",
    "        n_fft=2048,\n",
    "        hop_length=512,\n",
    "        use_archetype_supervision=True,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.architecture = architecture\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.device = device\n",
    "        \n",
    "        # Mel spectrogram transform\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True\n",
    "        )\n",
    "        \n",
    "        # Choose backbone architecture\n",
    "        if architecture == 'resnet':\n",
    "            self.backbone = ResNetAudioBackbone(embedding_dim)\n",
    "        elif architecture == 'ast':\n",
    "            self.backbone = AudioSpectrogramTransformer(embedding_dim, n_mels)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown architecture: {architecture}\")\n",
    "        \n",
    "        # Auxiliary archetype prediction head (for supervision)\n",
    "        self.use_archetype_supervision = use_archetype_supervision\n",
    "        if use_archetype_supervision:\n",
    "            self.archetype_classifier = nn.Sequential(\n",
    "                nn.Linear(embedding_dim, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(256, 5),  # 5 archetypes\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def extract_spectrogram(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert audio waveform to log-mel spectrogram\n",
    "        \n",
    "        Args:\n",
    "            audio: Audio tensor of shape (batch, samples) or (samples,)\n",
    "        \n",
    "        Returns:\n",
    "            Log-mel spectrogram of shape (batch, 1, n_mels, time)\n",
    "        \"\"\"\n",
    "        # Ensure batch dimension\n",
    "        if audio.dim() == 1:\n",
    "            audio = audio.unsqueeze(0)\n",
    "        \n",
    "        # Compute mel spectrogram\n",
    "        mel_spec = self.mel_transform(audio)\n",
    "        \n",
    "        # Convert to log scale\n",
    "        log_mel_spec = torch.log(mel_spec + 1e-9)\n",
    "        \n",
    "        # Add channel dimension if needed\n",
    "        if log_mel_spec.dim() == 3:\n",
    "            log_mel_spec = log_mel_spec.unsqueeze(1)\n",
    "        \n",
    "        return log_mel_spec\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        audio: torch.Tensor, \n",
    "        return_archetype_pred=False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Encode audio into embeddings\n",
    "        \n",
    "        Args:\n",
    "            audio: Audio waveform tensor (batch, samples)\n",
    "            return_archetype_pred: Whether to return archetype predictions\n",
    "        \n",
    "        Returns:\n",
    "            - Audio embeddings (batch, embedding_dim)\n",
    "            - Optional archetype predictions (batch, 5)\n",
    "        \"\"\"\n",
    "        # Extract spectrogram\n",
    "        spectrogram = self.extract_spectrogram(audio)\n",
    "        \n",
    "        # Pass through backbone\n",
    "        embeddings = self.backbone(spectrogram)\n",
    "        \n",
    "        # L2 normalize for contrastive learning\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Optional archetype prediction\n",
    "        archetype_pred = None\n",
    "        if return_archetype_pred and self.use_archetype_supervision:\n",
    "            archetype_pred = self.archetype_classifier(embeddings)\n",
    "        \n",
    "        return embeddings, archetype_pred\n",
    "    \n",
    "    def load_audio_file(self, audio_path: str, duration: Optional[float] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Load audio file and convert to tensor\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            duration: Optional duration to load (in seconds)\n",
    "        \n",
    "        Returns:\n",
    "            Audio tensor\n",
    "        \"\"\"\n",
    "        audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=duration)\n",
    "        audio_tensor = torch.from_numpy(audio).float().to(self.device)\n",
    "        return audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResNetAudioBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-18 based backbone for spectrogram encoding\n",
    "    Treats spectrograms as images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=768):\n",
    "        super(ResNetAudioBackbone, self).__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # ResNet blocks\n",
    "        self.layer1 = self._make_layer(64, 64, blocks=2)\n",
    "        self.layer2 = self._make_layer(64, 128, blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, blocks=2, stride=2)\n",
    "        \n",
    "        # Global pooling and projection\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, embedding_dim)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
    "        \"\"\"Create a ResNet layer with residual blocks\"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        # First block may downsample\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        \n",
    "        # Remaining blocks\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # ResNet blocks\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Pool and project\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Basic residual block for ResNet\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSpectrogramTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio Spectrogram Transformer (AST) using patch embeddings\n",
    "    Inspired by Vision Transformer for audio\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=768, n_mels=128, patch_size=16):\n",
    "        super(AudioSpectrogramTransformer, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "        # Patch embedding layer\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # Positional embedding (learnable)\n",
    "        self.num_patches = (n_mels // patch_size) * 10  # Assuming ~10 time patches\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, embedding_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        \n",
    "        # Classification token (like BERT's [CLS])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Extract patches\n",
    "        patches = self.patch_embed(x)  # (batch, embed_dim, h_patches, w_patches)\n",
    "        patches = patches.flatten(2).transpose(1, 2)  # (batch, num_patches, embed_dim)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        patches = patches + self.pos_embed[:, :patches.size(1), :]\n",
    "        \n",
    "        # Add classification token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        patches = torch.cat([cls_tokens, patches], dim=1)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        encoded = self.transformer(patches)\n",
    "        \n",
    "        # Use classification token as final embedding\n",
    "        embedding = encoded[:, 0]\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract traditional audio features for analysis and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=44100):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def extract_features(self, audio: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Extract comprehensive audio features\n",
    "        \n",
    "        Returns dict with:\n",
    "        - spectral_centroid, spectral_rolloff, spectral_bandwidth\n",
    "        - zero_crossing_rate, rms_energy, harmonic_ratio\n",
    "        - mfccs\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Spectral features\n",
    "        features['spectral_centroid'] = np.mean(\n",
    "            librosa.feature.spectral_centroid(y=audio, sr=self.sample_rate)\n",
    "        )\n",
    "        features['spectral_rolloff'] = np.mean(\n",
    "            librosa.feature.spectral_rolloff(y=audio, sr=self.sample_rate)\n",
    "        )\n",
    "        features['spectral_bandwidth'] = np.mean(\n",
    "            librosa.feature.spectral_bandwidth(y=audio, sr=self.sample_rate)\n",
    "        )\n",
    "        \n",
    "        # Zero crossing rate (indicates noisiness)\n",
    "        features['zero_crossing_rate'] = np.mean(\n",
    "            librosa.feature.zero_crossing_rate(audio)\n",
    "        )\n",
    "        \n",
    "        # RMS energy\n",
    "        features['rms_energy'] = np.mean(librosa.feature.rms(y=audio))\n",
    "        \n",
    "        # Harmonic/percussive separation\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(audio)\n",
    "        features['harmonic_ratio'] = (\n",
    "            np.mean(np.abs(y_harmonic)) / (np.mean(np.abs(audio)) + 1e-10)\n",
    "        )\n",
    "        \n",
    "        # MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=13)\n",
    "        features['mfccs'] = np.mean(mfccs, axis=1)\n",
    "        \n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "# Initialize audio encoder with ResNet\n",
    "audio_encoder_resnet = AudioEncoder(\n",
    "    embedding_dim=768,\n",
    "    architecture='resnet',\n",
    "    use_archetype_supervision=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with random audio\n",
    "batch_size = 4\n",
    "audio_length = 44100 * 2  # 2 seconds\n",
    "dummy_audio = torch.randn(batch_size, audio_length).to(device)\n",
    "\n",
    "embeddings, archetype_pred = audio_encoder_resnet(\n",
    "    dummy_audio, \n",
    "    return_archetype_pred=True\n",
    ")\n",
    "\n",
    "print(f\"Audio embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Archetype predictions shape: {archetype_pred.shape}\")\n",
    "print(f\"Sample embedding norm: {torch.norm(embeddings[0]).item():.4f}\")\n",
    "print(f\"Sample archetype prediction: {archetype_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AST architecture\n",
    "print(\"\\n--- Testing AST Architecture ---\")\n",
    "audio_encoder_ast = AudioEncoder(\n",
    "    embedding_dim=768,\n",
    "    architecture='ast',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "embeddings_ast, _ = audio_encoder_ast(dummy_audio)\n",
    "print(f\"AST embeddings shape: {embeddings_ast.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
