{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GXBMLtDBrUr"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/Users/shanthgopalswamy/Desktop/mids266_final/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "DDSP Transformation Engine - Differentiable Audio Synthesis\n",
        "Applies archetype-conditioned transformations to audio using DDSP principles\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import librosa\n",
        "from scipy import signal\n",
        "from typing import Dict, Tuple, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCjIcZbBBxn9"
      },
      "outputs": [],
      "source": [
        "class DDSPTransformationEngine(nn.Module):\n",
        "    \"\"\"\n",
        "    Differentiable Digital Signal Processing transformation engine\n",
        "\n",
        "    Transforms input audio based on predicted archetype mixture weights\n",
        "    Implements harmonic synthesis, filtering, and distortion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sample_rate=44100,\n",
        "        frame_size=64,\n",
        "        learnable_filters=True,\n",
        "        device='cpu'\n",
        "    ):\n",
        "        super(DDSPTransformationEngine, self).__init__()\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.frame_size = frame_size\n",
        "        self.device = device\n",
        "\n",
        "        # Learnable filter banks for each archetype\n",
        "        if learnable_filters:\n",
        "            self.archetype_filters = nn.ModuleDict({\n",
        "                'sine': LearnableFilter(num_bands=128, device=device),\n",
        "                'square': LearnableFilter(num_bands=128, device=device),\n",
        "                'sawtooth': LearnableFilter(num_bands=128, device=device),\n",
        "                'triangle': LearnableFilter(num_bands=128, device=device),\n",
        "                'noise': LearnableFilter(num_bands=128, device=device)\n",
        "            })\n",
        "        else:\n",
        "            self.archetype_filters = None\n",
        "\n",
        "        # Harmonic synthesizer\n",
        "        self.harmonic_synth = HarmonicSynthesizer(sample_rate, device=device)\n",
        "\n",
        "        # Noise generator\n",
        "        self.noise_gen = NoiseGenerator(device=device)\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def decompose_audio(self, audio: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Decompose audio into harmonic, noise, and residual components\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio (batch, samples)\n",
        "\n",
        "        Returns:\n",
        "            Dict with 'harmonic', 'noise', 'residual' components\n",
        "        \"\"\"\n",
        "        # Convert to numpy for librosa processing\n",
        "        audio_np = audio.cpu().numpy()\n",
        "\n",
        "        harmonics = []\n",
        "        noises = []\n",
        "\n",
        "        for i in range(audio_np.shape[0]):\n",
        "            # Harmonic-percussive separation\n",
        "            y_harmonic, y_percussive = librosa.effects.hpss(audio_np[i])\n",
        "\n",
        "            harmonics.append(y_harmonic)\n",
        "            noises.append(y_percussive)\n",
        "\n",
        "        harmonic = torch.from_numpy(np.stack(harmonics)).float().to(self.device)\n",
        "        noise = torch.from_numpy(np.stack(noises)).float().to(self.device)\n",
        "\n",
        "        # Residual is what's left\n",
        "        residual = audio - harmonic - noise\n",
        "\n",
        "        return {\n",
        "            'harmonic': harmonic,\n",
        "            'noise': noise,\n",
        "            'residual': residual\n",
        "        }\n",
        "\n",
        "    def apply_archetype_transformation(\n",
        "        self,\n",
        "        audio: torch.Tensor,\n",
        "        archetype_weights: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Transform audio based on archetype mixture\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio (batch, samples)\n",
        "            archetype_weights: Archetype weights (batch, 5)\n",
        "\n",
        "        Returns:\n",
        "            Transformed audio (batch, samples)\n",
        "        \"\"\"\n",
        "        batch_size = audio.shape[0]\n",
        "\n",
        "        # Decompose into components\n",
        "        components = self.decompose_audio(audio)\n",
        "\n",
        "        # Transform each component based on archetype weights\n",
        "        transformed = torch.zeros_like(audio)\n",
        "\n",
        "        archetype_names = ['sine', 'square', 'sawtooth', 'triangle', 'noise']\n",
        "\n",
        "        for i, archetype in enumerate(archetype_names):\n",
        "            weight = archetype_weights[:, i].unsqueeze(1)  # (batch, 1)\n",
        "\n",
        "            # Apply archetype-specific transformation\n",
        "            if archetype == 'sine':\n",
        "                # Enhance fundamental, suppress harmonics\n",
        "                component = self._apply_sine_transform(components['harmonic'])\n",
        "\n",
        "            elif archetype == 'square':\n",
        "                # Add odd harmonics, digital character\n",
        "                component = self._apply_square_transform(components['harmonic'])\n",
        "\n",
        "            elif archetype == 'sawtooth':\n",
        "                # Enhance all harmonics, brighten\n",
        "                component = self._apply_sawtooth_transform(components['harmonic'])\n",
        "\n",
        "            elif archetype == 'triangle':\n",
        "                # Reduce even harmonics, mellow\n",
        "                component = self._apply_triangle_transform(components['harmonic'])\n",
        "\n",
        "            elif archetype == 'noise':\n",
        "                # Add noise texture\n",
        "                component = self._apply_noise_transform(components['noise'])\n",
        "\n",
        "            # Weight and accumulate\n",
        "            transformed += weight * component\n",
        "\n",
        "        # Normalize to prevent clipping\n",
        "        max_val = torch.abs(transformed).max(dim=1, keepdim=True)[0]\n",
        "        transformed = transformed / (max_val + 1e-8) * 0.95\n",
        "\n",
        "        return transformed\n",
        "\n",
        "    def _apply_sine_transform(self, audio: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Make audio more sine-like: smooth, fundamental-focused\"\"\"\n",
        "        # Apply strong low-pass filter\n",
        "        filtered = self._lowpass_filter(audio, cutoff=0.3)\n",
        "        return filtered\n",
        "\n",
        "    def _apply_square_transform(self, audio: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Make audio more square-like: add odd harmonics\"\"\"\n",
        "        # Apply soft clipping to generate harmonics\n",
        "        clipped = torch.tanh(audio * 2.0) * 0.7\n",
        "        return clipped\n",
        "\n",
        "    def _apply_sawtooth_transform(self, audio: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Make audio more sawtooth-like: bright, all harmonics\"\"\"\n",
        "        # Emphasize high frequencies\n",
        "        filtered = self._highpass_filter(audio, cutoff=0.4)\n",
        "        enhanced = audio + filtered * 0.5\n",
        "        return enhanced\n",
        "\n",
        "    def _apply_triangle_transform(self, audio: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Make audio more triangle-like: mellow, reduced harmonics\"\"\"\n",
        "        # Gentle filtering\n",
        "        filtered = self._lowpass_filter(audio, cutoff=0.4)\n",
        "        return filtered\n",
        "\n",
        "    def _apply_noise_transform(self, audio: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add noise texture\"\"\"\n",
        "        # Generate shaped noise\n",
        "        noise = torch.randn_like(audio) * 0.1\n",
        "        return audio + noise\n",
        "\n",
        "    def _lowpass_filter(self, audio: torch.Tensor, cutoff: float) -> torch.Tensor:\n",
        "        \"\"\"Apply learnable low-pass filter\"\"\"\n",
        "        if self.archetype_filters is not None:\n",
        "            return self.archetype_filters['sine'](audio, filter_type='lowpass', cutoff=cutoff)\n",
        "        else:\n",
        "            # Fallback to simple filter\n",
        "            return self._simple_lowpass(audio, cutoff)\n",
        "\n",
        "    def _highpass_filter(self, audio: torch.Tensor, cutoff: float) -> torch.Tensor:\n",
        "        \"\"\"Apply learnable high-pass filter\"\"\"\n",
        "        if self.archetype_filters is not None:\n",
        "            return self.archetype_filters['sawtooth'](audio, filter_type='highpass', cutoff=cutoff)\n",
        "        else:\n",
        "            return self._simple_highpass(audio, cutoff)\n",
        "\n",
        "    def _simple_lowpass(self, audio: torch.Tensor, cutoff: float) -> torch.Tensor:\n",
        "        \"\"\"Simple IIR low-pass filter\"\"\"\n",
        "        # Convert to numpy for scipy filtering\n",
        "        audio_np = audio.cpu().numpy()\n",
        "\n",
        "        # Butterworth filter\n",
        "        b, a = signal.butter(4, cutoff, btype='low')\n",
        "\n",
        "        filtered = []\n",
        "        for i in range(audio_np.shape[0]):\n",
        "            y_filt = signal.filtfilt(b, a, audio_np[i])\n",
        "            filtered.append(y_filt)\n",
        "\n",
        "        return torch.from_numpy(np.stack(filtered)).float().to(self.device)\n",
        "\n",
        "    def _simple_highpass(self, audio: torch.Tensor, cutoff: float) -> torch.Tensor:\n",
        "        \"\"\"Simple IIR high-pass filter\"\"\"\n",
        "        audio_np = audio.cpu().numpy()\n",
        "\n",
        "        b, a = signal.butter(2, cutoff, btype='high')\n",
        "\n",
        "        filtered = []\n",
        "        for i in range(audio_np.shape[0]):\n",
        "            y_filt = signal.filtfilt(b, a, audio_np[i])\n",
        "            filtered.append(y_filt)\n",
        "\n",
        "        return torch.from_numpy(np.stack(filtered)).float().to(self.device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        audio: torch.Tensor,\n",
        "        archetype_weights: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass: transform audio with archetype weights\n",
        "        \"\"\"\n",
        "        return self.apply_archetype_transformation(audio, archetype_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MPD3VlsrCUGF"
      },
      "outputs": [],
      "source": [
        "class LearnableFilter(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable frequency-domain filter\n",
        "    Implements differentiable filtering in spectral domain\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_bands=128, device='cpu'):\n",
        "        super(LearnableFilter, self).__init__()\n",
        "\n",
        "        self.num_bands = num_bands\n",
        "\n",
        "        # Learnable filter response\n",
        "        self.filter_response = nn.Parameter(torch.ones(num_bands))\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        audio: torch.Tensor,\n",
        "        filter_type: str = 'lowpass',\n",
        "        cutoff: float = 0.5\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply learnable filter to audio\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio (batch, samples)\n",
        "            filter_type: 'lowpass' or 'highpass'\n",
        "            cutoff: Cutoff frequency (0-1, normalized)\n",
        "\n",
        "        Returns:\n",
        "            Filtered audio\n",
        "        \"\"\"\n",
        "        # FFT\n",
        "        audio_fft = torch.fft.rfft(audio)\n",
        "\n",
        "        # Create filter response\n",
        "        freq_bins = audio_fft.shape[-1]\n",
        "        filter_curve = F.interpolate(\n",
        "            self.filter_response.unsqueeze(0).unsqueeze(0),\n",
        "            size=freq_bins,\n",
        "            mode='linear',\n",
        "            align_corners=False\n",
        "        ).squeeze()\n",
        "\n",
        "        # Apply sigmoid for smooth response\n",
        "        filter_curve = torch.sigmoid(filter_curve)\n",
        "\n",
        "        # Modify based on filter type\n",
        "        cutoff_bin = int(cutoff * freq_bins)\n",
        "        if filter_type == 'lowpass':\n",
        "            mask = torch.ones_like(filter_curve)\n",
        "            mask[cutoff_bin:] *= 0.1\n",
        "            filter_curve = filter_curve * mask\n",
        "        elif filter_type == 'highpass':\n",
        "            mask = torch.ones_like(filter_curve)\n",
        "            mask[:cutoff_bin] *= 0.1\n",
        "            filter_curve = filter_curve * mask\n",
        "\n",
        "        # Apply filter in frequency domain\n",
        "        filtered_fft = audio_fft * filter_curve\n",
        "\n",
        "        # Inverse FFT\n",
        "        filtered_audio = torch.fft.irfft(filtered_fft, n=audio.shape[-1])\n",
        "\n",
        "        return filtered_audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bMvw9wpZB1ef"
      },
      "outputs": [],
      "source": [
        "class HarmonicSynthesizer(nn.Module):\n",
        "    \"\"\"\n",
        "    Synthesizes harmonic content based on fundamental frequency\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sample_rate=44100, max_harmonics=64, device='cpu'):\n",
        "        super(HarmonicSynthesizer, self).__init__()\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.max_harmonics = max_harmonics\n",
        "        self.device = device\n",
        "\n",
        "    def synthesize(\n",
        "        self,\n",
        "        f0: torch.Tensor,\n",
        "        amplitudes: torch.Tensor,\n",
        "        n_samples: int\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Synthesize harmonic audio from fundamental frequency and amplitudes\n",
        "\n",
        "        Args:\n",
        "            f0: Fundamental frequency (batch, frames)\n",
        "            amplitudes: Harmonic amplitudes (batch, frames, max_harmonics)\n",
        "            n_samples: Number of samples to generate\n",
        "\n",
        "        Returns:\n",
        "            Synthesized audio (batch, n_samples)\n",
        "        \"\"\"\n",
        "        batch_size = f0.shape[0]\n",
        "\n",
        "        # Generate time axis\n",
        "        t = torch.linspace(0, n_samples / self.sample_rate, n_samples).to(self.device)\n",
        "\n",
        "        # Synthesize each harmonic\n",
        "        audio = torch.zeros(batch_size, n_samples).to(self.device)\n",
        "\n",
        "        for h in range(self.max_harmonics):\n",
        "            # Interpolate f0 and amplitudes to sample rate\n",
        "            f0_interp = F.interpolate(f0.unsqueeze(1), size=n_samples, mode='linear').squeeze(1)\n",
        "            amp_interp = F.interpolate(\n",
        "                amplitudes[:, :, h].unsqueeze(1),\n",
        "                size=n_samples,\n",
        "                mode='linear'\n",
        "            ).squeeze(1)\n",
        "\n",
        "            # Generate harmonic\n",
        "            phase = 2 * np.pi * (h + 1) * f0_interp * t.unsqueeze(0)\n",
        "            harmonic = amp_interp * torch.sin(phase)\n",
        "\n",
        "            audio += harmonic\n",
        "\n",
        "        return audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q72o66gyB4MB"
      },
      "outputs": [],
      "source": [
        "class NoiseGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generates shaped noise for texture\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device='cpu'):\n",
        "        super(NoiseGenerator, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        n_samples: int,\n",
        "        batch_size: int = 1,\n",
        "        color: str = 'white'\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate colored noise\n",
        "\n",
        "        Args:\n",
        "            n_samples: Number of samples\n",
        "            batch_size: Batch size\n",
        "            color: 'white', 'pink', or 'brown'\n",
        "\n",
        "        Returns:\n",
        "            Noise tensor (batch_size, n_samples)\n",
        "        \"\"\"\n",
        "        # Generate white noise\n",
        "        noise = torch.randn(batch_size, n_samples).to(self.device)\n",
        "\n",
        "        if color == 'pink':\n",
        "            # Apply 1/f filtering\n",
        "            noise = self._pink_filter(noise)\n",
        "        elif color == 'brown':\n",
        "            # Apply 1/f^2 filtering\n",
        "            noise = self._brown_filter(noise)\n",
        "\n",
        "        return noise\n",
        "\n",
        "    def _pink_filter(self, noise: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply pink noise filtering (1/f spectrum)\"\"\"\n",
        "        # FFT\n",
        "        noise_fft = torch.fft.rfft(noise)\n",
        "\n",
        "        # Create 1/sqrt(f) filter\n",
        "        freqs = torch.arange(noise_fft.shape[-1]).float().to(self.device)\n",
        "        freqs[0] = 1  # Avoid division by zero\n",
        "        filter_curve = 1.0 / torch.sqrt(freqs)\n",
        "\n",
        "        # Apply filter\n",
        "        filtered_fft = noise_fft * filter_curve\n",
        "\n",
        "        # Inverse FFT\n",
        "        filtered = torch.fft.irfft(filtered_fft, n=noise.shape[-1])\n",
        "\n",
        "        # Normalize\n",
        "        filtered = filtered / filtered.std() * noise.std()\n",
        "\n",
        "        return filtered\n",
        "\n",
        "    def _brown_filter(self, noise: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply brown noise filtering (1/f^2 spectrum)\"\"\"\n",
        "        noise_fft = torch.fft.rfft(noise)\n",
        "\n",
        "        freqs = torch.arange(noise_fft.shape[-1]).float().to(self.device)\n",
        "        freqs[0] = 1\n",
        "        filter_curve = 1.0 / freqs\n",
        "\n",
        "        filtered_fft = noise_fft * filter_curve\n",
        "        filtered = torch.fft.irfft(filtered_fft, n=noise.shape[-1])\n",
        "        filtered = filtered / filtered.std() * noise.std()\n",
        "\n",
        "        return filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZzJg0qe4B6rb"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize DDSP engine\n",
        "ddsp_engine = DDSPTransformationEngine(\n",
        "    sample_rate=44100,\n",
        "    learnable_filters=True,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWWQjq7dB_7B",
        "outputId": "87515455-8776-4d5f-e9de-bd65caf2a681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Testing DDSP Transformation ===\n",
            "Input audio shape: torch.Size([2, 88200])\n",
            "Archetype weights:\n",
            "tensor([[0.1000, 0.1000, 0.6000, 0.1000, 0.1000],\n",
            "        [0.5000, 0.1000, 0.1000, 0.2000, 0.1000]])\n",
            "\n",
            "Transformed audio shape: torch.Size([2, 88200])\n",
            "Input RMS: 1.0008\n",
            "Output RMS: 0.2322\n"
          ]
        }
      ],
      "source": [
        "# Create dummy audio and archetype weights\n",
        "batch_size = 2\n",
        "n_samples = 44100 * 2  # 2 seconds\n",
        "\n",
        "dummy_audio = torch.randn(batch_size, n_samples).to(device)\n",
        "\n",
        "# Example: mostly sawtooth with some noise\n",
        "archetype_weights = torch.tensor([\n",
        "    [0.1, 0.1, 0.6, 0.1, 0.1],  # Bright sawtooth\n",
        "    [0.5, 0.1, 0.1, 0.2, 0.1]   # Smooth sine\n",
        "]).to(device)\n",
        "\n",
        "print(\"=== Testing DDSP Transformation ===\")\n",
        "print(f\"Input audio shape: {dummy_audio.shape}\")\n",
        "print(f\"Archetype weights:\\n{archetype_weights}\")\n",
        "\n",
        "# Transform audio\n",
        "transformed = ddsp_engine(dummy_audio, archetype_weights)\n",
        "\n",
        "print(f\"\\nTransformed audio shape: {transformed.shape}\")\n",
        "print(f\"Input RMS: {dummy_audio.pow(2).mean().sqrt().item():.4f}\")\n",
        "print(f\"Output RMS: {transformed.pow(2).mean().sqrt().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Display original audio\n",
        "print(\"\\n▶️  ORIGINAL AUDIO:\")\n",
        "display(ipd.Audio(dummy_audio, rate=44100, autoplay=False))\n",
        "\n",
        "# Display transformed audio\n",
        "print(\"\\n▶️  TRANSFORMED AUDIO:\")\n",
        "display(ipd.Audio(transformed, rate=44100, autoplay=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5K4YGD2CFTE",
        "outputId": "911d07eb-85a0-4e39-a04b-b36e2455d513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Noise Generator ===\n",
            "White noise RMS: 1.0022\n",
            "Pink noise RMS: 1.0063\n",
            "Brown noise RMS: 1.3032\n"
          ]
        }
      ],
      "source": [
        "# Test noise generator\n",
        "print(\"\\n=== Testing Noise Generator ===\")\n",
        "noise_gen = NoiseGenerator(device=device)\n",
        "\n",
        "white_noise = noise_gen.generate(44100, batch_size=1, color='white')\n",
        "pink_noise = noise_gen.generate(44100, batch_size=1, color='pink')\n",
        "brown_noise = noise_gen.generate(44100, batch_size=1, color='brown')\n",
        "\n",
        "print(f\"White noise RMS: {white_noise.pow(2).mean().sqrt().item():.4f}\")\n",
        "print(f\"Pink noise RMS: {pink_noise.pow(2).mean().sqrt().item():.4f}\")\n",
        "print(f\"Brown noise RMS: {brown_noise.pow(2).mean().sqrt().item():.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
