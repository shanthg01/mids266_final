{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Text tower for encoding sonic descriptions into semantic embeddings\n",
    "    \n",
    "    Architecture:\n",
    "    - BERT/RoBERTa backbone\n",
    "    - Projection layer to match audio embedding dimension\n",
    "    - Optional quantum attention augmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "        embedding_dim=768,\n",
    "        use_quantum_attention=False,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_quantum_attention = use_quantum_attention\n",
    "        \n",
    "        # Load pretrained sentence transformer\n",
    "        print(f\"Loading text encoder: {model_name}\")\n",
    "        self.sentence_model = SentenceTransformer(model_name)\n",
    "        self.base_dim = self.sentence_model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Projection layer to match audio embedding dimension\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.base_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Optional quantum attention block (placeholder for VQC integration)\n",
    "        if use_quantum_attention:\n",
    "            self.quantum_attention = QuantumAttentionBlock(embedding_dim)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text descriptions into embeddings\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text descriptions\n",
    "        \n",
    "        Returns:\n",
    "            Text embeddings of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Get base embeddings from sentence transformer\n",
    "        with torch.no_grad():\n",
    "            base_embeddings = self.sentence_model.encode(\n",
    "                texts, \n",
    "                convert_to_tensor=True,\n",
    "                device=self.device\n",
    "            )\n",
    "        \n",
    "        # Project to target dimension\n",
    "        embeddings = self.projection(base_embeddings)\n",
    "        \n",
    "        # Optional quantum attention\n",
    "        if self.use_quantum_attention:\n",
    "            embeddings = self.quantum_attention(embeddings)\n",
    "        \n",
    "        # L2 normalize for contrastive learning\n",
    "        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def encode_aspects(self, aspect_lists: List[List[str]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode aspect lists (comma-separated descriptors from MusicCaps)\n",
    "        \n",
    "        Args:\n",
    "            aspect_lists: List of aspect lists, e.g., [[\"bright\", \"metallic\"], [\"warm\", \"smooth\"]]\n",
    "        \n",
    "        Returns:\n",
    "            Aspect embeddings\n",
    "        \"\"\"\n",
    "        # Join aspects into single strings\n",
    "        texts = [\", \".join(aspects) for aspects in aspect_lists]\n",
    "        return self.forward(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum-inspired attention mechanism for enhanced semantic entanglement\n",
    "    \n",
    "    Placeholder for hybrid VQC encoder - can be extended with PennyLane/Qiskit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super(QuantumAttentionBlock, self).__init__()\n",
    "        \n",
    "        # Classical approximation of quantum attention\n",
    "        # Replace with actual VQC when implementing quantum extension\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=8,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply quantum-inspired attention\n",
    "        \n",
    "        In full quantum implementation, this would:\n",
    "        1. Encode embeddings into quantum states\n",
    "        2. Apply variational quantum circuit\n",
    "        3. Measure and decode back to classical embeddings\n",
    "        \"\"\"\n",
    "        # Self-attention (simulating quantum entanglement)\n",
    "        x_unsqueezed = x.unsqueeze(1)  # Add sequence dimension\n",
    "        attn_output, _ = self.attention(x_unsqueezed, x_unsqueezed, x_unsqueezed)\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        output = self.layer_norm(x + attn_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSynonymLearner:\n",
    "    \"\"\"\n",
    "    Learns mappings for unknown descriptors via embedding similarity\n",
    "    Handles adaptive vocabulary expansion\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_encoder: TextEncoder):\n",
    "        self.text_encoder = text_encoder\n",
    "        \n",
    "        # Core audio descriptor vocabulary\n",
    "        self.known_descriptors = {\n",
    "            'brightness': ['bright', 'dark', 'sharp', 'dull', 'brilliant'],\n",
    "            'warmth': ['warm', 'cold', 'cool', 'hot', 'cozy'],\n",
    "            'texture': ['smooth', 'rough', 'grainy', 'silky', 'crunchy'],\n",
    "            'hardness': ['soft', 'hard', 'gentle', 'aggressive', 'harsh'],\n",
    "            'density': ['thick', 'thin', 'fat', 'lean', 'full']\n",
    "        }\n",
    "        \n",
    "        # Build embedding cache for known descriptors\n",
    "        self.descriptor_embeddings = self._build_descriptor_cache()\n",
    "    \n",
    "    def _build_descriptor_cache(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Build embeddings for all known descriptors\"\"\"\n",
    "        cache = {}\n",
    "        \n",
    "        all_descriptors = []\n",
    "        for category, descriptors in self.known_descriptors.items():\n",
    "            all_descriptors.extend(descriptors)\n",
    "        \n",
    "        embeddings = self.text_encoder([desc for desc in all_descriptors])\n",
    "        \n",
    "        idx = 0\n",
    "        for category, descriptors in self.known_descriptors.items():\n",
    "            for desc in descriptors:\n",
    "                cache[desc] = embeddings[idx]\n",
    "                idx += 1\n",
    "        \n",
    "        return cache\n",
    "    \n",
    "    def find_similar_descriptors(\n",
    "        self, \n",
    "        unknown_word: str, \n",
    "        top_k: int = 3,\n",
    "        threshold: float = 0.6\n",
    "    ) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Find similar known descriptors for an unknown word\n",
    "        \n",
    "        Args:\n",
    "            unknown_word: New descriptor not in vocabulary\n",
    "            top_k: Number of similar words to return\n",
    "            threshold: Minimum similarity threshold\n",
    "        \n",
    "        Returns:\n",
    "            List of (descriptor, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        # Encode unknown word\n",
    "        unknown_embedding = self.text_encoder([unknown_word])[0]\n",
    "        \n",
    "        # Compute similarities to all known descriptors\n",
    "        similarities = []\n",
    "        for descriptor, embedding in self.descriptor_embeddings.items():\n",
    "            sim = torch.cosine_similarity(\n",
    "                unknown_embedding.unsqueeze(0),\n",
    "                embedding.unsqueeze(0)\n",
    "            ).item()\n",
    "            \n",
    "            if sim >= threshold:\n",
    "                similarities.append((descriptor, sim))\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def expand_vocabulary(self, new_descriptor: str, category: str):\n",
    "        \"\"\"\n",
    "        Add new descriptor to vocabulary\n",
    "        \n",
    "        Args:\n",
    "            new_descriptor: Descriptor to add\n",
    "            category: Which category it belongs to\n",
    "        \"\"\"\n",
    "        if category in self.known_descriptors:\n",
    "            self.known_descriptors[category].append(new_descriptor)\n",
    "            \n",
    "            # Update cache\n",
    "            embedding = self.text_encoder([new_descriptor])[0]\n",
    "            self.descriptor_embeddings[new_descriptor] = embedding\n",
    "            \n",
    "            print(f\"Added '{new_descriptor}' to {category} category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAugmentation:\n",
    "    \"\"\"\n",
    "    Data augmentation for text descriptions\n",
    "    Generates variations while preserving semantic meaning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.synonym_map = {\n",
    "            'bright': ['brilliant', 'sharp', 'crisp', 'clear'],\n",
    "            'warm': ['cozy', 'soft', 'mellow', 'rich'],\n",
    "            'harsh': ['aggressive', 'abrasive', 'hard', 'rough'],\n",
    "            'smooth': ['silky', 'gentle', 'soft', 'flowing'],\n",
    "            'crunchy': ['gritty', 'textured', 'grainy', 'distorted']\n",
    "        }\n",
    "    \n",
    "    def augment_description(self, text: str, num_variations: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate semantic variations of a description\n",
    "        \n",
    "        Args:\n",
    "            text: Original description\n",
    "            num_variations: Number of variations to generate\n",
    "        \n",
    "        Returns:\n",
    "            List of augmented descriptions\n",
    "        \"\"\"\n",
    "        words = text.lower().split()\n",
    "        variations = [text]  # Include original\n",
    "        \n",
    "        for _ in range(num_variations):\n",
    "            new_words = []\n",
    "            for word in words:\n",
    "                if word in self.synonym_map and np.random.random() > 0.5:\n",
    "                    # Replace with synonym\n",
    "                    new_words.append(np.random.choice(self.synonym_map[word]))\n",
    "                else:\n",
    "                    new_words.append(word)\n",
    "            \n",
    "            variations.append(' '.join(new_words))\n",
    "        \n",
    "        return variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text encoder\n",
    "text_encoder = TextEncoder(\n",
    "    embedding_dim=768,\n",
    "    use_quantum_attention=False,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode some descriptions\n",
    "descriptions = [\n",
    "    \"bright and crunchy guitar\",\n",
    "    \"warm smooth piano melody\",\n",
    "    \"harsh digital synth\"\n",
    "]\n",
    "\n",
    "embeddings = text_encoder(descriptions)\n",
    "print(f\"Text embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Sample embedding norm: {torch.norm(embeddings[0]).item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test synonym learner\n",
    "synonym_learner = SemanticSynonymLearner(text_encoder)\n",
    "similar = synonym_learner.find_similar_descriptors(\"sparkly\", top_k=3)\n",
    "print(f\"\\nSimilar to 'sparkly': {similar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test augmentation\n",
    "augmenter = TextAugmentation()\n",
    "variations = augmenter.augment_description(\"bright warm sound\", num_variations=3)\n",
    "print(f\"\\nAugmented descriptions: {variations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
